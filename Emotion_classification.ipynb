{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Emotion_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1XQYtzaUcwh6bt79vzjUqqHHQftc_0g9c",
      "authorship_tag": "ABX9TyOoYJx4uuGqbEIprP14eujz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04786d5e8d1244899ceb0429df81b93b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_440cec020a7147c896347fe7fc68fde8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_65814b1fee3f4694bb47f76be96cee37",
              "IPY_MODEL_ad76d5520ea54582b8f32b5855293804"
            ]
          }
        },
        "440cec020a7147c896347fe7fc68fde8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "65814b1fee3f4694bb47f76be96cee37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_049c20900ac24de89314a3199ac06807",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 779,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 779,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3f5814fd31e64484aba4109ca08d2e30"
          }
        },
        "ad76d5520ea54582b8f32b5855293804": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7ae6490e942547959d9334a5706a5436",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 779/779 [00:00&lt;00:00, 1.50kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4f2119983ec44ea0b1251b421d5a40c2"
          }
        },
        "049c20900ac24de89314a3199ac06807": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3f5814fd31e64484aba4109ca08d2e30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7ae6490e942547959d9334a5706a5436": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4f2119983ec44ea0b1251b421d5a40c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2d529d13e62b40428c8da7815e0dfb02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9f11039e06b8453985300a1eaefa9b18",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8a24f647b6b54ef2a972b30e82e0ac37",
              "IPY_MODEL_5f5a5b7de5e54f8b948b997724d0b83d"
            ]
          }
        },
        "9f11039e06b8453985300a1eaefa9b18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8a24f647b6b54ef2a972b30e82e0ac37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8e82be8c25d34b0eae99bd95fdae818d",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 898822,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 898822,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5a4b91e88f8749409c48f14cdfe8bfd3"
          }
        },
        "5f5a5b7de5e54f8b948b997724d0b83d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5fd2fcab05db49b6b5b78774564d9f45",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 899k/899k [00:02&lt;00:00, 364kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1b501d3e32f448168edad1b767358340"
          }
        },
        "8e82be8c25d34b0eae99bd95fdae818d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5a4b91e88f8749409c48f14cdfe8bfd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5fd2fcab05db49b6b5b78774564d9f45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1b501d3e32f448168edad1b767358340": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "89ba0c69183c489b9dd8c2adbb163532": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b214429c89674a188da59b91fe81fff9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_414b3266be044f6bab9cb3437ca891b4",
              "IPY_MODEL_626c06ab74694b1a80b4c7afb5568886"
            ]
          }
        },
        "b214429c89674a188da59b91fe81fff9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "414b3266be044f6bab9cb3437ca891b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c17ddc8856804a4e94acfc2caf5cba31",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b32b009e931546279192f622dbae86c9"
          }
        },
        "626c06ab74694b1a80b4c7afb5568886": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_43c9e11bc0554e2a83e1460006c4cdfe",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 456k/456k [00:01&lt;00:00, 309kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ae44fe9ce2d047c89be47e5cc4540c16"
          }
        },
        "c17ddc8856804a4e94acfc2caf5cba31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b32b009e931546279192f622dbae86c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "43c9e11bc0554e2a83e1460006c4cdfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ae44fe9ce2d047c89be47e5cc4540c16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3525881821304e419bdcfb8645aedaab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_783685c7a8f54fe78c5ef60d16039a14",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e077d895d24342ed91cf444938ca5ca6",
              "IPY_MODEL_fa3e4a5a577747aa9170f95cb53897b2"
            ]
          }
        },
        "783685c7a8f54fe78c5ef60d16039a14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e077d895d24342ed91cf444938ca5ca6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_24a0ea2a74b84611a2243b82384ccf61",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 150,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 150,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_05adb6c86d624ffca274d34967e8e9f7"
          }
        },
        "fa3e4a5a577747aa9170f95cb53897b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_25176c44b807453fa078b0cc786651ec",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 150/150 [00:00&lt;00:00, 430B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f9c1dcd868bd4972acb9be3097e26a83"
          }
        },
        "24a0ea2a74b84611a2243b82384ccf61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "05adb6c86d624ffca274d34967e8e9f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "25176c44b807453fa078b0cc786651ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f9c1dcd868bd4972acb9be3097e26a83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9558032a41f44a49a26288d0d8699252": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0134cac840e345d9a7eb01437e09486f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c4e7dc94aaea4201b825f5eb2053aaf1",
              "IPY_MODEL_f5fbec745cdd41daa210027f576f5374"
            ]
          }
        },
        "0134cac840e345d9a7eb01437e09486f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c4e7dc94aaea4201b825f5eb2053aaf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0093b72fe7484b3da4c0f37721ce73f6",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 501233376,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 501233376,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5d45996de1fd43aba39bc79e87a6fae4"
          }
        },
        "f5fbec745cdd41daa210027f576f5374": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f0234b6d47b9444dad2352307d611dd5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 501M/501M [00:14&lt;00:00, 35.6MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2552d6dc4e0b411f8641506434f8ff5b"
          }
        },
        "0093b72fe7484b3da4c0f37721ce73f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5d45996de1fd43aba39bc79e87a6fae4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f0234b6d47b9444dad2352307d611dd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2552d6dc4e0b411f8641506434f8ff5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/un21988/CE888-7-project-2/blob/main/Emotion_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-h7d8E9AlME"
      },
      "source": [
        "def preprocess(text):\n",
        "    new_text = []\n",
        "    for t in text.split(\" \"):\n",
        "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
        "        t = 'http' if t.startswith('http') else t\n",
        "        new_text.append(t)\n",
        "    return \" \".join(new_text)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9InvRWKArJ-",
        "outputId": "7f9f84ad-ec83-40d9-dbdc-a21b400f5844"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.1MB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 901kB 17.6MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.3MB 22.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212,
          "referenced_widgets": [
            "04786d5e8d1244899ceb0429df81b93b",
            "440cec020a7147c896347fe7fc68fde8",
            "65814b1fee3f4694bb47f76be96cee37",
            "ad76d5520ea54582b8f32b5855293804",
            "049c20900ac24de89314a3199ac06807",
            "3f5814fd31e64484aba4109ca08d2e30",
            "7ae6490e942547959d9334a5706a5436",
            "4f2119983ec44ea0b1251b421d5a40c2",
            "2d529d13e62b40428c8da7815e0dfb02",
            "9f11039e06b8453985300a1eaefa9b18",
            "8a24f647b6b54ef2a972b30e82e0ac37",
            "5f5a5b7de5e54f8b948b997724d0b83d",
            "8e82be8c25d34b0eae99bd95fdae818d",
            "5a4b91e88f8749409c48f14cdfe8bfd3",
            "5fd2fcab05db49b6b5b78774564d9f45",
            "1b501d3e32f448168edad1b767358340",
            "89ba0c69183c489b9dd8c2adbb163532",
            "b214429c89674a188da59b91fe81fff9",
            "414b3266be044f6bab9cb3437ca891b4",
            "626c06ab74694b1a80b4c7afb5568886",
            "c17ddc8856804a4e94acfc2caf5cba31",
            "b32b009e931546279192f622dbae86c9",
            "43c9e11bc0554e2a83e1460006c4cdfe",
            "ae44fe9ce2d047c89be47e5cc4540c16",
            "3525881821304e419bdcfb8645aedaab",
            "783685c7a8f54fe78c5ef60d16039a14",
            "e077d895d24342ed91cf444938ca5ca6",
            "fa3e4a5a577747aa9170f95cb53897b2",
            "24a0ea2a74b84611a2243b82384ccf61",
            "05adb6c86d624ffca274d34967e8e9f7",
            "25176c44b807453fa078b0cc786651ec",
            "f9c1dcd868bd4972acb9be3097e26a83"
          ]
        },
        "id": "TnGMft2aAuDB",
        "outputId": "3c2e3ea0-bc16-49dc-9c1f-e3bb1e179a12"
      },
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "import csv\n",
        "import urllib.request\n",
        "\n",
        "task='emotion'\n",
        "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "04786d5e8d1244899ceb0429df81b93b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=779.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2d529d13e62b40428c8da7815e0dfb02",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898822.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89ba0c69183c489b9dd8c2adbb163532",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3525881821304e419bdcfb8645aedaab",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=150.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vy8_Yxk1AxJF",
        "outputId": "808cb1cf-5df6-4e59-dcf0-8d789204971d"
      },
      "source": [
        "# download label mapping\n",
        "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emotion/mapping.txt\"\n",
        "with urllib.request.urlopen(mapping_link) as f:\n",
        "    html = f.read().decode('utf-8').split(\"\\n\")\n",
        "    csvreader = csv.reader(html, delimiter='\\t')\n",
        "labels = [row[0] for row in csvreader if len(row) > 0]\n",
        "labels"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['0', '1', '2', '3']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8k1swpkAzX0",
        "outputId": "a3776b72-fe0c-4fb4-8e1c-8d87f1461ea0"
      },
      "source": [
        "import pandas as pd\n",
        "import urllib.request \n",
        "import requests\n",
        "\n",
        "test_text_url = \"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emotion/test_text.txt\"\n",
        "val_text_url= \"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emotion/val_text.txt\"\n",
        "test_lable_url= \"https://raw.githubusercontent.com/un21988/CE888-7-project-2/main/Data-set/emotion/test_labels.txt\"\n",
        "\n",
        "\n",
        "\n",
        "r = requests.get(test_text_url, allow_redirects=True)\n",
        "open('test_text.txt', 'wb').write(r.content)\n",
        "\n",
        "r1 = requests.get(val_text_url, allow_redirects=True)\n",
        "open('val_text.txt', 'wb').write(r1.content)\n",
        "\n",
        "\n",
        "r2 = requests.get(test_lable_url, allow_redirects=True)\n",
        "open('test_lable.txt', 'wb').write(r2.content)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2842"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmQupj6IJ13V",
        "outputId": "277550d0-efd8-4ca1-99d5-f3289eb643bf"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Apr 22 13:13:13 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8    27W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b69E7AMcA3ME",
        "outputId": "a63f2588-1f3f-4fa9-88b0-b4a27027715d"
      },
      "source": [
        "\n",
        "stream=open(\"test_text.txt\")\n",
        "tweets=stream.readlines()\n",
        "stream.close()\n",
        "print(tweets[1])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "@user Interesting choice of words... Are you confirming that governments fund #terrorism? Bit of an open door, but still... \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0Xkb62VA43t",
        "outputId": "db61f613-5f01-4a53-db3f-211a0c239c26"
      },
      "source": [
        "stream1=open(\"test_lable.txt\")\n",
        "test_lable=stream1.readlines()\n",
        "stream1.close()\n",
        "#print(test_lable)\n",
        "test_data=[]\n",
        "print(type(test_lable[1]))\n",
        "for i in range(0,len(test_lable)):\n",
        "  temp_data=test_lable[i]\n",
        "  test_data.append(int(temp_data))\n",
        "print(test_data)\n",
        "print(type(test_data[1]))\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'str'>\n",
            "[3, 0, 3, 1, 1, 0, 3, 3, 3, 0, 0, 3, 3, 0, 0, 3, 1, 0, 0, 0, 0, 3, 3, 2, 2, 1, 1, 0, 3, 1, 3, 1, 0, 0, 0, 0, 2, 2, 0, 3, 2, 0, 0, 1, 0, 0, 0, 0, 1, 0, 3, 3, 2, 3, 1, 0, 3, 3, 2, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3, 0, 0, 0, 0, 3, 2, 2, 1, 0, 0, 1, 3, 0, 0, 0, 1, 1, 3, 3, 1, 1, 0, 1, 1, 1, 3, 0, 0, 0, 1, 3, 1, 0, 1, 0, 2, 1, 0, 3, 0, 0, 0, 3, 3, 3, 1, 0, 2, 1, 0, 1, 0, 0, 1, 0, 0, 0, 3, 1, 0, 0, 1, 1, 1, 2, 3, 3, 0, 3, 3, 0, 0, 3, 2, 0, 0, 0, 1, 2, 0, 0, 3, 0, 1, 3, 1, 3, 0, 3, 0, 2, 1, 1, 1, 0, 3, 0, 1, 0, 0, 2, 1, 2, 0, 0, 0, 1, 1, 1, 0, 0, 3, 3, 0, 3, 3, 3, 3, 0, 0, 2, 3, 2, 1, 1, 0, 0, 3, 1, 0, 0, 0, 1, 1, 0, 0, 3, 3, 1, 3, 2, 1, 1, 3, 0, 0, 3, 0, 2, 3, 3, 3, 3, 0, 2, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 1, 1, 0, 3, 0, 1, 3, 3, 3, 3, 1, 1, 3, 0, 0, 3, 0, 1, 1, 3, 0, 3, 1, 0, 2, 0, 2, 0, 3, 1, 0, 3, 0, 0, 3, 3, 1, 0, 3, 3, 3, 3, 3, 0, 2, 3, 0, 0, 0, 1, 0, 0, 0, 3, 3, 1, 3, 2, 1, 3, 1, 0, 1, 0, 0, 3, 1, 3, 0, 1, 3, 3, 0, 3, 0, 1, 0, 1, 3, 3, 3, 0, 3, 3, 0, 0, 1, 0, 0, 0, 3, 2, 3, 0, 2, 2, 3, 3, 1, 3, 3, 1, 0, 1, 3, 0, 1, 1, 2, 0, 3, 0, 1, 3, 3, 0, 1, 0, 1, 0, 1, 1, 3, 0, 0, 0, 0, 2, 0, 0, 1, 0, 2, 3, 1, 0, 2, 3, 1, 3, 0, 2, 3, 0, 3, 1, 3, 3, 1, 0, 2, 0, 1, 1, 3, 1, 1, 3, 1, 1, 3, 0, 0, 0, 0, 2, 3, 0, 1, 0, 2, 3, 0, 1, 1, 1, 1, 3, 0, 0, 2, 1, 1, 0, 3, 1, 0, 3, 1, 3, 0, 3, 3, 3, 1, 3, 3, 1, 0, 0, 1, 1, 0, 3, 3, 0, 2, 3, 1, 0, 3, 3, 3, 0, 3, 3, 2, 3, 0, 2, 3, 0, 0, 1, 1, 3, 1, 0, 0, 1, 1, 0, 3, 0, 0, 0, 3, 3, 0, 3, 0, 0, 1, 1, 3, 3, 3, 0, 1, 1, 1, 1, 3, 3, 3, 2, 0, 2, 1, 0, 3, 3, 3, 3, 0, 2, 3, 0, 1, 0, 0, 2, 1, 1, 0, 0, 3, 0, 3, 0, 1, 0, 0, 1, 0, 3, 1, 0, 0, 0, 3, 1, 3, 3, 0, 0, 1, 3, 1, 0, 1, 1, 3, 0, 0, 0, 3, 0, 1, 3, 3, 0, 1, 1, 1, 3, 0, 2, 0, 1, 2, 3, 1, 2, 2, 1, 1, 0, 0, 0, 0, 3, 3, 1, 1, 1, 0, 2, 0, 1, 3, 0, 1, 0, 1, 1, 1, 3, 0, 0, 1, 0, 0, 0, 1, 3, 0, 0, 1, 1, 1, 2, 2, 1, 0, 0, 2, 3, 1, 0, 3, 3, 0, 1, 3, 1, 2, 1, 1, 0, 1, 2, 0, 0, 3, 0, 0, 0, 3, 0, 0, 1, 1, 0, 3, 1, 0, 0, 0, 3, 1, 1, 3, 0, 0, 0, 3, 1, 0, 1, 0, 0, 3, 3, 1, 3, 0, 1, 3, 3, 1, 1, 3, 0, 0, 3, 2, 3, 1, 3, 0, 0, 0, 1, 0, 1, 2, 1, 0, 1, 0, 1, 1, 0, 3, 0, 0, 0, 3, 0, 3, 0, 1, 0, 0, 0, 1, 0, 1, 3, 1, 3, 1, 0, 1, 0, 2, 3, 0, 0, 3, 2, 2, 3, 1, 0, 3, 3, 3, 0, 3, 3, 0, 3, 1, 1, 3, 0, 1, 3, 3, 3, 1, 1, 0, 2, 1, 0, 3, 0, 3, 3, 1, 0, 2, 1, 0, 0, 0, 1, 0, 2, 0, 0, 0, 3, 0, 1, 0, 1, 0, 0, 1, 3, 1, 1, 1, 1, 1, 3, 0, 0, 0, 2, 0, 1, 3, 1, 3, 1, 0, 0, 2, 1, 1, 0, 0, 3, 3, 3, 3, 1, 1, 1, 2, 2, 0, 3, 0, 3, 0, 1, 0, 0, 0, 1, 1, 1, 2, 0, 1, 2, 0, 0, 0, 0, 0, 1, 1, 1, 3, 0, 3, 0, 1, 0, 3, 0, 3, 0, 0, 3, 0, 0, 3, 3, 2, 1, 3, 1, 0, 1, 1, 0, 0, 3, 1, 0, 3, 0, 0, 0, 0, 0, 2, 3, 0, 0, 0, 1, 1, 3, 0, 3, 0, 0, 2, 3, 1, 3, 1, 0, 1, 0, 3, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 2, 0, 3, 3, 0, 0, 2, 0, 1, 1, 1, 0, 2, 0, 3, 3, 0, 3, 0, 0, 1, 3, 3, 0, 0, 3, 1, 1, 2, 0, 3, 3, 3, 3, 3, 0, 1, 0, 0, 0, 0, 3, 0, 0, 0, 1, 1, 3, 3, 3, 0, 1, 2, 0, 0, 0, 0, 3, 3, 1, 3, 2, 1, 3, 0, 1, 0, 1, 3, 1, 0, 1, 2, 0, 2, 0, 2, 0, 2, 2, 0, 0, 0, 2, 0, 3, 3, 1, 2, 3, 0, 3, 0, 0, 0, 3, 3, 3, 3, 0, 1, 3, 0, 0, 2, 0, 3, 1, 1, 3, 3, 1, 3, 0, 2, 1, 1, 1, 1, 3, 0, 3, 2, 1, 1, 0, 0, 0, 0, 3, 0, 1, 1, 0, 0, 0, 1, 0, 3, 1, 0, 3, 0, 0, 0, 1, 3, 1, 2, 0, 2, 3, 1, 3, 0, 3, 1, 0, 1, 3, 0, 0, 0, 0, 1, 3, 0, 3, 3, 1, 3, 0, 1, 3, 1, 3, 2, 1, 3, 3, 0, 1, 3, 3, 3, 2, 1, 0, 0, 1, 2, 1, 3, 3, 0, 3, 1, 1, 3, 0, 3, 0, 3, 0, 1, 0, 2, 3, 3, 0, 1, 1, 2, 2, 2, 0, 1, 0, 3, 1, 1, 3, 0, 2, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 0, 1, 1, 3, 0, 0, 3, 3, 1, 0, 3, 3, 3, 3, 0, 1, 0, 3, 1, 0, 3, 0, 0, 0, 0, 1, 0, 3, 3, 0, 3, 0, 3, 3, 1, 0, 1, 2, 2, 2, 1, 2, 0, 0, 1, 2, 1, 0, 0, 3, 0, 3, 0, 1, 0, 0, 1, 0, 3, 3, 0, 0, 2, 0, 0, 0, 3, 0, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 2, 3, 0, 2, 3, 1, 1, 0, 0, 1, 0, 0, 2, 2, 1, 1, 0, 1, 3, 0, 0, 1, 1, 3, 3, 2, 0, 0, 1, 2, 3, 1, 1, 3, 3, 0, 0, 3, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 3, 1, 3, 0, 3, 1, 3, 0, 0, 1, 0, 0, 0, 2, 2, 3, 0, 3, 0, 3, 0, 0, 0, 1, 1, 0, 1, 2, 0, 0, 3, 1, 0, 2, 1, 1, 0, 3, 3, 0, 3, 0, 0, 0, 1, 0, 2, 0, 0, 0, 3, 0, 2, 1, 3, 2, 0, 0, 2, 1, 0, 0, 0, 3, 0, 0, 1, 0, 1, 1, 3, 0, 3, 3, 1, 0, 0, 3, 3, 3, 3, 1, 3, 1, 0, 1, 3, 0, 0, 0, 0, 1, 1, 3, 0, 2, 3, 0, 3, 3, 0, 3, 1, 3, 3, 0, 0, 3, 0, 0, 0, 3, 0, 1, 3, 0, 0, 0, 3, 1, 0, 3, 0, 0, 0, 2, 3, 3, 3, 1, 1, 3, 3, 2, 1, 3, 3, 1, 1, 3, 0, 3, 1, 2, 1, 3, 0, 0, 1, 3, 3, 1, 0, 3, 0, 1, 3, 3, 1, 3, 0, 0, 1]\n",
            "<class 'int'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwkAqs2uA6f8",
        "outputId": "3bccf435-c2c5-4a1f-a3d9-b35bc01ba1f1"
      },
      "source": [
        "stream2=open(\"val_text.txt\")\n",
        "val_text=stream2.readlines()\n",
        "stream2.close()\n",
        "print(val_text[1])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "if not then #teamchristine bc all tana has done is provoke her by tweeting shady shit and trying to be a hard bitch begging for a fight \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rO0TSda8A8Jm",
        "outputId": "0b84734d-9d30-4b9b-a358-372922d1f126"
      },
      "source": [
        "tweet_preprocess=[]\n",
        "\n",
        "#print(len(tweets))\n",
        "for i in range(0,len(tweets)):\n",
        "  tweet_preprocess.append(preprocess(tweets[i]))\n",
        "  \n",
        "\n",
        "print(tweet_preprocess[0])\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#Deppression is real. Partners w/ #depressed people truly dont understand the depth in which they affect us. Add in #anxiety &amp;makes it worse \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9558032a41f44a49a26288d0d8699252",
            "0134cac840e345d9a7eb01437e09486f",
            "c4e7dc94aaea4201b825f5eb2053aaf1",
            "f5fbec745cdd41daa210027f576f5374",
            "0093b72fe7484b3da4c0f37721ce73f6",
            "5d45996de1fd43aba39bc79e87a6fae4",
            "f0234b6d47b9444dad2352307d611dd5",
            "2552d6dc4e0b411f8641506434f8ff5b"
          ]
        },
        "id": "Au08DuLBA-IT",
        "outputId": "22f1897c-d018-47a5-fe07-d33aea20be29"
      },
      "source": [
        "result= []\n",
        "for i in  range(0,len(tweet_preprocess)):\n",
        "  result.append(model(tweet_preprocess[i]))\n",
        "  \n",
        " \n",
        "#print(result)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9558032a41f44a49a26288d0d8699252",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=501233376.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cFbyynZ_kWo",
        "outputId": "6b5c9bc8-4922-498e-d250-fb294bc887c7"
      },
      "source": [
        "print(result)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['1) 3 0.966', '2) 0 0.0184', '3) 2 0.011', '4) 1 0.0046'], ['1) 2 0.4538', '2) 0 0.2607', '3) 3 0.2034', '4) 1 0.0822'], ['1) 3 0.984', '2) 0 0.007', '3) 1 0.005', '4) 2 0.004'], ['1) 1 0.9216', '2) 2 0.0455', '3) 0 0.0179', '4) 3 0.0151'], ['1) 1 0.5961', '2) 2 0.3472', '3) 3 0.0411', '4) 0 0.0156'], ['1) 0 0.9687', '2) 2 0.0126', '3) 3 0.011', '4) 1 0.0077'], ['1) 3 0.9746', '2) 1 0.0146', '3) 2 0.0057', '4) 0 0.0051'], ['1) 3 0.9759', '2) 0 0.0117', '3) 2 0.0064', '4) 1 0.006'], ['1) 1 0.7872', '2) 2 0.0941', '3) 3 0.0778', '4) 0 0.0409'], ['1) 0 0.9809', '2) 3 0.0087', '3) 1 0.0054', '4) 2 0.005'], ['1) 0 0.9804', '2) 3 0.0101', '3) 2 0.0052', '4) 1 0.0043'], ['1) 3 0.9377', '2) 0 0.0356', '3) 2 0.0176', '4) 1 0.0091'], ['1) 3 0.9643', '2) 2 0.0195', '3) 0 0.011', '4) 1 0.0053'], ['1) 0 0.9763', '2) 2 0.0106', '3) 3 0.0081', '4) 1 0.005'], ['1) 0 0.9121', '2) 2 0.0535', '3) 3 0.0265', '4) 1 0.0079'], ['1) 3 0.9743', '2) 2 0.0102', '3) 0 0.0079', '4) 1 0.0076'], ['1) 1 0.9237', '2) 2 0.0401', '3) 0 0.0263', '4) 3 0.0099'], ['1) 3 0.8219', '2) 0 0.1072', '3) 2 0.0588', '4) 1 0.0121'], ['1) 0 0.8918', '2) 3 0.0746', '3) 2 0.0277', '4) 1 0.006'], ['1) 0 0.9685', '2) 3 0.0132', '3) 1 0.0098', '4) 2 0.0084'], ['1) 0 0.9856', '2) 2 0.0053', '3) 1 0.0048', '4) 3 0.0043'], ['1) 3 0.9853', '2) 0 0.0058', '3) 1 0.0054', '4) 2 0.0035'], ['1) 3 0.9646', '2) 2 0.017', '3) 0 0.014', '4) 1 0.0044'], ['1) 0 0.7276', '2) 2 0.1653', '3) 3 0.0944', '4) 1 0.0128'], ['1) 2 0.8201', '2) 1 0.1025', '3) 3 0.064', '4) 0 0.0134'], ['1) 1 0.9362', '2) 0 0.0236', '3) 3 0.0207', '4) 2 0.0195'], ['1) 3 0.8763', '2) 1 0.0567', '3) 0 0.0431', '4) 2 0.0239'], ['1) 0 0.9145', '2) 2 0.0412', '3) 3 0.0377', '4) 1 0.0067'], ['1) 3 0.9804', '2) 0 0.0069', '3) 2 0.0065', '4) 1 0.0062'], ['1) 1 0.7683', '2) 2 0.1509', '3) 3 0.0686', '4) 0 0.0122'], ['1) 3 0.643', '2) 0 0.3076', '3) 1 0.0326', '4) 2 0.0168'], ['1) 1 0.9449', '2) 2 0.0319', '3) 3 0.0122', '4) 0 0.011'], ['1) 0 0.8318', '2) 3 0.1336', '3) 1 0.0246', '4) 2 0.01'], ['1) 0 0.9715', '2) 3 0.0129', '3) 1 0.008', '4) 2 0.0077'], ['1) 0 0.9649', '2) 2 0.0139', '3) 1 0.0116', '4) 3 0.0096'], ['1) 0 0.977', '2) 3 0.0092', '3) 1 0.0071', '4) 2 0.0067'], ['1) 2 0.5263', '2) 0 0.4023', '3) 3 0.0528', '4) 1 0.0186'], ['1) 3 0.7076', '2) 2 0.2368', '3) 1 0.0389', '4) 0 0.0167'], ['1) 0 0.982', '2) 3 0.0087', '3) 2 0.0054', '4) 1 0.0039'], ['1) 3 0.5952', '2) 1 0.2037', '3) 0 0.1808', '4) 2 0.0202'], ['1) 0 0.9197', '2) 2 0.0443', '3) 3 0.0297', '4) 1 0.0063'], ['1) 0 0.9824', '2) 2 0.0071', '3) 3 0.0058', '4) 1 0.0047'], ['1) 3 0.8923', '2) 0 0.0789', '3) 2 0.0188', '4) 1 0.01'], ['1) 1 0.9647', '2) 2 0.0172', '3) 0 0.0106', '4) 3 0.0075'], ['1) 3 0.9183', '2) 0 0.0536', '3) 2 0.0144', '4) 1 0.0136'], ['1) 0 0.9714', '2) 2 0.0119', '3) 3 0.0094', '4) 1 0.0073'], ['1) 0 0.8624', '2) 3 0.0936', '3) 2 0.0346', '4) 1 0.0095'], ['1) 1 0.9234', '2) 0 0.0393', '3) 2 0.025', '4) 3 0.0122'], ['1) 1 0.8853', '2) 2 0.0527', '3) 0 0.0451', '4) 3 0.0169'], ['1) 1 0.9584', '2) 0 0.0162', '3) 3 0.0142', '4) 2 0.0112'], ['1) 3 0.9521', '2) 0 0.0259', '3) 1 0.0147', '4) 2 0.0073'], ['1) 3 0.936', '2) 2 0.0466', '3) 0 0.0113', '4) 1 0.0061'], ['1) 1 0.4863', '2) 2 0.4637', '3) 3 0.0305', '4) 0 0.0195'], ['1) 3 0.8165', '2) 2 0.115', '3) 1 0.0357', '4) 0 0.0329'], ['1) 2 0.8233', '2) 1 0.1134', '3) 3 0.0473', '4) 0 0.0161'], ['1) 0 0.4875', '2) 3 0.427', '3) 2 0.0677', '4) 1 0.0177'], ['1) 3 0.9757', '2) 2 0.0113', '3) 0 0.0075', '4) 1 0.0055'], ['1) 3 0.9532', '2) 0 0.0253', '3) 2 0.0145', '4) 1 0.007'], ['1) 2 0.9519', '2) 3 0.022', '3) 1 0.0136', '4) 0 0.0124'], ['1) 0 0.9398', '2) 3 0.0442', '3) 1 0.0091', '4) 2 0.0069'], ['1) 0 0.7813', '2) 3 0.1805', '3) 2 0.0264', '4) 1 0.0118'], ['1) 1 0.961', '2) 3 0.0165', '3) 2 0.0152', '4) 0 0.0073'], ['1) 0 0.9195', '2) 3 0.051', '3) 2 0.0231', '4) 1 0.0064'], ['1) 0 0.9798', '2) 1 0.0102', '3) 3 0.0053', '4) 2 0.0047'], ['1) 0 0.7876', '2) 1 0.1097', '3) 2 0.0545', '4) 3 0.0483'], ['1) 0 0.6328', '2) 3 0.3405', '3) 2 0.0154', '4) 1 0.0113'], ['1) 1 0.6741', '2) 2 0.1792', '3) 0 0.1111', '4) 3 0.0356'], ['1) 0 0.9801', '2) 2 0.0095', '3) 3 0.0059', '4) 1 0.0045'], ['1) 0 0.5003', '2) 3 0.4801', '3) 2 0.0099', '4) 1 0.0097'], ['1) 0 0.9109', '2) 3 0.0699', '3) 2 0.0129', '4) 1 0.0062'], ['1) 0 0.7115', '2) 3 0.26', '3) 2 0.0193', '4) 1 0.0092'], ['1) 3 0.4821', '2) 2 0.3281', '3) 0 0.1008', '4) 1 0.0889'], ['1) 0 0.981', '2) 1 0.0074', '3) 2 0.0068', '4) 3 0.0048'], ['1) 0 0.9545', '2) 2 0.0262', '3) 3 0.0128', '4) 1 0.0065'], ['1) 0 0.9815', '2) 2 0.0078', '3) 1 0.0056', '4) 3 0.0051'], ['1) 0 0.979', '2) 3 0.0077', '3) 2 0.0075', '4) 1 0.0059'], ['1) 3 0.8984', '2) 0 0.0768', '3) 1 0.0174', '4) 2 0.0074'], ['1) 2 0.7569', '2) 1 0.1826', '3) 0 0.0324', '4) 3 0.0281'], ['1) 2 0.9317', '2) 1 0.0239', '3) 3 0.023', '4) 0 0.0214'], ['1) 1 0.8627', '2) 3 0.1013', '3) 0 0.0199', '4) 2 0.0161'], ['1) 0 0.9763', '2) 3 0.014', '3) 2 0.0049', '4) 1 0.0048'], ['1) 0 0.9814', '2) 2 0.0068', '3) 3 0.0064', '4) 1 0.0054'], ['1) 1 0.9705', '2) 2 0.012', '3) 0 0.0093', '4) 3 0.0083'], ['1) 3 0.7988', '2) 0 0.1713', '3) 1 0.0216', '4) 2 0.0083'], ['1) 0 0.8095', '2) 3 0.1575', '3) 2 0.0221', '4) 1 0.0109'], ['1) 0 0.9765', '2) 3 0.0103', '3) 2 0.008', '4) 1 0.0052'], ['1) 0 0.9787', '2) 2 0.0095', '3) 1 0.0069', '4) 3 0.0049'], ['1) 1 0.9713', '2) 2 0.012', '3) 3 0.0091', '4) 0 0.0076'], ['1) 1 0.968', '2) 0 0.0119', '3) 2 0.0103', '4) 3 0.0099'], ['1) 3 0.9732', '2) 0 0.0114', '3) 2 0.011', '4) 1 0.0045'], ['1) 3 0.981', '2) 0 0.0109', '3) 1 0.0047', '4) 2 0.0034'], ['1) 1 0.9478', '2) 3 0.0201', '3) 2 0.0178', '4) 0 0.0142'], ['1) 3 0.4802', '2) 1 0.2682', '3) 0 0.1466', '4) 2 0.105'], ['1) 0 0.9801', '2) 3 0.0078', '3) 1 0.0068', '4) 2 0.0053'], ['1) 0 0.4126', '2) 2 0.2871', '3) 1 0.2508', '4) 3 0.0495'], ['1) 1 0.9284', '2) 2 0.0275', '3) 0 0.0259', '4) 3 0.0182'], ['1) 1 0.7425', '2) 0 0.2282', '3) 2 0.0178', '4) 3 0.0115'], ['1) 3 0.9803', '2) 1 0.0085', '3) 0 0.0071', '4) 2 0.0041'], ['1) 0 0.6324', '2) 3 0.3421', '3) 1 0.0176', '4) 2 0.0079'], ['1) 0 0.9839', '2) 2 0.0062', '3) 1 0.0058', '4) 3 0.0042'], ['1) 0 0.82', '2) 3 0.0965', '3) 2 0.0728', '4) 1 0.0107'], ['1) 1 0.9687', '2) 2 0.0135', '3) 3 0.0098', '4) 0 0.0079'], ['1) 1 0.8051', '2) 3 0.1264', '3) 0 0.0345', '4) 2 0.034'], ['1) 1 0.7596', '2) 2 0.1928', '3) 0 0.0312', '4) 3 0.0164'], ['1) 0 0.9638', '2) 3 0.0164', '3) 2 0.0154', '4) 1 0.0043'], ['1) 1 0.6091', '2) 2 0.2267', '3) 0 0.0872', '4) 3 0.077'], ['1) 0 0.9717', '2) 1 0.0108', '3) 2 0.0101', '4) 3 0.0074'], ['1) 2 0.8687', '2) 3 0.086', '3) 1 0.029', '4) 0 0.0163'], ['1) 1 0.9705', '2) 0 0.0139', '3) 2 0.0102', '4) 3 0.0054'], ['1) 0 0.9747', '2) 3 0.0103', '3) 2 0.0093', '4) 1 0.0058'], ['1) 2 0.457', '2) 3 0.3462', '3) 0 0.1343', '4) 1 0.0625'], ['1) 0 0.983', '2) 2 0.0063', '3) 1 0.006', '4) 3 0.0047'], ['1) 0 0.8897', '2) 1 0.0723', '3) 3 0.0207', '4) 2 0.0173'], ['1) 0 0.6731', '2) 3 0.3044', '3) 2 0.0145', '4) 1 0.008'], ['1) 3 0.9827', '2) 1 0.0073', '3) 0 0.0067', '4) 2 0.0033'], ['1) 1 0.8801', '2) 2 0.0636', '3) 3 0.0331', '4) 0 0.0232'], ['1) 3 0.9675', '2) 1 0.017', '3) 0 0.0113', '4) 2 0.0041'], ['1) 3 0.623', '2) 0 0.3066', '3) 2 0.0602', '4) 1 0.0101'], ['1) 0 0.6269', '2) 3 0.2988', '3) 2 0.0571', '4) 1 0.0172'], ['1) 2 0.7849', '2) 1 0.1729', '3) 0 0.0245', '4) 3 0.0177'], ['1) 3 0.7484', '2) 1 0.1507', '3) 0 0.0545', '4) 2 0.0464'], ['1) 0 0.9185', '2) 3 0.0537', '3) 2 0.0172', '4) 1 0.0106'], ['1) 3 0.5218', '2) 0 0.2121', '3) 2 0.1709', '4) 1 0.0952'], ['1) 0 0.979', '2) 1 0.0082', '3) 2 0.0074', '4) 3 0.0054'], ['1) 0 0.9583', '2) 1 0.0198', '3) 2 0.0144', '4) 3 0.0075'], ['1) 3 0.8068', '2) 0 0.1415', '3) 2 0.0349', '4) 1 0.0169'], ['1) 0 0.9612', '2) 2 0.0229', '3) 1 0.0087', '4) 3 0.0072'], ['1) 0 0.9664', '2) 3 0.0151', '3) 2 0.0113', '4) 1 0.0072'], ['1) 0 0.9823', '2) 2 0.0077', '3) 1 0.0051', '4) 3 0.0049'], ['1) 3 0.9278', '2) 2 0.0525', '3) 0 0.0112', '4) 1 0.0085'], ['1) 1 0.6487', '2) 2 0.2126', '3) 3 0.0988', '4) 0 0.0399'], ['1) 0 0.9744', '2) 3 0.011', '3) 2 0.0102', '4) 1 0.0044'], ['1) 0 0.8847', '2) 3 0.1016', '3) 1 0.0082', '4) 2 0.0055'], ['1) 1 0.9732', '2) 0 0.0108', '3) 2 0.0091', '4) 3 0.0069'], ['1) 1 0.7681', '2) 2 0.1525', '3) 0 0.045', '4) 3 0.0343'], ['1) 1 0.7575', '2) 2 0.2049', '3) 0 0.0195', '4) 3 0.0181'], ['1) 0 0.4414', '2) 3 0.396', '3) 1 0.1108', '4) 2 0.0518'], ['1) 3 0.9679', '2) 0 0.021', '3) 1 0.0073', '4) 2 0.0038'], ['1) 3 0.9302', '2) 1 0.0446', '3) 0 0.0169', '4) 2 0.0084'], ['1) 3 0.7322', '2) 0 0.2444', '3) 1 0.0165', '4) 2 0.0068'], ['1) 3 0.8682', '2) 1 0.0983', '3) 0 0.0179', '4) 2 0.0156'], ['1) 0 0.4929', '2) 3 0.4674', '3) 1 0.0239', '4) 2 0.0158'], ['1) 0 0.9756', '2) 2 0.0101', '3) 3 0.0091', '4) 1 0.0052'], ['1) 0 0.9597', '2) 2 0.0168', '3) 3 0.0149', '4) 1 0.0086'], ['1) 3 0.6382', '2) 2 0.1899', '3) 0 0.1591', '4) 1 0.0127'], ['1) 1 0.5104', '2) 3 0.306', '3) 0 0.1176', '4) 2 0.066'], ['1) 0 0.9827', '2) 3 0.0077', '3) 2 0.0056', '4) 1 0.004'], ['1) 0 0.9627', '2) 3 0.0154', '3) 2 0.0141', '4) 1 0.0079'], ['1) 3 0.9095', '2) 0 0.0481', '3) 1 0.0278', '4) 2 0.0145'], ['1) 3 0.5922', '2) 1 0.2494', '3) 2 0.1408', '4) 0 0.0176'], ['1) 2 0.9377', '2) 1 0.0286', '3) 0 0.0176', '4) 3 0.016'], ['1) 0 0.9822', '2) 3 0.0068', '3) 1 0.0059', '4) 2 0.0051'], ['1) 0 0.9539', '2) 3 0.0261', '3) 2 0.0119', '4) 1 0.0081'], ['1) 3 0.9233', '2) 2 0.0257', '3) 0 0.0256', '4) 1 0.0255'], ['1) 2 0.8817', '2) 0 0.0679', '3) 3 0.0403', '4) 1 0.01'], ['1) 0 0.487', '2) 3 0.487', '3) 1 0.0173', '4) 2 0.0086'], ['1) 3 0.6032', '2) 0 0.3592', '3) 2 0.0216', '4) 1 0.0161'], ['1) 1 0.9548', '2) 0 0.0232', '3) 2 0.0157', '4) 3 0.0063'], ['1) 3 0.9798', '2) 1 0.0092', '3) 0 0.0073', '4) 2 0.0038'], ['1) 0 0.9293', '2) 3 0.0546', '3) 2 0.0119', '4) 1 0.0042'], ['1) 2 0.7301', '2) 3 0.1267', '3) 1 0.0828', '4) 0 0.0605'], ['1) 0 0.9754', '2) 1 0.0099', '3) 3 0.0079', '4) 2 0.0068'], ['1) 2 0.7726', '2) 0 0.1562', '3) 3 0.055', '4) 1 0.0162'], ['1) 1 0.5938', '2) 3 0.3151', '3) 0 0.058', '4) 2 0.0332'], ['1) 1 0.7959', '2) 0 0.1032', '3) 2 0.056', '4) 3 0.045'], ['1) 1 0.9701', '2) 2 0.0107', '3) 0 0.01', '4) 3 0.0091'], ['1) 0 0.8017', '2) 3 0.1685', '3) 2 0.0229', '4) 1 0.007'], ['1) 3 0.9824', '2) 0 0.0073', '3) 1 0.007', '4) 2 0.0033'], ['1) 0 0.9035', '2) 3 0.0785', '3) 1 0.0126', '4) 2 0.0053'], ['1) 1 0.8736', '2) 2 0.0518', '3) 0 0.0518', '4) 3 0.0228'], ['1) 0 0.5218', '2) 3 0.4395', '3) 2 0.028', '4) 1 0.0107'], ['1) 0 0.9724', '2) 3 0.0139', '3) 1 0.0074', '4) 2 0.0062'], ['1) 1 0.7968', '2) 2 0.1105', '3) 0 0.063', '4) 3 0.0298'], ['1) 1 0.8683', '2) 2 0.1042', '3) 0 0.0154', '4) 3 0.012'], ['1) 2 0.7549', '2) 1 0.1255', '3) 3 0.0754', '4) 0 0.0442'], ['1) 0 0.7842', '2) 3 0.1943', '3) 1 0.0135', '4) 2 0.008'], ['1) 0 0.7675', '2) 2 0.1184', '3) 3 0.0953', '4) 1 0.0187'], ['1) 0 0.9188', '2) 3 0.0384', '3) 1 0.0367', '4) 2 0.0061'], ['1) 1 0.9357', '2) 2 0.0339', '3) 0 0.0191', '4) 3 0.0112'], ['1) 1 0.6213', '2) 2 0.2572', '3) 3 0.0802', '4) 0 0.0413'], ['1) 1 0.9631', '2) 2 0.0149', '3) 0 0.0142', '4) 3 0.0079'], ['1) 0 0.9685', '2) 3 0.0144', '3) 1 0.0109', '4) 2 0.0062'], ['1) 0 0.9792', '2) 3 0.0083', '3) 2 0.0063', '4) 1 0.0062'], ['1) 3 0.9449', '2) 1 0.0276', '3) 0 0.0222', '4) 2 0.0053'], ['1) 3 0.9841', '2) 0 0.0079', '3) 1 0.0049', '4) 2 0.0031'], ['1) 0 0.9651', '2) 3 0.0163', '3) 1 0.0141', '4) 2 0.0046'], ['1) 0 0.8306', '2) 3 0.1517', '3) 2 0.0091', '4) 1 0.0087'], ['1) 0 0.4997', '2) 3 0.4586', '3) 2 0.0288', '4) 1 0.013'], ['1) 3 0.9655', '2) 0 0.0188', '3) 1 0.011', '4) 2 0.0047'], ['1) 1 0.498', '2) 3 0.2954', '3) 2 0.1464', '4) 0 0.0602'], ['1) 0 0.904', '2) 2 0.0658', '3) 3 0.0238', '4) 1 0.0064'], ['1) 0 0.9586', '2) 2 0.0239', '3) 3 0.011', '4) 1 0.0065'], ['1) 2 0.4425', '2) 0 0.2855', '3) 1 0.1624', '4) 3 0.1096'], ['1) 3 0.9741', '2) 1 0.0142', '3) 0 0.0081', '4) 2 0.0037'], ['1) 0 0.8146', '2) 2 0.1332', '3) 3 0.0442', '4) 1 0.008'], ['1) 1 0.841', '2) 2 0.112', '3) 0 0.0245', '4) 3 0.0225'], ['1) 1 0.9047', '2) 3 0.054', '3) 2 0.0257', '4) 0 0.0157'], ['1) 0 0.9031', '2) 2 0.0574', '3) 3 0.0311', '4) 1 0.0085'], ['1) 0 0.9758', '2) 3 0.0117', '3) 2 0.007', '4) 1 0.0054'], ['1) 3 0.9787', '2) 0 0.0094', '3) 1 0.0066', '4) 2 0.0053'], ['1) 2 0.9099', '2) 3 0.0471', '3) 0 0.0261', '4) 1 0.0168'], ['1) 1 0.9204', '2) 2 0.0324', '3) 0 0.0259', '4) 3 0.0213'], ['1) 0 0.9832', '2) 2 0.0066', '3) 3 0.0061', '4) 1 0.0041'], ['1) 0 0.9576', '2) 3 0.0195', '3) 2 0.0179', '4) 1 0.0049'], ['1) 1 0.655', '2) 3 0.2177', '3) 2 0.082', '4) 0 0.0453'], ['1) 1 0.937', '2) 3 0.0275', '3) 2 0.026', '4) 0 0.0094'], ['1) 0 0.9477', '2) 2 0.028', '3) 3 0.0159', '4) 1 0.0084'], ['1) 0 0.9763', '2) 2 0.0109', '3) 3 0.0078', '4) 1 0.005'], ['1) 1 0.932', '2) 2 0.0354', '3) 3 0.0245', '4) 0 0.0082'], ['1) 3 0.9288', '2) 2 0.0372', '3) 0 0.0228', '4) 1 0.0113'], ['1) 1 0.9358', '2) 2 0.0396', '3) 3 0.0141', '4) 0 0.0105'], ['1) 3 0.9769', '2) 0 0.0108', '3) 2 0.0083', '4) 1 0.0041'], ['1) 2 0.8131', '2) 3 0.1261', '3) 0 0.0464', '4) 1 0.0144'], ['1) 3 0.5236', '2) 2 0.3606', '3) 0 0.0887', '4) 1 0.0271'], ['1) 1 0.8379', '2) 3 0.1075', '3) 2 0.0365', '4) 0 0.018'], ['1) 3 0.9792', '2) 2 0.0084', '3) 0 0.0067', '4) 1 0.0057'], ['1) 0 0.9023', '2) 3 0.0735', '3) 1 0.0178', '4) 2 0.0064'], ['1) 0 0.9466', '2) 3 0.0283', '3) 1 0.015', '4) 2 0.0101'], ['1) 3 0.8576', '2) 1 0.1169', '3) 2 0.0165', '4) 0 0.009'], ['1) 0 0.5495', '2) 3 0.2583', '3) 2 0.1831', '4) 1 0.0091'], ['1) 2 0.458', '2) 1 0.2872', '3) 0 0.2273', '4) 3 0.0275'], ['1) 3 0.6653', '2) 1 0.2248', '3) 2 0.0972', '4) 0 0.0128'], ['1) 3 0.972', '2) 0 0.0129', '3) 2 0.0108', '4) 1 0.0043'], ['1) 3 0.9432', '2) 0 0.044', '3) 2 0.0065', '4) 1 0.0062'], ['1) 3 0.98', '2) 1 0.0098', '3) 0 0.006', '4) 2 0.0042'], ['1) 0 0.9466', '2) 3 0.0251', '3) 2 0.0236', '4) 1 0.0047'], ['1) 2 0.9588', '2) 1 0.0156', '3) 3 0.0141', '4) 0 0.0115'], ['1) 3 0.8725', '2) 0 0.0927', '3) 2 0.0204', '4) 1 0.0144'], ['1) 0 0.8984', '2) 2 0.0669', '3) 1 0.0176', '4) 3 0.0171'], ['1) 0 0.9463', '2) 3 0.0284', '3) 2 0.0185', '4) 1 0.0068'], ['1) 0 0.9846', '2) 2 0.006', '3) 1 0.0057', '4) 3 0.0037'], ['1) 1 0.9352', '2) 2 0.034', '3) 3 0.023', '4) 0 0.0078'], ['1) 0 0.9809', '2) 3 0.0096', '3) 2 0.0055', '4) 1 0.0039'], ['1) 0 0.9785', '2) 3 0.0084', '3) 1 0.0075', '4) 2 0.0057'], ['1) 0 0.9002', '2) 3 0.0452', '3) 2 0.0346', '4) 1 0.02'], ['1) 0 0.7728', '2) 3 0.1559', '3) 1 0.0533', '4) 2 0.0179'], ['1) 0 0.9751', '2) 2 0.0095', '3) 3 0.0088', '4) 1 0.0066'], ['1) 3 0.8948', '2) 1 0.0576', '3) 0 0.0393', '4) 2 0.0083'], ['1) 0 0.9724', '2) 2 0.0178', '3) 3 0.0052', '4) 1 0.0046'], ['1) 1 0.8918', '2) 0 0.0732', '3) 2 0.0192', '4) 3 0.0158'], ['1) 1 0.9682', '2) 0 0.013', '3) 3 0.0094', '4) 2 0.0094'], ['1) 0 0.4977', '2) 3 0.4663', '3) 1 0.0236', '4) 2 0.0124'], ['1) 3 0.9356', '2) 0 0.0326', '3) 2 0.0185', '4) 1 0.0133'], ['1) 2 0.6008', '2) 0 0.3372', '3) 3 0.0376', '4) 1 0.0244'], ['1) 1 0.9705', '2) 2 0.0139', '3) 0 0.0079', '4) 3 0.0078'], ['1) 3 0.9813', '2) 0 0.0095', '3) 2 0.0046', '4) 1 0.0046'], ['1) 3 0.9806', '2) 0 0.0099', '3) 1 0.0052', '4) 2 0.0042'], ['1) 3 0.9745', '2) 1 0.0095', '3) 0 0.0095', '4) 2 0.0065'], ['1) 3 0.9764', '2) 0 0.0098', '3) 1 0.0091', '4) 2 0.0047'], ['1) 1 0.9661', '2) 2 0.0134', '3) 0 0.0115', '4) 3 0.009'], ['1) 1 0.9076', '2) 2 0.0542', '3) 3 0.029', '4) 0 0.0092'], ['1) 3 0.9819', '2) 1 0.0073', '3) 0 0.0066', '4) 2 0.0041'], ['1) 0 0.9817', '2) 3 0.0079', '3) 2 0.0061', '4) 1 0.0044'], ['1) 0 0.947', '2) 2 0.0284', '3) 1 0.0129', '4) 3 0.0116'], ['1) 3 0.9448', '2) 0 0.0326', '3) 1 0.0154', '4) 2 0.0073'], ['1) 0 0.9732', '2) 3 0.0139', '3) 1 0.0095', '4) 2 0.0035'], ['1) 1 0.9553', '2) 2 0.0259', '3) 3 0.0094', '4) 0 0.0093'], ['1) 1 0.8194', '2) 2 0.1361', '3) 0 0.0295', '4) 3 0.015'], ['1) 3 0.5269', '2) 0 0.3318', '3) 1 0.096', '4) 2 0.0453'], ['1) 0 0.8455', '2) 2 0.0873', '3) 3 0.0463', '4) 1 0.0209'], ['1) 0 0.9317', '2) 3 0.0388', '3) 2 0.025', '4) 1 0.0044'], ['1) 1 0.9307', '2) 3 0.0315', '3) 2 0.0292', '4) 0 0.0086'], ['1) 0 0.9682', '2) 3 0.0143', '3) 2 0.0115', '4) 1 0.006'], ['1) 1 0.4242', '2) 2 0.4209', '3) 0 0.0937', '4) 3 0.0613'], ['1) 0 0.9721', '2) 1 0.0105', '3) 2 0.0096', '4) 3 0.0079'], ['1) 2 0.923', '2) 0 0.0409', '3) 3 0.0202', '4) 1 0.016'], ['1) 0 0.8006', '2) 3 0.1787', '3) 1 0.0108', '4) 2 0.0099'], ['1) 3 0.9235', '2) 1 0.0411', '3) 0 0.029', '4) 2 0.0064'], ['1) 1 0.9144', '2) 2 0.0495', '3) 0 0.0232', '4) 3 0.013'], ['1) 0 0.9282', '2) 2 0.0403', '3) 3 0.0242', '4) 1 0.0073'], ['1) 3 0.97', '2) 0 0.0202', '3) 1 0.005', '4) 2 0.0047'], ['1) 0 0.9801', '2) 3 0.0111', '3) 1 0.0046', '4) 2 0.0043'], ['1) 0 0.8529', '2) 3 0.0974', '3) 2 0.0399', '4) 1 0.0098'], ['1) 3 0.9103', '2) 0 0.0539', '3) 1 0.0263', '4) 2 0.0095'], ['1) 3 0.9237', '2) 0 0.0517', '3) 2 0.0185', '4) 1 0.0061'], ['1) 1 0.3798', '2) 3 0.3095', '3) 0 0.2638', '4) 2 0.0468'], ['1) 0 0.9707', '2) 2 0.0156', '3) 3 0.0079', '4) 1 0.0059'], ['1) 1 0.7837', '2) 3 0.0777', '3) 0 0.0715', '4) 2 0.0671'], ['1) 3 0.984', '2) 0 0.0072', '3) 2 0.0047', '4) 1 0.0041'], ['1) 3 0.8917', '2) 0 0.0505', '3) 1 0.0302', '4) 2 0.0276'], ['1) 3 0.9406', '2) 1 0.0466', '3) 2 0.0073', '4) 0 0.0055'], ['1) 0 0.9213', '2) 3 0.0615', '3) 2 0.0091', '4) 1 0.008'], ['1) 0 0.977', '2) 2 0.009', '3) 1 0.0082', '4) 3 0.0058'], ['1) 2 0.8498', '2) 3 0.1121', '3) 0 0.0252', '4) 1 0.0129'], ['1) 1 0.8238', '2) 0 0.1079', '3) 3 0.046', '4) 2 0.0223'], ['1) 0 0.9809', '2) 2 0.0091', '3) 3 0.0052', '4) 1 0.0048'], ['1) 0 0.9676', '2) 1 0.0176', '3) 3 0.0095', '4) 2 0.0053'], ['1) 3 0.4955', '2) 0 0.4825', '3) 1 0.0152', '4) 2 0.0069'], ['1) 1 0.9613', '2) 2 0.018', '3) 0 0.0115', '4) 3 0.0092'], ['1) 0 0.4609', '2) 3 0.2807', '3) 1 0.2412', '4) 2 0.0172'], ['1) 0 0.9817', '2) 3 0.0093', '3) 1 0.0048', '4) 2 0.0042'], ['1) 0 0.9835', '2) 1 0.0066', '3) 2 0.0051', '4) 3 0.0047'], ['1) 3 0.6768', '2) 0 0.2818', '3) 2 0.0299', '4) 1 0.0116'], ['1) 3 0.9841', '2) 1 0.0064', '3) 0 0.0063', '4) 2 0.0032'], ['1) 1 0.5239', '2) 2 0.2922', '3) 0 0.1116', '4) 3 0.0723'], ['1) 3 0.9839', '2) 1 0.0077', '3) 0 0.005', '4) 2 0.0034'], ['1) 2 0.941', '2) 1 0.0229', '3) 0 0.021', '4) 3 0.0151'], ['1) 1 0.9564', '2) 2 0.0232', '3) 3 0.0117', '4) 0 0.0086'], ['1) 3 0.9716', '2) 2 0.0101', '3) 1 0.0092', '4) 0 0.009'], ['1) 3 0.7438', '2) 0 0.2032', '3) 2 0.0296', '4) 1 0.0234'], ['1) 0 0.976', '2) 2 0.0107', '3) 3 0.0067', '4) 1 0.0066'], ['1) 1 0.4275', '2) 3 0.3362', '3) 2 0.1869', '4) 0 0.0493'], ['1) 0 0.9669', '2) 2 0.0174', '3) 3 0.01', '4) 1 0.0056'], ['1) 0 0.9807', '2) 3 0.0076', '3) 2 0.0063', '4) 1 0.0053'], ['1) 3 0.9445', '2) 0 0.0422', '3) 1 0.0084', '4) 2 0.0049'], ['1) 2 0.5888', '2) 1 0.2295', '3) 3 0.1364', '4) 0 0.0453'], ['1) 3 0.9621', '2) 0 0.0266', '3) 1 0.0065', '4) 2 0.0048'], ['1) 0 0.9614', '2) 3 0.0258', '3) 2 0.0095', '4) 1 0.0033'], ['1) 1 0.9656', '2) 2 0.0148', '3) 3 0.0117', '4) 0 0.0079'], ['1) 3 0.9242', '2) 0 0.032', '3) 2 0.0315', '4) 1 0.0123'], ['1) 3 0.9701', '2) 1 0.0117', '3) 2 0.0099', '4) 0 0.0082'], ['1) 0 0.8918', '2) 3 0.0914', '3) 2 0.0107', '4) 1 0.006'], ['1) 3 0.8944', '2) 0 0.0515', '3) 1 0.0281', '4) 2 0.026'], ['1) 0 0.9836', '2) 3 0.0063', '3) 2 0.0062', '4) 1 0.0038'], ['1) 3 0.521', '2) 1 0.3801', '3) 0 0.0807', '4) 2 0.0182'], ['1) 0 0.937', '2) 2 0.0309', '3) 3 0.0262', '4) 1 0.0059'], ['1) 1 0.9571', '2) 0 0.0165', '3) 2 0.0141', '4) 3 0.0122'], ['1) 3 0.9766', '2) 2 0.0123', '3) 0 0.0068', '4) 1 0.0043'], ['1) 0 0.5362', '2) 3 0.4362', '3) 1 0.0138', '4) 2 0.0137'], ['1) 3 0.9664', '2) 0 0.0173', '3) 2 0.0087', '4) 1 0.0076'], ['1) 0 0.9785', '2) 3 0.0094', '3) 1 0.0079', '4) 2 0.0042'], ['1) 3 0.4757', '2) 2 0.1929', '3) 0 0.1672', '4) 1 0.1642'], ['1) 3 0.9807', '2) 0 0.0082', '3) 1 0.0067', '4) 2 0.0044'], ['1) 0 0.967', '2) 3 0.0155', '3) 2 0.0134', '4) 1 0.004'], ['1) 0 0.8682', '2) 3 0.0686', '3) 2 0.0492', '4) 1 0.014'], ['1) 1 0.9342', '2) 2 0.0409', '3) 3 0.0149', '4) 0 0.01'], ['1) 0 0.9604', '2) 2 0.0194', '3) 3 0.0135', '4) 1 0.0067'], ['1) 0 0.9419', '2) 3 0.0332', '3) 2 0.0152', '4) 1 0.0098'], ['1) 0 0.9622', '2) 2 0.0177', '3) 1 0.0108', '4) 3 0.0094'], ['1) 3 0.8615', '2) 2 0.0996', '3) 0 0.0312', '4) 1 0.0077'], ['1) 2 0.8976', '2) 0 0.051', '3) 3 0.0371', '4) 1 0.0143'], ['1) 3 0.9733', '2) 0 0.0177', '3) 1 0.0047', '4) 2 0.0043'], ['1) 1 0.3304', '2) 2 0.2844', '3) 0 0.2673', '4) 3 0.1178'], ['1) 2 0.6469', '2) 1 0.2032', '3) 0 0.1093', '4) 3 0.0405'], ['1) 2 0.5032', '2) 3 0.385', '3) 1 0.076', '4) 0 0.0359'], ['1) 0 0.6865', '2) 3 0.264', '3) 2 0.0414', '4) 1 0.0081'], ['1) 3 0.7019', '2) 2 0.2078', '3) 0 0.0807', '4) 1 0.0096'], ['1) 1 0.9646', '2) 2 0.0153', '3) 3 0.0108', '4) 0 0.0093'], ['1) 1 0.6347', '2) 3 0.24', '3) 2 0.1044', '4) 0 0.0209'], ['1) 3 0.889', '2) 0 0.0843', '3) 1 0.0155', '4) 2 0.0112'], ['1) 1 0.9604', '2) 2 0.0204', '3) 3 0.0104', '4) 0 0.0088'], ['1) 2 0.8497', '2) 0 0.0682', '3) 3 0.0546', '4) 1 0.0274'], ['1) 2 0.5891', '2) 1 0.2941', '3) 0 0.0913', '4) 3 0.0254'], ['1) 3 0.9807', '2) 1 0.0079', '3) 0 0.0074', '4) 2 0.004'], ['1) 0 0.9743', '2) 1 0.01', '3) 2 0.0093', '4) 3 0.0064'], ['1) 1 0.7824', '2) 2 0.1559', '3) 3 0.0495', '4) 0 0.0122'], ['1) 1 0.9725', '2) 3 0.0108', '3) 2 0.01', '4) 0 0.0067'], ['1) 0 0.504', '2) 2 0.3919', '3) 3 0.0549', '4) 1 0.0492'], ['1) 0 0.945', '2) 2 0.0277', '3) 3 0.0187', '4) 1 0.0085'], ['1) 3 0.9608', '2) 1 0.0146', '3) 2 0.0138', '4) 0 0.0108'], ['1) 0 0.9802', '2) 2 0.0097', '3) 3 0.0068', '4) 1 0.0034'], ['1) 1 0.9618', '2) 2 0.0139', '3) 0 0.0129', '4) 3 0.0114'], ['1) 3 0.9793', '2) 1 0.0113', '3) 0 0.0056', '4) 2 0.0037'], ['1) 1 0.4475', '2) 3 0.4049', '3) 0 0.1198', '4) 2 0.0278'], ['1) 0 0.9782', '2) 3 0.0083', '3) 1 0.0069', '4) 2 0.0066'], ['1) 3 0.9553', '2) 1 0.0238', '3) 2 0.0119', '4) 0 0.009'], ['1) 0 0.9793', '2) 2 0.0089', '3) 3 0.0059', '4) 1 0.0059'], ['1) 1 0.944', '2) 2 0.0327', '3) 3 0.0148', '4) 0 0.0085'], ['1) 0 0.9027', '2) 3 0.0399', '3) 1 0.0388', '4) 2 0.0186'], ['1) 0 0.7361', '2) 2 0.1316', '3) 3 0.1106', '4) 1 0.0217'], ['1) 1 0.9222', '2) 3 0.0397', '3) 2 0.0255', '4) 0 0.0126'], ['1) 3 0.9021', '2) 0 0.0754', '3) 2 0.0115', '4) 1 0.011'], ['1) 0 0.9792', '2) 2 0.0077', '3) 3 0.007', '4) 1 0.0061'], ['1) 0 0.9611', '2) 2 0.0153', '3) 3 0.0126', '4) 1 0.011'], ['1) 3 0.596', '2) 0 0.3699', '3) 1 0.0173', '4) 2 0.0168'], ['1) 0 0.7046', '2) 3 0.2398', '3) 1 0.0436', '4) 2 0.012'], ['1) 2 0.9404', '2) 3 0.0291', '3) 0 0.0181', '4) 1 0.0124'], ['1) 0 0.9523', '2) 3 0.0217', '3) 1 0.018', '4) 2 0.0079'], ['1) 0 0.9381', '2) 2 0.0376', '3) 3 0.0124', '4) 1 0.012'], ['1) 1 0.9665', '2) 2 0.0144', '3) 0 0.0113', '4) 3 0.0078'], ['1) 2 0.5186', '2) 1 0.2976', '3) 3 0.1357', '4) 0 0.048'], ['1) 2 0.9115', '2) 3 0.0556', '3) 0 0.0212', '4) 1 0.0118'], ['1) 0 0.5273', '2) 3 0.4493', '3) 1 0.0143', '4) 2 0.0091'], ['1) 1 0.9612', '2) 0 0.0164', '3) 2 0.0161', '4) 3 0.0063'], ['1) 0 0.9817', '2) 3 0.0083', '3) 2 0.0053', '4) 1 0.0047'], ['1) 2 0.649', '2) 3 0.2019', '3) 0 0.1339', '4) 1 0.0152'], ['1) 3 0.9685', '2) 1 0.0194', '3) 0 0.0075', '4) 2 0.0046'], ['1) 1 0.937', '2) 2 0.0242', '3) 0 0.0241', '4) 3 0.0147'], ['1) 3 0.9812', '2) 0 0.0094', '3) 1 0.0057', '4) 2 0.0037'], ['1) 0 0.8781', '2) 3 0.0937', '3) 1 0.0174', '4) 2 0.0109'], ['1) 2 0.9016', '2) 3 0.0491', '3) 0 0.0375', '4) 1 0.0119'], ['1) 3 0.9835', '2) 0 0.0073', '3) 1 0.0058', '4) 2 0.0034'], ['1) 3 0.5272', '2) 0 0.3972', '3) 1 0.0593', '4) 2 0.0164'], ['1) 3 0.9407', '2) 1 0.0444', '3) 0 0.0094', '4) 2 0.0055'], ['1) 1 0.532', '2) 3 0.2876', '3) 2 0.103', '4) 0 0.0775'], ['1) 3 0.9717', '2) 0 0.0119', '3) 1 0.0117', '4) 2 0.0047'], ['1) 3 0.983', '2) 2 0.0063', '3) 1 0.0056', '4) 0 0.0051'], ['1) 1 0.8793', '2) 2 0.0637', '3) 0 0.0314', '4) 3 0.0255'], ['1) 0 0.9152', '2) 2 0.0386', '3) 3 0.038', '4) 1 0.0083'], ['1) 2 0.9446', '2) 3 0.0273', '3) 1 0.0144', '4) 0 0.0136'], ['1) 0 0.9694', '2) 2 0.0135', '3) 3 0.0109', '4) 1 0.0063'], ['1) 1 0.9646', '2) 0 0.0143', '3) 2 0.0132', '4) 3 0.0079'], ['1) 0 0.9669', '2) 2 0.0155', '3) 1 0.0115', '4) 3 0.0061'], ['1) 3 0.9561', '2) 2 0.025', '3) 1 0.01', '4) 0 0.0089'], ['1) 1 0.8963', '2) 2 0.0608', '3) 0 0.0319', '4) 3 0.0109'], ['1) 0 0.8402', '2) 1 0.0605', '3) 2 0.0504', '4) 3 0.0489'], ['1) 0 0.6945', '2) 3 0.2301', '3) 2 0.0608', '4) 1 0.0146'], ['1) 1 0.7162', '2) 2 0.0995', '3) 0 0.0948', '4) 3 0.0895'], ['1) 1 0.8596', '2) 3 0.0878', '3) 2 0.0362', '4) 0 0.0164'], ['1) 3 0.9432', '2) 0 0.0324', '3) 2 0.0126', '4) 1 0.0117'], ['1) 0 0.9056', '2) 3 0.0823', '3) 2 0.0074', '4) 1 0.0046'], ['1) 0 0.9562', '2) 3 0.024', '3) 2 0.0145', '4) 1 0.0052'], ['1) 0 0.9725', '2) 1 0.0098', '3) 2 0.0098', '4) 3 0.0079'], ['1) 0 0.8124', '2) 3 0.1193', '3) 1 0.0556', '4) 2 0.0128'], ['1) 2 0.9253', '2) 1 0.0361', '3) 0 0.0203', '4) 3 0.0183'], ['1) 3 0.983', '2) 0 0.0076', '3) 1 0.0057', '4) 2 0.0038'], ['1) 0 0.947', '2) 3 0.0404', '3) 1 0.0077', '4) 2 0.0049'], ['1) 1 0.9617', '2) 2 0.0173', '3) 0 0.0112', '4) 3 0.0098'], ['1) 0 0.9847', '2) 3 0.0057', '3) 1 0.0049', '4) 2 0.0047'], ['1) 1 0.858', '2) 2 0.0787', '3) 3 0.0337', '4) 0 0.0296'], ['1) 3 0.9824', '2) 0 0.0087', '3) 2 0.0046', '4) 1 0.0043'], ['1) 0 0.9485', '2) 3 0.0427', '3) 1 0.0046', '4) 2 0.0042'], ['1) 1 0.8668', '2) 2 0.0615', '3) 3 0.0359', '4) 0 0.0358'], ['1) 1 0.9584', '2) 2 0.0171', '3) 0 0.0133', '4) 3 0.0112'], ['1) 1 0.9025', '2) 3 0.0445', '3) 2 0.027', '4) 0 0.0261'], ['1) 1 0.9466', '2) 2 0.0283', '3) 0 0.0159', '4) 3 0.0092'], ['1) 3 0.9686', '2) 0 0.0182', '3) 2 0.0072', '4) 1 0.0059'], ['1) 0 0.9359', '2) 2 0.0332', '3) 3 0.0219', '4) 1 0.009'], ['1) 0 0.9744', '2) 3 0.0117', '3) 2 0.0084', '4) 1 0.0055'], ['1) 2 0.9456', '2) 1 0.0203', '3) 0 0.0173', '4) 3 0.0168'], ['1) 1 0.9703', '2) 2 0.0133', '3) 0 0.0091', '4) 3 0.0072'], ['1) 1 0.5324', '2) 2 0.3871', '3) 3 0.0413', '4) 0 0.0393'], ['1) 0 0.9737', '2) 3 0.0115', '3) 2 0.0079', '4) 1 0.0069'], ['1) 3 0.9534', '2) 1 0.0312', '3) 2 0.0101', '4) 0 0.0054'], ['1) 1 0.7778', '2) 0 0.0921', '3) 3 0.0895', '4) 2 0.0407'], ['1) 0 0.9805', '2) 2 0.0073', '3) 1 0.0062', '4) 3 0.0061'], ['1) 3 0.7696', '2) 0 0.1412', '3) 1 0.0481', '4) 2 0.0411'], ['1) 1 0.896', '2) 2 0.0602', '3) 0 0.0296', '4) 3 0.0142'], ['1) 3 0.9761', '2) 0 0.0104', '3) 2 0.0091', '4) 1 0.0044'], ['1) 3 0.5727', '2) 0 0.4068', '3) 1 0.0147', '4) 2 0.0058'], ['1) 3 0.9694', '2) 0 0.0188', '3) 2 0.0065', '4) 1 0.0053'], ['1) 1 0.3541', '2) 3 0.3388', '3) 0 0.2581', '4) 2 0.0491'], ['1) 3 0.6519', '2) 0 0.3238', '3) 1 0.0143', '4) 2 0.01'], ['1) 1 0.9567', '2) 2 0.0217', '3) 0 0.0117', '4) 3 0.0099'], ['1) 2 0.9134', '2) 3 0.0444', '3) 1 0.0308', '4) 0 0.0114'], ['1) 3 0.7929', '2) 2 0.1305', '3) 1 0.0592', '4) 0 0.0174'], ['1) 1 0.8192', '2) 3 0.1388', '3) 2 0.0267', '4) 0 0.0154'], ['1) 0 0.9772', '2) 2 0.0098', '3) 3 0.0072', '4) 1 0.0057'], ['1) 0 0.8748', '2) 1 0.0651', '3) 3 0.0486', '4) 2 0.0115'], ['1) 3 0.7731', '2) 0 0.1686', '3) 1 0.0376', '4) 2 0.0208'], ['1) 1 0.9336', '2) 2 0.0402', '3) 0 0.0159', '4) 3 0.0103'], ['1) 0 0.8736', '2) 3 0.1016', '3) 1 0.0194', '4) 2 0.0054'], ['1) 3 0.9843', '2) 0 0.0064', '3) 1 0.0057', '4) 2 0.0036'], ['1) 3 0.9729', '2) 1 0.0115', '3) 0 0.0094', '4) 2 0.0063'], ['1) 0 0.7245', '2) 3 0.2543', '3) 2 0.014', '4) 1 0.0072'], ['1) 2 0.926', '2) 1 0.0336', '3) 3 0.0256', '4) 0 0.0148'], ['1) 3 0.9665', '2) 0 0.0207', '3) 1 0.0068', '4) 2 0.0059'], ['1) 3 0.598', '2) 1 0.2687', '3) 2 0.1145', '4) 0 0.0188'], ['1) 0 0.9017', '2) 3 0.0635', '3) 2 0.0186', '4) 1 0.0162'], ['1) 3 0.9294', '2) 0 0.039', '3) 2 0.0245', '4) 1 0.007'], ['1) 3 0.6564', '2) 1 0.2917', '3) 2 0.0264', '4) 0 0.0255'], ['1) 0 0.9', '2) 3 0.0852', '3) 2 0.0095', '4) 1 0.0052'], ['1) 0 0.9724', '2) 2 0.0124', '3) 1 0.0076', '4) 3 0.0076'], ['1) 1 0.4744', '2) 2 0.3954', '3) 3 0.0675', '4) 0 0.0627'], ['1) 3 0.8089', '2) 0 0.1333', '3) 2 0.0403', '4) 1 0.0176'], ['1) 2 0.8968', '2) 3 0.0368', '3) 0 0.0362', '4) 1 0.0302'], ['1) 3 0.7782', '2) 1 0.1162', '3) 0 0.0886', '4) 2 0.017'], ['1) 0 0.8847', '2) 3 0.0715', '3) 2 0.0255', '4) 1 0.0183'], ['1) 3 0.6235', '2) 2 0.3379', '3) 0 0.0275', '4) 1 0.0111'], ['1) 0 0.7692', '2) 2 0.1599', '3) 3 0.0599', '4) 1 0.011'], ['1) 0 0.9491', '2) 3 0.0315', '3) 1 0.0154', '4) 2 0.004'], ['1) 0 0.6433', '2) 3 0.2786', '3) 2 0.0474', '4) 1 0.0306'], ['1) 1 0.9534', '2) 3 0.0199', '3) 2 0.0135', '4) 0 0.0131'], ['1) 1 0.9672', '2) 2 0.0128', '3) 0 0.0102', '4) 3 0.0099'], ['1) 3 0.9604', '2) 0 0.0259', '3) 1 0.0089', '4) 2 0.0047'], ['1) 1 0.9601', '2) 2 0.0166', '3) 0 0.0136', '4) 3 0.0097'], ['1) 0 0.9501', '2) 3 0.0372', '3) 1 0.007', '4) 2 0.0057'], ['1) 0 0.9694', '2) 2 0.0131', '3) 1 0.0092', '4) 3 0.0083'], ['1) 1 0.9111', '2) 2 0.0423', '3) 0 0.0346', '4) 3 0.0121'], ['1) 1 0.9539', '2) 0 0.0177', '3) 2 0.0172', '4) 3 0.0112'], ['1) 0 0.9806', '2) 2 0.0077', '3) 1 0.0073', '4) 3 0.0044'], ['1) 3 0.978', '2) 1 0.0122', '3) 0 0.0051', '4) 2 0.0047'], ['1) 0 0.9633', '2) 3 0.0158', '3) 2 0.0157', '4) 1 0.0052'], ['1) 0 0.9799', '2) 3 0.0089', '3) 2 0.0068', '4) 1 0.0045'], ['1) 0 0.4176', '2) 1 0.3472', '3) 3 0.2003', '4) 2 0.0349'], ['1) 1 0.9142', '2) 2 0.0419', '3) 3 0.0289', '4) 0 0.015'], ['1) 1 0.589', '2) 3 0.3362', '3) 0 0.0521', '4) 2 0.0228'], ['1) 0 0.6232', '2) 3 0.2966', '3) 2 0.0653', '4) 1 0.015'], ['1) 3 0.9635', '2) 0 0.0223', '3) 2 0.0072', '4) 1 0.0069'], ['1) 0 0.9489', '2) 2 0.0347', '3) 3 0.0119', '4) 1 0.0044'], ['1) 0 0.9503', '2) 2 0.0306', '3) 3 0.0112', '4) 1 0.0079'], ['1) 1 0.9681', '2) 0 0.0132', '3) 3 0.0098', '4) 2 0.0089'], ['1) 1 0.9208', '2) 2 0.0518', '3) 3 0.0154', '4) 0 0.0119'], ['1) 3 0.9863', '2) 1 0.0053', '3) 0 0.0051', '4) 2 0.0034'], ['1) 3 0.8313', '2) 0 0.1516', '3) 1 0.0087', '4) 2 0.0083'], ['1) 3 0.9314', '2) 1 0.034', '3) 0 0.0183', '4) 2 0.0164'], ['1) 0 0.9728', '2) 3 0.0141', '3) 2 0.0084', '4) 1 0.0047'], ['1) 1 0.9663', '2) 0 0.0143', '3) 2 0.0117', '4) 3 0.0078'], ['1) 1 0.9691', '2) 0 0.0115', '3) 3 0.0101', '4) 2 0.0093'], ['1) 1 0.936', '2) 2 0.0387', '3) 3 0.0132', '4) 0 0.0121'], ['1) 1 0.9526', '2) 0 0.0305', '3) 2 0.0095', '4) 3 0.0074'], ['1) 3 0.9303', '2) 0 0.0486', '3) 1 0.0154', '4) 2 0.0057'], ['1) 3 0.9797', '2) 0 0.01', '3) 2 0.006', '4) 1 0.0042'], ['1) 3 0.5043', '2) 1 0.4614', '3) 2 0.0214', '4) 0 0.0128'], ['1) 2 0.9559', '2) 0 0.0153', '3) 1 0.0147', '4) 3 0.0141'], ['1) 0 0.921', '2) 2 0.0474', '3) 3 0.0204', '4) 1 0.0112'], ['1) 3 0.5469', '2) 1 0.2977', '3) 2 0.1169', '4) 0 0.0384'], ['1) 1 0.9487', '2) 2 0.0265', '3) 3 0.0133', '4) 0 0.0115'], ['1) 0 0.9479', '2) 2 0.0238', '3) 3 0.0166', '4) 1 0.0117'], ['1) 1 0.7235', '2) 2 0.1631', '3) 3 0.1026', '4) 0 0.0109'], ['1) 3 0.9845', '2) 1 0.0062', '3) 0 0.0057', '4) 2 0.0035'], ['1) 3 0.9596', '2) 0 0.0216', '3) 1 0.0124', '4) 2 0.0065'], ['1) 3 0.986', '2) 1 0.0064', '3) 0 0.0046', '4) 2 0.0031'], ['1) 0 0.9823', '2) 2 0.0073', '3) 3 0.0055', '4) 1 0.0049'], ['1) 2 0.9241', '2) 0 0.0347', '3) 3 0.0301', '4) 1 0.011'], ['1) 3 0.982', '2) 1 0.0077', '3) 0 0.0064', '4) 2 0.004'], ['1) 0 0.9619', '2) 3 0.0247', '3) 2 0.01', '4) 1 0.0034'], ['1) 0 0.9737', '2) 1 0.0107', '3) 3 0.0103', '4) 2 0.0053'], ['1) 0 0.9556', '2) 3 0.0218', '3) 2 0.0166', '4) 1 0.0059'], ['1) 0 0.4951', '2) 3 0.4623', '3) 2 0.0314', '4) 1 0.0112'], ['1) 2 0.7751', '2) 3 0.1468', '3) 0 0.0551', '4) 1 0.023'], ['1) 1 0.9339', '2) 2 0.0379', '3) 3 0.0163', '4) 0 0.012'], ['1) 1 0.8501', '2) 0 0.0523', '3) 2 0.0492', '4) 3 0.0485'], ['1) 0 0.9624', '2) 2 0.0213', '3) 3 0.0119', '4) 1 0.0044'], ['1) 0 0.9661', '2) 3 0.0232', '3) 2 0.007', '4) 1 0.0038'], ['1) 3 0.9781', '2) 1 0.0096', '3) 0 0.007', '4) 2 0.0052'], ['1) 0 0.8504', '2) 3 0.0924', '3) 1 0.0314', '4) 2 0.0259'], ['1) 0 0.6462', '2) 2 0.2634', '3) 3 0.0742', '4) 1 0.0162'], ['1) 0 0.968', '2) 2 0.0153', '3) 3 0.0119', '4) 1 0.0049'], ['1) 2 0.3813', '2) 1 0.2606', '3) 3 0.2215', '4) 0 0.1365'], ['1) 2 0.7145', '2) 0 0.2075', '3) 1 0.0418', '4) 3 0.0363'], ['1) 0 0.9268', '2) 3 0.0624', '3) 1 0.0057', '4) 2 0.0051'], ['1) 1 0.8939', '2) 3 0.0488', '3) 0 0.0303', '4) 2 0.027'], ['1) 0 0.9781', '2) 1 0.0082', '3) 2 0.0081', '4) 3 0.0056'], ['1) 3 0.9646', '2) 2 0.0205', '3) 0 0.0075', '4) 1 0.0073'], ['1) 1 0.6103', '2) 3 0.1682', '3) 0 0.1321', '4) 2 0.0894'], ['1) 0 0.8494', '2) 2 0.0817', '3) 3 0.0617', '4) 1 0.0072'], ['1) 0 0.9252', '2) 3 0.0351', '3) 2 0.0324', '4) 1 0.0074'], ['1) 0 0.9532', '2) 2 0.0218', '3) 3 0.0149', '4) 1 0.0102'], ['1) 3 0.472', '2) 1 0.4444', '3) 0 0.0428', '4) 2 0.0408'], ['1) 3 0.9703', '2) 2 0.0167', '3) 0 0.007', '4) 1 0.0061'], ['1) 0 0.9257', '2) 3 0.057', '3) 2 0.0094', '4) 1 0.0079'], ['1) 3 0.7454', '2) 1 0.1594', '3) 0 0.0702', '4) 2 0.025'], ['1) 1 0.7991', '2) 2 0.125', '3) 3 0.0532', '4) 0 0.0228'], ['1) 3 0.9339', '2) 0 0.0459', '3) 2 0.0122', '4) 1 0.008'], ['1) 1 0.9', '2) 2 0.0543', '3) 3 0.0271', '4) 0 0.0186'], ['1) 0 0.9433', '2) 3 0.0386', '3) 1 0.0102', '4) 2 0.0079'], ['1) 1 0.9434', '2) 2 0.0249', '3) 0 0.0181', '4) 3 0.0136'], ['1) 0 0.9759', '2) 3 0.011', '3) 2 0.0088', '4) 1 0.0043'], ['1) 1 0.8273', '2) 2 0.1335', '3) 0 0.024', '4) 3 0.0152'], ['1) 1 0.9715', '2) 0 0.0109', '3) 2 0.0089', '4) 3 0.0087'], ['1) 3 0.9839', '2) 0 0.0075', '3) 1 0.0049', '4) 2 0.0037'], ['1) 0 0.9757', '2) 3 0.0115', '3) 2 0.007', '4) 1 0.0057'], ['1) 1 0.5757', '2) 3 0.185', '3) 0 0.1702', '4) 2 0.069'], ['1) 0 0.9806', '2) 2 0.0086', '3) 3 0.0056', '4) 1 0.0052'], ['1) 3 0.9048', '2) 0 0.0788', '3) 1 0.0116', '4) 2 0.0048'], ['1) 0 0.8247', '2) 3 0.1136', '3) 2 0.0525', '4) 1 0.0092'], ['1) 3 0.8436', '2) 0 0.0973', '3) 2 0.0347', '4) 1 0.0243'], ['1) 3 0.8802', '2) 0 0.1005', '3) 1 0.01', '4) 2 0.0093'], ['1) 3 0.9153', '2) 0 0.0477', '3) 2 0.0222', '4) 1 0.0148'], ['1) 0 0.9361', '2) 2 0.0368', '3) 3 0.0152', '4) 1 0.0118'], ['1) 1 0.9611', '2) 2 0.0168', '3) 3 0.0113', '4) 0 0.0108'], ['1) 3 0.9606', '2) 1 0.023', '3) 0 0.0086', '4) 2 0.0078'], ['1) 2 0.6288', '2) 1 0.2845', '3) 0 0.0442', '4) 3 0.0426'], ['1) 0 0.7864', '2) 2 0.1453', '3) 3 0.0456', '4) 1 0.0226'], ['1) 0 0.9761', '2) 2 0.0084', '3) 3 0.0083', '4) 1 0.0072'], ['1) 2 0.7507', '2) 3 0.1613', '3) 0 0.0696', '4) 1 0.0184'], ['1) 0 0.9631', '2) 3 0.0283', '3) 2 0.0048', '4) 1 0.0038'], ['1) 1 0.962', '2) 2 0.016', '3) 3 0.0124', '4) 0 0.0096'], ['1) 1 0.9107', '2) 2 0.0438', '3) 0 0.0289', '4) 3 0.0166'], ['1) 3 0.9784', '2) 1 0.0107', '3) 0 0.0077', '4) 2 0.0032'], ['1) 1 0.9555', '2) 2 0.0184', '3) 3 0.0172', '4) 0 0.0089'], ['1) 1 0.5985', '2) 0 0.1938', '3) 2 0.1231', '4) 3 0.0845'], ['1) 1 0.7344', '2) 2 0.192', '3) 3 0.048', '4) 0 0.0257'], ['1) 1 0.9686', '2) 2 0.0129', '3) 0 0.012', '4) 3 0.0066'], ['1) 1 0.9572', '2) 3 0.0191', '3) 2 0.0158', '4) 0 0.0079'], ['1) 0 0.8798', '2) 3 0.1008', '3) 1 0.0104', '4) 2 0.009'], ['1) 0 0.9343', '2) 1 0.0275', '3) 2 0.0224', '4) 3 0.0158'], ['1) 0 0.9812', '2) 3 0.0089', '3) 2 0.0056', '4) 1 0.0042'], ['1) 0 0.9546', '2) 2 0.0247', '3) 3 0.0121', '4) 1 0.0086'], ['1) 3 0.9853', '2) 0 0.0054', '3) 2 0.0049', '4) 1 0.0044'], ['1) 3 0.982', '2) 1 0.0076', '3) 2 0.0054', '4) 0 0.005'], ['1) 1 0.955', '2) 2 0.0207', '3) 3 0.0142', '4) 0 0.0101'], ['1) 1 0.5966', '2) 2 0.3018', '3) 3 0.0627', '4) 0 0.039'], ['1) 1 0.9568', '2) 2 0.0196', '3) 0 0.0129', '4) 3 0.0107'], ['1) 1 0.9087', '2) 0 0.0437', '3) 2 0.0267', '4) 3 0.021'], ['1) 2 0.88', '2) 0 0.0572', '3) 3 0.0511', '4) 1 0.0117'], ['1) 0 0.9115', '2) 3 0.0605', '3) 2 0.0169', '4) 1 0.0111'], ['1) 1 0.943', '2) 2 0.0316', '3) 0 0.0157', '4) 3 0.0096'], ['1) 3 0.9815', '2) 1 0.0101', '3) 0 0.0043', '4) 2 0.0041'], ['1) 0 0.9721', '2) 2 0.0135', '3) 1 0.0078', '4) 3 0.0066'], ['1) 3 0.4914', '2) 0 0.4602', '3) 1 0.036', '4) 2 0.0123'], ['1) 0 0.9492', '2) 3 0.0293', '3) 2 0.0148', '4) 1 0.0067'], ['1) 1 0.9193', '2) 0 0.0343', '3) 3 0.0335', '4) 2 0.0129'], ['1) 1 0.9182', '2) 2 0.0573', '3) 0 0.0143', '4) 3 0.0102'], ['1) 1 0.9525', '2) 0 0.0207', '3) 2 0.0182', '4) 3 0.0086'], ['1) 3 0.9569', '2) 0 0.0169', '3) 1 0.0164', '4) 2 0.0098'], ['1) 0 0.9768', '2) 1 0.0094', '3) 2 0.0083', '4) 3 0.0055'], ['1) 0 0.9783', '2) 3 0.0079', '3) 2 0.0073', '4) 1 0.0065'], ['1) 1 0.9707', '2) 2 0.0118', '3) 3 0.0089', '4) 0 0.0087'], ['1) 0 0.9774', '2) 1 0.0083', '3) 3 0.0078', '4) 2 0.0066'], ['1) 0 0.9577', '2) 3 0.0259', '3) 1 0.0098', '4) 2 0.0066'], ['1) 3 0.7522', '2) 0 0.1547', '3) 2 0.0626', '4) 1 0.0305'], ['1) 1 0.9691', '2) 2 0.0139', '3) 0 0.0101', '4) 3 0.0069'], ['1) 0 0.9795', '2) 3 0.0091', '3) 2 0.0071', '4) 1 0.0044'], ['1) 0 0.9744', '2) 2 0.011', '3) 1 0.009', '4) 3 0.0056'], ['1) 0 0.8985', '2) 3 0.0801', '3) 2 0.0109', '4) 1 0.0105'], ['1) 1 0.9627', '2) 2 0.0198', '3) 0 0.0089', '4) 3 0.0086'], ['1) 3 0.7732', '2) 1 0.1681', '3) 0 0.0342', '4) 2 0.0245'], ['1) 1 0.9588', '2) 2 0.0199', '3) 0 0.0131', '4) 3 0.0083'], ['1) 2 0.4919', '2) 3 0.3238', '3) 0 0.1751', '4) 1 0.0092'], ['1) 1 0.4531', '2) 2 0.3625', '3) 0 0.1168', '4) 3 0.0676'], ['1) 1 0.9675', '2) 0 0.0142', '3) 2 0.0113', '4) 3 0.0071'], ['1) 0 0.9825', '2) 3 0.0062', '3) 1 0.0058', '4) 2 0.0054'], ['1) 0 0.8702', '2) 3 0.0909', '3) 2 0.0208', '4) 1 0.0181'], ['1) 1 0.8778', '2) 2 0.0715', '3) 3 0.0383', '4) 0 0.0125'], ['1) 3 0.9692', '2) 1 0.0186', '3) 0 0.0083', '4) 2 0.0039'], ['1) 1 0.8939', '2) 2 0.0613', '3) 3 0.0301', '4) 0 0.0147'], ['1) 0 0.9788', '2) 3 0.0091', '3) 1 0.0069', '4) 2 0.0051'], ['1) 0 0.6572', '2) 3 0.3086', '3) 1 0.0241', '4) 2 0.0101'], ['1) 3 0.9865', '2) 1 0.0058', '3) 0 0.0042', '4) 2 0.0035'], ['1) 0 0.977', '2) 3 0.0095', '3) 2 0.0082', '4) 1 0.0052'], ['1) 1 0.971', '2) 2 0.0123', '3) 0 0.0096', '4) 3 0.0071'], ['1) 3 0.9845', '2) 0 0.0073', '3) 1 0.0049', '4) 2 0.0032'], ['1) 1 0.9728', '2) 2 0.0105', '3) 0 0.0087', '4) 3 0.008'], ['1) 0 0.8941', '2) 3 0.0819', '3) 2 0.0185', '4) 1 0.0055'], ['1) 1 0.9579', '2) 2 0.0222', '3) 3 0.0105', '4) 0 0.0095'], ['1) 1 0.8931', '2) 2 0.0489', '3) 0 0.035', '4) 3 0.0231'], ['1) 0 0.9764', '2) 3 0.0087', '3) 1 0.0076', '4) 2 0.0073'], ['1) 1 0.9407', '2) 3 0.0224', '3) 2 0.0201', '4) 0 0.0168'], ['1) 2 0.9125', '2) 0 0.0498', '3) 3 0.0251', '4) 1 0.0125'], ['1) 0 0.9562', '2) 2 0.0218', '3) 3 0.0169', '4) 1 0.0051'], ['1) 0 0.9827', '2) 1 0.007', '3) 2 0.0053', '4) 3 0.0049'], ['1) 3 0.9782', '2) 1 0.0104', '3) 0 0.0068', '4) 2 0.0046'], ['1) 0 0.8701', '2) 3 0.0694', '3) 2 0.0515', '4) 1 0.0089'], ['1) 0 0.9813', '2) 2 0.0075', '3) 1 0.0064', '4) 3 0.0048'], ['1) 0 0.9259', '2) 3 0.0379', '3) 2 0.0235', '4) 1 0.0127'], ['1) 3 0.8319', '2) 0 0.1338', '3) 1 0.0264', '4) 2 0.0079'], ['1) 3 0.4123', '2) 0 0.3212', '3) 2 0.2364', '4) 1 0.03'], ['1) 0 0.9752', '2) 3 0.014', '3) 2 0.0064', '4) 1 0.0044'], ['1) 1 0.9682', '2) 3 0.0119', '3) 0 0.0101', '4) 2 0.0099'], ['1) 1 0.5792', '2) 0 0.2378', '3) 2 0.1387', '4) 3 0.0443'], ['1) 0 0.9326', '2) 3 0.0427', '3) 2 0.0176', '4) 1 0.0071'], ['1) 3 0.4651', '2) 0 0.2621', '3) 2 0.2566', '4) 1 0.0162'], ['1) 1 0.9644', '2) 2 0.0139', '3) 3 0.0131', '4) 0 0.0086'], ['1) 0 0.9568', '2) 3 0.0319', '3) 1 0.0062', '4) 2 0.0051'], ['1) 0 0.9696', '2) 2 0.0138', '3) 3 0.0108', '4) 1 0.0058'], ['1) 1 0.5368', '2) 3 0.2185', '3) 2 0.1776', '4) 0 0.0672'], ['1) 3 0.9272', '2) 1 0.0351', '3) 2 0.0242', '4) 0 0.0135'], ['1) 1 0.9411', '2) 2 0.0347', '3) 0 0.0124', '4) 3 0.0119'], ['1) 0 0.9245', '2) 1 0.0541', '3) 2 0.0135', '4) 3 0.0079'], ['1) 3 0.9816', '2) 1 0.0102', '3) 0 0.0047', '4) 2 0.0036'], ['1) 2 0.6635', '2) 0 0.254', '3) 3 0.0656', '4) 1 0.0169'], ['1) 0 0.9609', '2) 3 0.0207', '3) 2 0.0128', '4) 1 0.0056'], ['1) 0 0.9795', '2) 2 0.0078', '3) 3 0.0074', '4) 1 0.0053'], ['1) 3 0.8272', '2) 0 0.1116', '3) 1 0.0446', '4) 2 0.0166'], ['1) 1 0.8697', '2) 0 0.0628', '3) 2 0.0397', '4) 3 0.0278'], ['1) 0 0.6635', '2) 3 0.2425', '3) 1 0.0761', '4) 2 0.0179'], ['1) 1 0.9211', '2) 0 0.0321', '3) 3 0.0278', '4) 2 0.0189'], ['1) 0 0.9022', '2) 3 0.0817', '3) 1 0.0084', '4) 2 0.0077'], ['1) 0 0.9675', '2) 2 0.0173', '3) 3 0.0079', '4) 1 0.0074'], ['1) 3 0.9793', '2) 1 0.011', '3) 0 0.006', '4) 2 0.0037'], ['1) 3 0.979', '2) 1 0.0106', '3) 0 0.0053', '4) 2 0.0051'], ['1) 1 0.959', '2) 2 0.0199', '3) 3 0.0107', '4) 0 0.0104'], ['1) 3 0.8866', '2) 0 0.1007', '3) 2 0.0076', '4) 1 0.0051'], ['1) 0 0.9625', '2) 3 0.0219', '3) 2 0.0092', '4) 1 0.0064'], ['1) 1 0.82', '2) 0 0.0797', '3) 2 0.0542', '4) 3 0.0462'], ['1) 3 0.9158', '2) 0 0.0652', '3) 2 0.0132', '4) 1 0.0058'], ['1) 3 0.9294', '2) 0 0.05', '3) 1 0.0117', '4) 2 0.0089'], ['1) 1 0.9669', '2) 0 0.0152', '3) 2 0.0109', '4) 3 0.0069'], ['1) 1 0.9138', '2) 2 0.0494', '3) 0 0.0267', '4) 3 0.0101'], ['1) 0 0.6986', '2) 3 0.2309', '3) 2 0.0604', '4) 1 0.0101'], ['1) 0 0.947', '2) 2 0.0234', '3) 3 0.0212', '4) 1 0.0084'], ['1) 0 0.9755', '2) 3 0.015', '3) 1 0.0064', '4) 2 0.0031'], ['1) 3 0.417', '2) 2 0.4023', '3) 0 0.1115', '4) 1 0.0692'], ['1) 3 0.4557', '2) 2 0.3961', '3) 0 0.1203', '4) 1 0.0279'], ['1) 3 0.9847', '2) 0 0.0063', '3) 1 0.0059', '4) 2 0.0031'], ['1) 1 0.893', '2) 0 0.0519', '3) 3 0.0311', '4) 2 0.024'], ['1) 3 0.9826', '2) 1 0.0093', '3) 0 0.0044', '4) 2 0.0037'], ['1) 0 0.9624', '2) 3 0.018', '3) 2 0.0115', '4) 1 0.0081'], ['1) 0 0.8838', '2) 3 0.0583', '3) 2 0.0331', '4) 1 0.0248'], ['1) 3 0.8621', '2) 0 0.1073', '3) 2 0.0164', '4) 1 0.0142'], ['1) 1 0.9694', '2) 2 0.0146', '3) 0 0.0081', '4) 3 0.008'], ['1) 0 0.9323', '2) 3 0.0375', '3) 2 0.025', '4) 1 0.0052'], ['1) 1 0.6201', '2) 3 0.279', '3) 2 0.0645', '4) 0 0.0364'], ['1) 2 0.9424', '2) 0 0.0254', '3) 3 0.02', '4) 1 0.0121'], ['1) 1 0.9613', '2) 0 0.0146', '3) 2 0.0139', '4) 3 0.0102'], ['1) 0 0.9697', '2) 3 0.0155', '3) 2 0.0097', '4) 1 0.0052'], ['1) 1 0.9529', '2) 2 0.0215', '3) 3 0.0167', '4) 0 0.0089'], ['1) 0 0.9738', '2) 3 0.0118', '3) 2 0.0112', '4) 1 0.0033'], ['1) 1 0.8619', '2) 2 0.0956', '3) 3 0.0234', '4) 0 0.0191'], ['1) 1 0.8775', '2) 2 0.0957', '3) 0 0.0164', '4) 3 0.0104'], ['1) 0 0.9713', '2) 2 0.0112', '3) 3 0.0101', '4) 1 0.0074'], ['1) 3 0.9779', '2) 0 0.0112', '3) 1 0.0056', '4) 2 0.0052'], ['1) 0 0.9805', '2) 2 0.008', '3) 3 0.0068', '4) 1 0.0047'], ['1) 0 0.8416', '2) 2 0.0774', '3) 3 0.066', '4) 1 0.0151'], ['1) 0 0.9431', '2) 1 0.0313', '3) 3 0.02', '4) 2 0.0056'], ['1) 3 0.646', '2) 1 0.2441', '3) 0 0.0864', '4) 2 0.0234'], ['1) 0 0.9541', '2) 3 0.0256', '3) 2 0.0153', '4) 1 0.005'], ['1) 3 0.9632', '2) 2 0.0218', '3) 0 0.0086', '4) 1 0.0064'], ['1) 0 0.9819', '2) 3 0.0075', '3) 2 0.0061', '4) 1 0.0045'], ['1) 3 0.4895', '2) 1 0.2558', '3) 0 0.2241', '4) 2 0.0305'], ['1) 0 0.9691', '2) 3 0.0151', '3) 2 0.0105', '4) 1 0.0053'], ['1) 0 0.9647', '2) 3 0.0272', '3) 1 0.0042', '4) 2 0.0039'], ['1) 2 0.5169', '2) 0 0.4412', '3) 3 0.0294', '4) 1 0.0125'], ['1) 1 0.9407', '2) 2 0.0341', '3) 0 0.0144', '4) 3 0.0108'], ['1) 1 0.749', '2) 3 0.1438', '3) 2 0.0776', '4) 0 0.0295'], ['1) 1 0.8933', '2) 0 0.0446', '3) 3 0.0364', '4) 2 0.0257'], ['1) 3 0.9787', '2) 0 0.0125', '3) 2 0.0046', '4) 1 0.0042'], ['1) 1 0.9519', '2) 2 0.0211', '3) 3 0.0181', '4) 0 0.0089'], ['1) 3 0.9682', '2) 0 0.0194', '3) 2 0.0063', '4) 1 0.006'], ['1) 1 0.9185', '2) 2 0.0531', '3) 0 0.017', '4) 3 0.0114'], ['1) 0 0.744', '2) 3 0.1698', '3) 2 0.0478', '4) 1 0.0384'], ['1) 1 0.9687', '2) 2 0.0157', '3) 0 0.008', '4) 3 0.0077'], ['1) 0 0.9811', '2) 3 0.0078', '3) 2 0.0059', '4) 1 0.0052'], ['1) 2 0.936', '2) 3 0.0271', '3) 0 0.0225', '4) 1 0.0144'], ['1) 3 0.9698', '2) 0 0.0192', '3) 1 0.006', '4) 2 0.005'], ['1) 0 0.9723', '2) 2 0.014', '3) 3 0.0077', '4) 1 0.0059'], ['1) 0 0.9753', '2) 3 0.0106', '3) 2 0.0084', '4) 1 0.0057'], ['1) 3 0.9592', '2) 1 0.021', '3) 0 0.0133', '4) 2 0.0066'], ['1) 1 0.5702', '2) 3 0.2366', '3) 2 0.1311', '4) 0 0.0621'], ['1) 2 0.9168', '2) 3 0.0536', '3) 1 0.0154', '4) 0 0.0143'], ['1) 3 0.9853', '2) 0 0.0057', '3) 1 0.0045', '4) 2 0.0045'], ['1) 1 0.9573', '2) 2 0.0173', '3) 3 0.0163', '4) 0 0.0091'], ['1) 3 0.4633', '2) 0 0.3515', '3) 2 0.1348', '4) 1 0.0504'], ['1) 3 0.8553', '2) 0 0.0892', '3) 1 0.0435', '4) 2 0.0121'], ['1) 3 0.9392', '2) 0 0.0258', '3) 2 0.0254', '4) 1 0.0096'], ['1) 3 0.9726', '2) 1 0.0106', '3) 2 0.0088', '4) 0 0.0081'], ['1) 0 0.9692', '2) 3 0.0226', '3) 1 0.0045', '4) 2 0.0037'], ['1) 3 0.9461', '2) 2 0.0273', '3) 0 0.0211', '4) 1 0.0055'], ['1) 0 0.9668', '2) 3 0.0142', '3) 1 0.0105', '4) 2 0.0085'], ['1) 0 0.9389', '2) 3 0.0307', '3) 2 0.0201', '4) 1 0.0104'], ['1) 3 0.9786', '2) 2 0.0097', '3) 0 0.0076', '4) 1 0.0041'], ['1) 1 0.851', '2) 3 0.0892', '3) 2 0.0396', '4) 0 0.0202'], ['1) 1 0.4229', '2) 2 0.4001', '3) 3 0.1595', '4) 0 0.0175'], ['1) 3 0.4551', '2) 0 0.4142', '3) 2 0.096', '4) 1 0.0347'], ['1) 0 0.9775', '2) 1 0.0086', '3) 2 0.0078', '4) 3 0.0061'], ['1) 1 0.5961', '2) 3 0.3124', '3) 2 0.0742', '4) 0 0.0173'], ['1) 3 0.9051', '2) 1 0.0424', '3) 2 0.0404', '4) 0 0.0121'], ['1) 0 0.7931', '2) 3 0.1846', '3) 2 0.0124', '4) 1 0.0099'], ['1) 3 0.9849', '2) 0 0.0052', '3) 2 0.0052', '4) 1 0.0048'], ['1) 1 0.938', '2) 2 0.0357', '3) 3 0.0159', '4) 0 0.0104'], ['1) 1 0.9669', '2) 2 0.0131', '3) 0 0.012', '4) 3 0.008'], ['1) 0 0.9385', '2) 3 0.0354', '3) 2 0.0151', '4) 1 0.011'], ['1) 2 0.8649', '2) 1 0.0768', '3) 0 0.0296', '4) 3 0.0287'], ['1) 0 0.5275', '2) 3 0.419', '3) 1 0.03', '4) 2 0.0235'], ['1) 0 0.4557', '2) 1 0.3145', '3) 3 0.171', '4) 2 0.0588'], ['1) 3 0.9338', '2) 0 0.0404', '3) 2 0.0203', '4) 1 0.0055'], ['1) 0 0.9108', '2) 3 0.0529', '3) 2 0.0271', '4) 1 0.0092'], ['1) 3 0.9771', '2) 2 0.0115', '3) 1 0.0059', '4) 0 0.0055'], ['1) 3 0.9689', '2) 0 0.0149', '3) 1 0.0094', '4) 2 0.0067'], ['1) 1 0.961', '2) 2 0.0158', '3) 3 0.013', '4) 0 0.0101'], ['1) 0 0.94', '2) 3 0.0424', '3) 2 0.0113', '4) 1 0.0063'], ['1) 1 0.7549', '2) 2 0.1048', '3) 3 0.0739', '4) 0 0.0664'], ['1) 2 0.9251', '2) 3 0.0276', '3) 1 0.0261', '4) 0 0.0212'], ['1) 3 0.8452', '2) 0 0.1027', '3) 2 0.0346', '4) 1 0.0176'], ['1) 0 0.9571', '2) 3 0.0201', '3) 2 0.0123', '4) 1 0.0105'], ['1) 0 0.9627', '2) 3 0.0198', '3) 1 0.0099', '4) 2 0.0076'], ['1) 1 0.9662', '2) 2 0.0149', '3) 3 0.0101', '4) 0 0.0088'], ['1) 0 0.9733', '2) 3 0.0096', '3) 2 0.0095', '4) 1 0.0075'], ['1) 2 0.8926', '2) 3 0.046', '3) 0 0.0346', '4) 1 0.0267'], ['1) 1 0.9368', '2) 0 0.0286', '3) 3 0.0178', '4) 2 0.0168'], ['1) 0 0.9709', '2) 1 0.0156', '3) 3 0.0074', '4) 2 0.006'], ['1) 0 0.9589', '2) 2 0.0225', '3) 3 0.0125', '4) 1 0.006'], ['1) 3 0.8318', '2) 0 0.1329', '3) 2 0.0185', '4) 1 0.0168'], ['1) 0 0.9068', '2) 3 0.0559', '3) 2 0.0317', '4) 1 0.0055'], ['1) 1 0.9582', '2) 2 0.0184', '3) 3 0.0121', '4) 0 0.0113'], ['1) 0 0.8974', '2) 3 0.0473', '3) 2 0.0472', '4) 1 0.008'], ['1) 1 0.8864', '2) 2 0.085', '3) 0 0.0153', '4) 3 0.0134'], ['1) 0 0.9147', '2) 3 0.0615', '3) 2 0.0206', '4) 1 0.0032'], ['1) 3 0.9691', '2) 0 0.0165', '3) 1 0.0099', '4) 2 0.0045'], ['1) 1 0.9404', '2) 2 0.0228', '3) 3 0.022', '4) 0 0.0148'], ['1) 3 0.9794', '2) 0 0.0111', '3) 1 0.0055', '4) 2 0.004'], ['1) 1 0.9561', '2) 2 0.02', '3) 0 0.0135', '4) 3 0.0104'], ['1) 1 0.829', '2) 2 0.0995', '3) 3 0.048', '4) 0 0.0235'], ['1) 1 0.9609', '2) 2 0.0144', '3) 3 0.014', '4) 0 0.0107'], ['1) 1 0.9699', '2) 0 0.0134', '3) 2 0.0096', '4) 3 0.0071'], ['1) 1 0.786', '2) 2 0.1739', '3) 0 0.0222', '4) 3 0.018'], ['1) 1 0.3937', '2) 3 0.3844', '3) 2 0.1987', '4) 0 0.0232'], ['1) 0 0.9729', '2) 3 0.0108', '3) 2 0.0094', '4) 1 0.0069'], ['1) 0 0.9788', '2) 3 0.008', '3) 1 0.0071', '4) 2 0.0061'], ['1) 0 0.9544', '2) 3 0.0343', '3) 2 0.0062', '4) 1 0.0051'], ['1) 2 0.6841', '2) 0 0.2356', '3) 3 0.0633', '4) 1 0.0169'], ['1) 0 0.9394', '2) 3 0.0316', '3) 2 0.0174', '4) 1 0.0117'], ['1) 1 0.9644', '2) 2 0.0147', '3) 0 0.0107', '4) 3 0.0102'], ['1) 0 0.9611', '2) 3 0.0194', '3) 2 0.0162', '4) 1 0.0033'], ['1) 1 0.9368', '2) 3 0.0371', '3) 2 0.0132', '4) 0 0.0129'], ['1) 0 0.8051', '2) 3 0.1758', '3) 1 0.0106', '4) 2 0.0085'], ['1) 1 0.9561', '2) 0 0.0176', '3) 2 0.017', '4) 3 0.0093'], ['1) 0 0.9152', '2) 2 0.0475', '3) 3 0.0266', '4) 1 0.0107'], ['1) 0 0.9806', '2) 3 0.0078', '3) 2 0.0074', '4) 1 0.0043'], ['1) 2 0.6106', '2) 0 0.3155', '3) 3 0.0609', '4) 1 0.0129'], ['1) 1 0.7782', '2) 2 0.1293', '3) 0 0.0705', '4) 3 0.022'], ['1) 0 0.728', '2) 3 0.2372', '3) 1 0.022', '4) 2 0.0129'], ['1) 0 0.8965', '2) 3 0.0541', '3) 1 0.0341', '4) 2 0.0153'], ['1) 0 0.9723', '2) 3 0.0183', '3) 1 0.0052', '4) 2 0.0043'], ['1) 3 0.7937', '2) 1 0.1379', '3) 0 0.0539', '4) 2 0.0144'], ['1) 3 0.9851', '2) 1 0.0072', '3) 0 0.0044', '4) 2 0.0032'], ['1) 3 0.9709', '2) 0 0.0139', '3) 1 0.0088', '4) 2 0.0063'], ['1) 0 0.8425', '2) 1 0.0818', '3) 2 0.0456', '4) 3 0.0301'], ['1) 1 0.9701', '2) 2 0.0125', '3) 0 0.01', '4) 3 0.0074'], ['1) 1 0.9602', '2) 2 0.0188', '3) 3 0.0125', '4) 0 0.0084'], ['1) 1 0.9604', '2) 2 0.0188', '3) 0 0.0144', '4) 3 0.0064'], ['1) 2 0.4373', '2) 3 0.2824', '3) 0 0.2475', '4) 1 0.0328'], ['1) 2 0.8282', '2) 0 0.0841', '3) 3 0.0682', '4) 1 0.0195'], ['1) 0 0.5636', '2) 3 0.4012', '3) 1 0.023', '4) 2 0.0122'], ['1) 3 0.9653', '2) 0 0.0215', '3) 2 0.0076', '4) 1 0.0056'], ['1) 0 0.9121', '2) 3 0.0493', '3) 2 0.0294', '4) 1 0.0092'], ['1) 3 0.9264', '2) 2 0.0424', '3) 0 0.0248', '4) 1 0.0063'], ['1) 1 0.9061', '2) 0 0.0354', '3) 3 0.0333', '4) 2 0.0252'], ['1) 1 0.9471', '2) 2 0.027', '3) 0 0.014', '4) 3 0.0119'], ['1) 1 0.8054', '2) 0 0.1186', '3) 3 0.0522', '4) 2 0.0238'], ['1) 0 0.9631', '2) 3 0.0165', '3) 2 0.0147', '4) 1 0.0057'], ['1) 0 0.9525', '2) 3 0.0284', '3) 2 0.01', '4) 1 0.0091'], ['1) 1 0.8573', '2) 2 0.0579', '3) 0 0.0548', '4) 3 0.0301'], ['1) 1 0.9099', '2) 2 0.0348', '3) 0 0.0303', '4) 3 0.0249'], ['1) 1 0.8092', '2) 2 0.1036', '3) 0 0.0602', '4) 3 0.027'], ['1) 0 0.7542', '2) 2 0.173', '3) 3 0.0655', '4) 1 0.0074'], ['1) 0 0.8997', '2) 2 0.0507', '3) 3 0.0443', '4) 1 0.0054'], ['1) 1 0.9704', '2) 2 0.0123', '3) 3 0.0101', '4) 0 0.0072'], ['1) 2 0.6773', '2) 3 0.1376', '3) 0 0.1036', '4) 1 0.0815'], ['1) 0 0.9746', '2) 2 0.0103', '3) 3 0.0093', '4) 1 0.0058'], ['1) 0 0.693', '2) 1 0.1743', '3) 3 0.0924', '4) 2 0.0404'], ['1) 0 0.9819', '2) 3 0.0062', '3) 1 0.0061', '4) 2 0.0058'], ['1) 0 0.7336', '2) 3 0.1784', '3) 2 0.0778', '4) 1 0.0101'], ['1) 0 0.8983', '2) 3 0.0887', '3) 1 0.0066', '4) 2 0.0065'], ['1) 1 0.8359', '2) 0 0.0663', '3) 2 0.0593', '4) 3 0.0385'], ['1) 1 0.7112', '2) 2 0.2347', '3) 3 0.037', '4) 0 0.017'], ['1) 1 0.6648', '2) 0 0.1555', '3) 2 0.1383', '4) 3 0.0413'], ['1) 3 0.9776', '2) 1 0.0113', '3) 0 0.0061', '4) 2 0.0049'], ['1) 0 0.8152', '2) 3 0.1529', '3) 2 0.0222', '4) 1 0.0096'], ['1) 3 0.9754', '2) 2 0.0103', '3) 1 0.0086', '4) 0 0.0058'], ['1) 0 0.8669', '2) 3 0.1182', '3) 2 0.0075', '4) 1 0.0074'], ['1) 1 0.9664', '2) 0 0.0154', '3) 2 0.0103', '4) 3 0.0079'], ['1) 0 0.9743', '2) 3 0.0116', '3) 2 0.0083', '4) 1 0.0058'], ['1) 3 0.9661', '2) 1 0.0181', '3) 2 0.0083', '4) 0 0.0074'], ['1) 0 0.9379', '2) 3 0.0495', '3) 1 0.0078', '4) 2 0.0048'], ['1) 3 0.8518', '2) 1 0.0703', '3) 0 0.0572', '4) 2 0.0207'], ['1) 0 0.9071', '2) 3 0.0741', '3) 1 0.0103', '4) 2 0.0085'], ['1) 1 0.7953', '2) 3 0.0829', '3) 2 0.0652', '4) 0 0.0566'], ['1) 3 0.8567', '2) 0 0.1223', '3) 2 0.0138', '4) 1 0.0073'], ['1) 0 0.8298', '2) 3 0.1512', '3) 1 0.0116', '4) 2 0.0075'], ['1) 0 0.9814', '2) 2 0.0085', '3) 3 0.0058', '4) 1 0.0044'], ['1) 3 0.9597', '2) 0 0.0168', '3) 1 0.0142', '4) 2 0.0093'], ['1) 0 0.5816', '2) 3 0.3858', '3) 1 0.0257', '4) 2 0.0069'], ['1) 2 0.7805', '2) 3 0.1773', '3) 0 0.0226', '4) 1 0.0197'], ['1) 1 0.9336', '2) 2 0.0314', '3) 0 0.023', '4) 3 0.012'], ['1) 3 0.9619', '2) 2 0.018', '3) 0 0.0109', '4) 1 0.0092'], ['1) 1 0.5436', '2) 2 0.3555', '3) 3 0.0781', '4) 0 0.0228'], ['1) 0 0.9277', '2) 1 0.0321', '3) 3 0.031', '4) 2 0.0091'], ['1) 1 0.9534', '2) 3 0.027', '3) 2 0.0108', '4) 0 0.0088'], ['1) 1 0.9299', '2) 2 0.0488', '3) 0 0.0138', '4) 3 0.0075'], ['1) 0 0.966', '2) 3 0.0204', '3) 1 0.0073', '4) 2 0.0063'], ['1) 3 0.9504', '2) 0 0.0302', '3) 1 0.015', '4) 2 0.0044'], ['1) 3 0.7812', '2) 2 0.1746', '3) 0 0.0308', '4) 1 0.0135'], ['1) 1 0.9424', '2) 2 0.0264', '3) 0 0.0194', '4) 3 0.0118'], ['1) 0 0.8444', '2) 2 0.0769', '3) 1 0.0571', '4) 3 0.0216'], ['1) 3 0.9409', '2) 0 0.0425', '3) 2 0.0084', '4) 1 0.0083'], ['1) 0 0.9544', '2) 3 0.0226', '3) 1 0.0147', '4) 2 0.0083'], ['1) 0 0.9822', '2) 2 0.0077', '3) 3 0.0061', '4) 1 0.0039'], ['1) 0 0.9456', '2) 1 0.0226', '3) 2 0.0166', '4) 3 0.0153'], ['1) 0 0.9549', '2) 2 0.0195', '3) 3 0.0181', '4) 1 0.0076'], ['1) 0 0.9496', '2) 3 0.026', '3) 2 0.0191', '4) 1 0.0053'], ['1) 2 0.8808', '2) 3 0.0769', '3) 0 0.0311', '4) 1 0.0112'], ['1) 3 0.9672', '2) 1 0.0158', '3) 0 0.0109', '4) 2 0.006'], ['1) 0 0.8714', '2) 3 0.0548', '3) 1 0.0496', '4) 2 0.0243'], ['1) 0 0.9345', '2) 3 0.0554', '3) 2 0.0053', '4) 1 0.0048'], ['1) 0 0.9663', '2) 3 0.0143', '3) 2 0.0126', '4) 1 0.0068'], ['1) 1 0.9628', '2) 2 0.0177', '3) 0 0.0102', '4) 3 0.0092'], ['1) 1 0.9547', '2) 3 0.0217', '3) 2 0.0126', '4) 0 0.011'], ['1) 3 0.9421', '2) 0 0.0347', '3) 1 0.0189', '4) 2 0.0043'], ['1) 3 0.8501', '2) 0 0.119', '3) 1 0.0228', '4) 2 0.0081'], ['1) 3 0.9622', '2) 1 0.0157', '3) 2 0.0155', '4) 0 0.0066'], ['1) 0 0.893', '2) 2 0.0767', '3) 3 0.0196', '4) 1 0.0107'], ['1) 2 0.5021', '2) 0 0.3775', '3) 3 0.0898', '4) 1 0.0306'], ['1) 1 0.7102', '2) 3 0.1308', '3) 0 0.083', '4) 2 0.0761'], ['1) 0 0.6758', '2) 3 0.2601', '3) 2 0.0554', '4) 1 0.0087'], ['1) 1 0.945', '2) 2 0.0278', '3) 0 0.0145', '4) 3 0.0127'], ['1) 3 0.9689', '2) 0 0.0184', '3) 1 0.0085', '4) 2 0.0042'], ['1) 1 0.6862', '2) 2 0.1981', '3) 3 0.0708', '4) 0 0.0449'], ['1) 0 0.6913', '2) 3 0.2908', '3) 2 0.011', '4) 1 0.007'], ['1) 1 0.9692', '2) 2 0.0134', '3) 3 0.0093', '4) 0 0.0081'], ['1) 0 0.9493', '2) 1 0.0222', '3) 3 0.0155', '4) 2 0.013'], ['1) 3 0.9797', '2) 1 0.0102', '3) 0 0.0067', '4) 2 0.0033'], ['1) 1 0.9059', '2) 2 0.062', '3) 0 0.0189', '4) 3 0.0132'], ['1) 0 0.9364', '2) 2 0.0346', '3) 3 0.0236', '4) 1 0.0054'], ['1) 0 0.6947', '2) 3 0.192', '3) 2 0.1046', '4) 1 0.0087'], ['1) 1 0.9641', '2) 2 0.0188', '3) 3 0.0095', '4) 0 0.0076'], ['1) 0 0.9014', '2) 2 0.0531', '3) 3 0.0359', '4) 1 0.0096'], ['1) 1 0.9438', '2) 0 0.023', '3) 2 0.0204', '4) 3 0.0127'], ['1) 0 0.6945', '2) 3 0.2148', '3) 1 0.0696', '4) 2 0.0211'], ['1) 1 0.9675', '2) 3 0.0127', '3) 2 0.0119', '4) 0 0.0079'], ['1) 3 0.7916', '2) 0 0.152', '3) 2 0.0352', '4) 1 0.0212'], ['1) 0 0.9737', '2) 3 0.0157', '3) 1 0.0067', '4) 2 0.0039'], ['1) 1 0.7875', '2) 0 0.1049', '3) 2 0.0555', '4) 3 0.0521'], ['1) 0 0.9736', '2) 3 0.0123', '3) 2 0.009', '4) 1 0.0051'], ['1) 1 0.8433', '2) 2 0.1173', '3) 0 0.022', '4) 3 0.0174'], ['1) 1 0.8978', '2) 3 0.0498', '3) 2 0.0375', '4) 0 0.0149'], ['1) 0 0.9494', '2) 2 0.0307', '3) 3 0.0105', '4) 1 0.0094'], ['1) 1 0.96', '2) 2 0.0183', '3) 3 0.0117', '4) 0 0.01'], ['1) 0 0.8338', '2) 3 0.1363', '3) 2 0.0177', '4) 1 0.0123'], ['1) 0 0.9363', '2) 3 0.0273', '3) 1 0.0192', '4) 2 0.0172'], ['1) 3 0.7003', '2) 2 0.1533', '3) 0 0.1361', '4) 1 0.0102'], ['1) 3 0.8792', '2) 0 0.0702', '3) 2 0.0323', '4) 1 0.0182'], ['1) 0 0.8857', '2) 3 0.081', '3) 2 0.026', '4) 1 0.0073'], ['1) 3 0.7896', '2) 0 0.1734', '3) 1 0.0306', '4) 2 0.0064'], ['1) 0 0.7516', '2) 3 0.1985', '3) 2 0.0347', '4) 1 0.0152'], ['1) 0 0.7984', '2) 2 0.117', '3) 3 0.0455', '4) 1 0.0391'], ['1) 0 0.9817', '2) 3 0.0081', '3) 2 0.0057', '4) 1 0.0045'], ['1) 3 0.8057', '2) 2 0.1257', '3) 0 0.053', '4) 1 0.0156'], ['1) 0 0.9751', '2) 2 0.0105', '3) 1 0.008', '4) 3 0.0065'], ['1) 1 0.9646', '2) 0 0.0203', '3) 2 0.0091', '4) 3 0.006'], ['1) 3 0.8476', '2) 2 0.0931', '3) 1 0.0467', '4) 0 0.0127'], ['1) 2 0.6878', '2) 1 0.2486', '3) 3 0.0369', '4) 0 0.0268'], ['1) 3 0.7593', '2) 2 0.1076', '3) 0 0.0807', '4) 1 0.0524'], ['1) 2 0.6178', '2) 0 0.1834', '3) 3 0.1715', '4) 1 0.0272'], ['1) 0 0.9755', '2) 2 0.0114', '3) 3 0.0088', '4) 1 0.0043'], ['1) 3 0.9712', '2) 0 0.0184', '3) 1 0.0063', '4) 2 0.0041'], ['1) 3 0.9205', '2) 0 0.04', '3) 2 0.0212', '4) 1 0.0183'], ['1) 0 0.9815', '2) 2 0.008', '3) 1 0.0054', '4) 3 0.0051'], ['1) 0 0.6878', '2) 2 0.2118', '3) 3 0.0853', '4) 1 0.015'], ['1) 0 0.9758', '2) 3 0.0096', '3) 2 0.0077', '4) 1 0.0069'], ['1) 0 0.9513', '2) 2 0.0272', '3) 3 0.0164', '4) 1 0.0051'], ['1) 1 0.5434', '2) 0 0.3224', '3) 2 0.0886', '4) 3 0.0457'], ['1) 3 0.5605', '2) 1 0.2747', '3) 0 0.1478', '4) 2 0.017'], ['1) 3 0.9016', '2) 0 0.0839', '3) 1 0.009', '4) 2 0.0054'], ['1) 0 0.9497', '2) 3 0.0329', '3) 2 0.0132', '4) 1 0.0043'], ['1) 0 0.9054', '2) 2 0.0469', '3) 3 0.0437', '4) 1 0.0041'], ['1) 3 0.9794', '2) 0 0.0084', '3) 2 0.0077', '4) 1 0.0045'], ['1) 1 0.7593', '2) 0 0.1799', '3) 3 0.0328', '4) 2 0.0279'], ['1) 1 0.9501', '2) 2 0.0246', '3) 3 0.0132', '4) 0 0.0122'], ['1) 2 0.8066', '2) 0 0.1025', '3) 3 0.0708', '4) 1 0.0201'], ['1) 0 0.9749', '2) 3 0.0105', '3) 1 0.008', '4) 2 0.0065'], ['1) 3 0.9752', '2) 0 0.0148', '3) 2 0.0053', '4) 1 0.0047'], ['1) 0 0.464', '2) 3 0.4539', '3) 2 0.0596', '4) 1 0.0226'], ['1) 3 0.5689', '2) 0 0.4142', '3) 1 0.0104', '4) 2 0.0065'], ['1) 3 0.9489', '2) 2 0.0306', '3) 0 0.0128', '4) 1 0.0077'], ['1) 3 0.9729', '2) 0 0.016', '3) 2 0.0065', '4) 1 0.0046'], ['1) 3 0.6222', '2) 0 0.3404', '3) 1 0.0273', '4) 2 0.0101'], ['1) 1 0.9454', '2) 2 0.0286', '3) 3 0.0144', '4) 0 0.0116'], ['1) 0 0.9613', '2) 3 0.0181', '3) 2 0.0121', '4) 1 0.0086'], ['1) 0 0.9777', '2) 2 0.0095', '3) 3 0.0065', '4) 1 0.0063'], ['1) 3 0.6396', '2) 0 0.34', '3) 2 0.0108', '4) 1 0.0096'], ['1) 0 0.9594', '2) 2 0.0215', '3) 3 0.0139', '4) 1 0.0052'], ['1) 3 0.9767', '2) 2 0.0123', '3) 1 0.0062', '4) 0 0.0048'], ['1) 3 0.6353', '2) 1 0.255', '3) 0 0.0872', '4) 2 0.0226'], ['1) 0 0.9773', '2) 2 0.0092', '3) 3 0.0088', '4) 1 0.0046'], ['1) 0 0.9396', '2) 3 0.0323', '3) 1 0.0206', '4) 2 0.0074'], ['1) 1 0.9476', '2) 2 0.0317', '3) 0 0.0119', '4) 3 0.0088'], ['1) 1 0.749', '2) 2 0.1951', '3) 0 0.0327', '4) 3 0.0231'], ['1) 0 0.4869', '2) 3 0.4861', '3) 2 0.017', '4) 1 0.0101'], ['1) 0 0.9689', '2) 3 0.0134', '3) 2 0.0112', '4) 1 0.0065'], ['1) 3 0.9774', '2) 1 0.0092', '3) 0 0.0085', '4) 2 0.0048'], ['1) 1 0.5214', '2) 0 0.4025', '3) 2 0.048', '4) 3 0.0281'], ['1) 0 0.7632', '2) 3 0.145', '3) 1 0.0719', '4) 2 0.0199'], ['1) 2 0.5945', '2) 3 0.3618', '3) 1 0.0257', '4) 0 0.0179'], ['1) 3 0.8305', '2) 0 0.1527', '3) 2 0.0092', '4) 1 0.0076'], ['1) 0 0.8124', '2) 3 0.102', '3) 2 0.0764', '4) 1 0.0093'], ['1) 0 0.9703', '2) 3 0.0139', '3) 1 0.0103', '4) 2 0.0055'], ['1) 0 0.9419', '2) 3 0.0349', '3) 1 0.0145', '4) 2 0.0087'], ['1) 3 0.9783', '2) 0 0.0109', '3) 1 0.0068', '4) 2 0.0039'], ['1) 3 0.9848', '2) 0 0.0063', '3) 1 0.0058', '4) 2 0.003'], ['1) 3 0.8768', '2) 1 0.0608', '3) 0 0.0372', '4) 2 0.0253'], ['1) 3 0.9626', '2) 2 0.0165', '3) 1 0.013', '4) 0 0.0079'], ['1) 2 0.9415', '2) 0 0.0222', '3) 3 0.0185', '4) 1 0.0178'], ['1) 1 0.944', '2) 2 0.0325', '3) 0 0.0123', '4) 3 0.0113'], ['1) 3 0.9053', '2) 0 0.0696', '3) 2 0.0176', '4) 1 0.0075'], ['1) 0 0.9805', '2) 2 0.0088', '3) 1 0.0059', '4) 3 0.0049'], ['1) 1 0.961', '2) 2 0.0172', '3) 0 0.0144', '4) 3 0.0073'], ['1) 0 0.8479', '2) 1 0.0829', '3) 3 0.063', '4) 2 0.0061'], ['1) 1 0.95', '2) 2 0.0178', '3) 0 0.0169', '4) 3 0.0153'], ['1) 3 0.9813', '2) 0 0.0086', '3) 2 0.0051', '4) 1 0.005'], ['1) 1 0.924', '2) 0 0.0361', '3) 2 0.0224', '4) 3 0.0174'], ['1) 0 0.9636', '2) 3 0.0222', '3) 2 0.0094', '4) 1 0.0048'], ['1) 1 0.9716', '2) 3 0.0103', '3) 2 0.01', '4) 0 0.008'], ['1) 2 0.6453', '2) 3 0.2762', '3) 0 0.067', '4) 1 0.0115'], ['1) 0 0.9707', '2) 2 0.0119', '3) 3 0.0119', '4) 1 0.0055'], ['1) 2 0.6682', '2) 0 0.2445', '3) 3 0.0534', '4) 1 0.0338'], ['1) 0 0.8473', '2) 3 0.1177', '3) 1 0.0228', '4) 2 0.0121'], ['1) 2 0.9107', '2) 1 0.043', '3) 3 0.0232', '4) 0 0.0231'], ['1) 0 0.6964', '2) 1 0.1521', '3) 3 0.1303', '4) 2 0.0211'], ['1) 2 0.8075', '2) 3 0.1439', '3) 0 0.0329', '4) 1 0.0157'], ['1) 3 0.8953', '2) 0 0.0517', '3) 1 0.0294', '4) 2 0.0236'], ['1) 0 0.9807', '2) 2 0.0065', '3) 3 0.0064', '4) 1 0.0064'], ['1) 0 0.9806', '2) 3 0.0097', '3) 1 0.0049', '4) 2 0.0049'], ['1) 0 0.979', '2) 1 0.0086', '3) 2 0.0074', '4) 3 0.005'], ['1) 1 0.5739', '2) 2 0.2333', '3) 3 0.1704', '4) 0 0.0223'], ['1) 0 0.9714', '2) 3 0.0125', '3) 2 0.0095', '4) 1 0.0065'], ['1) 3 0.9767', '2) 0 0.0116', '3) 2 0.0063', '4) 1 0.0054'], ['1) 3 0.7965', '2) 0 0.1065', '3) 2 0.0645', '4) 1 0.0325'], ['1) 1 0.9227', '2) 2 0.0449', '3) 3 0.0172', '4) 0 0.0151'], ['1) 2 0.4712', '2) 3 0.341', '3) 1 0.1317', '4) 0 0.0561'], ['1) 3 0.9558', '2) 2 0.0197', '3) 1 0.0132', '4) 0 0.0113'], ['1) 0 0.9208', '2) 3 0.0626', '3) 2 0.0092', '4) 1 0.0074'], ['1) 3 0.9535', '2) 0 0.0203', '3) 1 0.0166', '4) 2 0.0096'], ['1) 0 0.9453', '2) 3 0.023', '3) 2 0.0228', '4) 1 0.0088'], ['1) 0 0.982', '2) 1 0.007', '3) 3 0.0057', '4) 2 0.0054'], ['1) 0 0.6593', '2) 3 0.3095', '3) 1 0.0239', '4) 2 0.0073'], ['1) 3 0.9542', '2) 2 0.0215', '3) 1 0.0159', '4) 0 0.0084'], ['1) 3 0.4747', '2) 0 0.4559', '3) 2 0.048', '4) 1 0.0213'], ['1) 3 0.9526', '2) 0 0.0207', '3) 1 0.0144', '4) 2 0.0123'], ['1) 3 0.9846', '2) 0 0.0052', '3) 1 0.0052', '4) 2 0.005'], ['1) 0 0.9572', '2) 3 0.0286', '3) 1 0.0079', '4) 2 0.0063'], ['1) 1 0.94', '2) 3 0.0211', '3) 0 0.0199', '4) 2 0.019'], ['1) 3 0.7739', '2) 0 0.195', '3) 2 0.0208', '4) 1 0.0102'], ['1) 0 0.804', '2) 3 0.0971', '3) 1 0.0553', '4) 2 0.0436'], ['1) 0 0.8969', '2) 3 0.0648', '3) 1 0.0228', '4) 2 0.0155'], ['1) 2 0.8717', '2) 3 0.0694', '3) 0 0.0475', '4) 1 0.0115'], ['1) 0 0.9566', '2) 3 0.0232', '3) 2 0.0132', '4) 1 0.007'], ['1) 1 0.5549', '2) 3 0.4069', '3) 0 0.0211', '4) 2 0.017'], ['1) 1 0.9067', '2) 2 0.0447', '3) 0 0.0297', '4) 3 0.0189'], ['1) 1 0.8471', '2) 2 0.1046', '3) 3 0.0345', '4) 0 0.0138'], ['1) 3 0.9753', '2) 0 0.0128', '3) 2 0.0066', '4) 1 0.0054'], ['1) 3 0.9666', '2) 0 0.0163', '3) 1 0.0108', '4) 2 0.0064'], ['1) 1 0.9336', '2) 2 0.0285', '3) 3 0.0278', '4) 0 0.0101'], ['1) 3 0.9809', '2) 1 0.0076', '3) 0 0.0068', '4) 2 0.0047'], ['1) 0 0.886', '2) 2 0.057', '3) 3 0.0375', '4) 1 0.0195'], ['1) 0 0.9662', '2) 2 0.0202', '3) 3 0.0093', '4) 1 0.0042'], ['1) 1 0.965', '2) 3 0.0128', '3) 0 0.012', '4) 2 0.0101'], ['1) 3 0.7466', '2) 1 0.2217', '3) 2 0.0185', '4) 0 0.0132'], ['1) 1 0.9705', '2) 3 0.0112', '3) 2 0.0103', '4) 0 0.008'], ['1) 1 0.9455', '2) 2 0.0264', '3) 3 0.0145', '4) 0 0.0136'], ['1) 3 0.9167', '2) 1 0.0588', '3) 0 0.0171', '4) 2 0.0075'], ['1) 0 0.9236', '2) 3 0.0393', '3) 2 0.0267', '4) 1 0.0104'], ['1) 3 0.9194', '2) 0 0.0598', '3) 2 0.0135', '4) 1 0.0073'], ['1) 3 0.4614', '2) 2 0.4003', '3) 0 0.1208', '4) 1 0.0175'], ['1) 1 0.9541', '2) 2 0.0211', '3) 0 0.013', '4) 3 0.0117'], ['1) 1 0.9488', '2) 2 0.0279', '3) 0 0.0126', '4) 3 0.0107'], ['1) 0 0.5296', '2) 3 0.3023', '3) 2 0.1331', '4) 1 0.035'], ['1) 1 0.9248', '2) 3 0.0468', '3) 2 0.0156', '4) 0 0.0127'], ['1) 0 0.7625', '2) 2 0.112', '3) 1 0.0978', '4) 3 0.0277'], ['1) 0 0.98', '2) 2 0.0083', '3) 1 0.0058', '4) 3 0.0058'], ['1) 0 0.4975', '2) 3 0.4548', '3) 2 0.0337', '4) 1 0.014'], ['1) 0 0.9749', '2) 3 0.0121', '3) 2 0.0079', '4) 1 0.0051'], ['1) 1 0.9717', '2) 2 0.0124', '3) 3 0.0089', '4) 0 0.007'], ['1) 2 0.5451', '2) 3 0.2657', '3) 1 0.1312', '4) 0 0.058'], ['1) 0 0.7663', '2) 2 0.0999', '3) 3 0.0939', '4) 1 0.0399'], ['1) 2 0.4751', '2) 3 0.2767', '3) 0 0.1837', '4) 1 0.0646'], ['1) 0 0.8427', '2) 3 0.1407', '3) 1 0.009', '4) 2 0.0077'], ['1) 1 0.9581', '2) 2 0.0191', '3) 0 0.0147', '4) 3 0.0081'], ['1) 0 0.8868', '2) 2 0.0906', '3) 3 0.0131', '4) 1 0.0095'], ['1) 0 0.8692', '2) 3 0.1088', '3) 2 0.0127', '4) 1 0.0094'], ['1) 1 0.5101', '2) 3 0.1977', '3) 0 0.1734', '4) 2 0.1187'], ['1) 0 0.9674', '2) 3 0.0176', '3) 2 0.0082', '4) 1 0.0068'], ['1) 3 0.9768', '2) 0 0.0144', '3) 1 0.0053', '4) 2 0.0035'], ['1) 0 0.9696', '2) 1 0.015', '3) 3 0.0098', '4) 2 0.0056'], ['1) 0 0.9753', '2) 3 0.0101', '3) 2 0.0074', '4) 1 0.0072'], ['1) 0 0.8985', '2) 3 0.0666', '3) 1 0.0255', '4) 2 0.0094'], ['1) 1 0.9596', '2) 2 0.0192', '3) 3 0.0139', '4) 0 0.0073'], ['1) 3 0.923', '2) 1 0.0515', '3) 2 0.0142', '4) 0 0.0114'], ['1) 1 0.9577', '2) 2 0.0194', '3) 0 0.0141', '4) 3 0.0089'], ['1) 3 0.4325', '2) 1 0.3736', '3) 2 0.1214', '4) 0 0.0725'], ['1) 0 0.9217', '2) 3 0.052', '3) 2 0.0192', '4) 1 0.0071'], ['1) 2 0.4863', '2) 1 0.2659', '3) 3 0.2358', '4) 0 0.012'], ['1) 3 0.8972', '2) 0 0.0899', '3) 1 0.0082', '4) 2 0.0047'], ['1) 1 0.9256', '2) 3 0.0329', '3) 0 0.022', '4) 2 0.0195'], ['1) 3 0.9825', '2) 1 0.0077', '3) 0 0.006', '4) 2 0.0038'], ['1) 0 0.9672', '2) 2 0.0182', '3) 3 0.01', '4) 1 0.0047'], ['1) 3 0.9848', '2) 0 0.006', '3) 2 0.0053', '4) 1 0.0038'], ['1) 1 0.9401', '2) 2 0.0373', '3) 3 0.0148', '4) 0 0.0078'], ['1) 1 0.9531', '2) 2 0.0208', '3) 3 0.0152', '4) 0 0.0108'], ['1) 1 0.9693', '2) 2 0.0129', '3) 0 0.0108', '4) 3 0.007'], ['1) 3 0.7546', '2) 0 0.144', '3) 2 0.0793', '4) 1 0.0221'], ['1) 0 0.88', '2) 3 0.0641', '3) 2 0.0473', '4) 1 0.0086'], ['1) 0 0.984', '2) 3 0.0072', '3) 2 0.0044', '4) 1 0.0044'], ['1) 0 0.9815', '2) 3 0.0063', '3) 2 0.0062', '4) 1 0.006'], ['1) 0 0.9778', '2) 3 0.0111', '3) 2 0.0077', '4) 1 0.0035'], ['1) 1 0.8214', '2) 2 0.1333', '3) 3 0.0363', '4) 0 0.009'], ['1) 3 0.9789', '2) 0 0.0082', '3) 1 0.0078', '4) 2 0.0051'], ['1) 0 0.8339', '2) 3 0.1505', '3) 1 0.008', '4) 2 0.0076'], ['1) 3 0.9555', '2) 0 0.0298', '3) 1 0.0093', '4) 2 0.0055'], ['1) 3 0.9783', '2) 1 0.0109', '3) 2 0.0057', '4) 0 0.0051'], ['1) 1 0.9619', '2) 2 0.0142', '3) 0 0.0131', '4) 3 0.0108'], ['1) 1 0.7051', '2) 2 0.1437', '3) 3 0.1338', '4) 0 0.0175'], ['1) 0 0.8293', '2) 1 0.0758', '3) 2 0.0593', '4) 3 0.0356'], ['1) 1 0.9024', '2) 2 0.0672', '3) 3 0.0191', '4) 0 0.0113'], ['1) 3 0.961', '2) 0 0.027', '3) 1 0.0077', '4) 2 0.0043'], ['1) 1 0.9668', '2) 2 0.0158', '3) 3 0.01', '4) 0 0.0074'], ['1) 3 0.7879', '2) 0 0.1458', '3) 1 0.0425', '4) 2 0.0239'], ['1) 0 0.8365', '2) 2 0.0818', '3) 3 0.0734', '4) 1 0.0084'], ['1) 1 0.4084', '2) 3 0.2401', '3) 0 0.1986', '4) 2 0.153'], ['1) 3 0.9787', '2) 0 0.0086', '3) 1 0.0085', '4) 2 0.0042'], ['1) 3 0.9843', '2) 0 0.0058', '3) 1 0.0057', '4) 2 0.0042'], ['1) 0 0.9805', '2) 3 0.0078', '3) 2 0.0062', '4) 1 0.0055'], ['1) 1 0.9659', '2) 2 0.0152', '3) 3 0.0094', '4) 0 0.0094'], ['1) 3 0.9618', '2) 0 0.0201', '3) 2 0.0093', '4) 1 0.0088'], ['1) 3 0.9809', '2) 0 0.0101', '3) 1 0.0049', '4) 2 0.004'], ['1) 1 0.372', '2) 2 0.2498', '3) 0 0.2289', '4) 3 0.1492'], ['1) 2 0.9481', '2) 1 0.0201', '3) 3 0.0164', '4) 0 0.0154'], ['1) 1 0.9405', '2) 2 0.0291', '3) 0 0.0168', '4) 3 0.0136'], ['1) 0 0.8391', '2) 2 0.1112', '3) 3 0.0382', '4) 1 0.0114'], ['1) 0 0.9235', '2) 1 0.0403', '3) 3 0.0237', '4) 2 0.0126'], ['1) 1 0.8677', '2) 2 0.0663', '3) 3 0.0441', '4) 0 0.0219'], ['1) 0 0.696', '2) 3 0.2653', '3) 1 0.0212', '4) 2 0.0176'], ['1) 1 0.8586', '2) 0 0.1168', '3) 2 0.0149', '4) 3 0.0097'], ['1) 3 0.9206', '2) 0 0.0608', '3) 2 0.0137', '4) 1 0.0049'], ['1) 2 0.6055', '2) 3 0.313', '3) 1 0.0663', '4) 0 0.0152'], ['1) 0 0.8041', '2) 3 0.1567', '3) 2 0.0278', '4) 1 0.0115'], ['1) 3 0.9802', '2) 1 0.0079', '3) 2 0.0075', '4) 0 0.0043'], ['1) 1 0.5981', '2) 2 0.2365', '3) 3 0.0976', '4) 0 0.0678'], ['1) 1 0.9236', '2) 0 0.0411', '3) 2 0.0213', '4) 3 0.0141'], ['1) 3 0.78', '2) 2 0.1747', '3) 0 0.0301', '4) 1 0.0152'], ['1) 0 0.9687', '2) 2 0.0145', '3) 1 0.0108', '4) 3 0.006'], ['1) 3 0.9481', '2) 1 0.0258', '3) 0 0.0135', '4) 2 0.0126'], ['1) 0 0.9481', '2) 2 0.0227', '3) 3 0.0178', '4) 1 0.0114'], ['1) 3 0.919', '2) 0 0.0354', '3) 1 0.0332', '4) 2 0.0124'], ['1) 0 0.9557', '2) 1 0.019', '3) 2 0.0158', '4) 3 0.0096'], ['1) 1 0.952', '2) 2 0.0286', '3) 3 0.0098', '4) 0 0.0095'], ['1) 0 0.6589', '2) 1 0.2466', '3) 2 0.0612', '4) 3 0.0333'], ['1) 0 0.4547', '2) 2 0.3669', '3) 1 0.0916', '4) 3 0.0868'], ['1) 3 0.9538', '2) 1 0.0302', '3) 2 0.0086', '4) 0 0.0073'], ['1) 3 0.9828', '2) 0 0.0079', '3) 2 0.005', '4) 1 0.0043'], ['1) 0 0.978', '2) 2 0.008', '3) 3 0.0079', '4) 1 0.0061'], ['1) 1 0.9045', '2) 2 0.0569', '3) 3 0.0238', '4) 0 0.0147'], ['1) 1 0.9687', '2) 3 0.0107', '3) 0 0.0106', '4) 2 0.0101'], ['1) 2 0.6445', '2) 3 0.29', '3) 0 0.0384', '4) 1 0.0271'], ['1) 2 0.8364', '2) 1 0.0971', '3) 3 0.0371', '4) 0 0.0293'], ['1) 2 0.5621', '2) 0 0.3756', '3) 1 0.0332', '4) 3 0.0291'], ['1) 0 0.9321', '2) 2 0.0326', '3) 3 0.0304', '4) 1 0.0049'], ['1) 1 0.9494', '2) 2 0.0245', '3) 0 0.0188', '4) 3 0.0072'], ['1) 0 0.9735', '2) 2 0.0098', '3) 3 0.0092', '4) 1 0.0076'], ['1) 3 0.8284', '2) 2 0.0927', '3) 0 0.0673', '4) 1 0.0116'], ['1) 1 0.7566', '2) 2 0.1652', '3) 0 0.0503', '4) 3 0.0279'], ['1) 1 0.9474', '2) 2 0.0249', '3) 0 0.0174', '4) 3 0.0104'], ['1) 3 0.9834', '2) 0 0.006', '3) 1 0.0054', '4) 2 0.0053'], ['1) 0 0.9722', '2) 2 0.0121', '3) 3 0.0091', '4) 1 0.0066'], ['1) 2 0.4045', '2) 0 0.355', '3) 1 0.1397', '4) 3 0.1008'], ['1) 0 0.8819', '2) 3 0.0859', '3) 2 0.0237', '4) 1 0.0085'], ['1) 0 0.969', '2) 2 0.0155', '3) 3 0.0086', '4) 1 0.007'], ['1) 3 0.973', '2) 1 0.0119', '3) 0 0.0113', '4) 2 0.0038'], ['1) 0 0.9617', '2) 3 0.019', '3) 2 0.0108', '4) 1 0.0085'], ['1) 0 0.9806', '2) 3 0.0069', '3) 1 0.0065', '4) 2 0.006'], ['1) 0 0.9652', '2) 3 0.0147', '3) 2 0.0122', '4) 1 0.0079'], ['1) 0 0.9641', '2) 2 0.0187', '3) 3 0.0109', '4) 1 0.0063'], ['1) 0 0.8317', '2) 3 0.0845', '3) 2 0.0691', '4) 1 0.0147'], ['1) 0 0.9586', '2) 2 0.0222', '3) 3 0.0142', '4) 1 0.0049'], ['1) 0 0.969', '2) 1 0.0121', '3) 3 0.0113', '4) 2 0.0076'], ['1) 0 0.9326', '2) 2 0.0382', '3) 3 0.0216', '4) 1 0.0075'], ['1) 3 0.5787', '2) 0 0.3568', '3) 1 0.0415', '4) 2 0.0231'], ['1) 3 0.6171', '2) 1 0.2986', '3) 2 0.0476', '4) 0 0.0367'], ['1) 0 0.9533', '2) 2 0.0256', '3) 3 0.0117', '4) 1 0.0094'], ['1) 0 0.9846', '2) 3 0.0063', '3) 2 0.0048', '4) 1 0.0043'], ['1) 0 0.9529', '2) 3 0.017', '3) 2 0.0153', '4) 1 0.0147'], ['1) 1 0.9216', '2) 0 0.031', '3) 2 0.0281', '4) 3 0.0193'], ['1) 1 0.8751', '2) 3 0.0528', '3) 0 0.0373', '4) 2 0.0348'], ['1) 1 0.416', '2) 2 0.3794', '3) 3 0.1748', '4) 0 0.0297'], ['1) 0 0.9598', '2) 3 0.0283', '3) 1 0.0065', '4) 2 0.0054'], ['1) 0 0.8571', '2) 2 0.0935', '3) 3 0.0323', '4) 1 0.0171'], ['1) 3 0.5918', '2) 2 0.2716', '3) 0 0.1192', '4) 1 0.0174'], ['1) 3 0.9227', '2) 0 0.0408', '3) 2 0.0304', '4) 1 0.006'], ['1) 1 0.9574', '2) 2 0.0223', '3) 0 0.0103', '4) 3 0.01'], ['1) 0 0.9529', '2) 3 0.0313', '3) 1 0.0091', '4) 2 0.0067'], ['1) 1 0.5769', '2) 3 0.2753', '3) 2 0.1314', '4) 0 0.0164'], ['1) 3 0.9498', '2) 0 0.0323', '3) 2 0.0118', '4) 1 0.0061'], ['1) 3 0.9776', '2) 2 0.0102', '3) 0 0.0067', '4) 1 0.0055'], ['1) 3 0.9855', '2) 0 0.0058', '3) 1 0.0048', '4) 2 0.0039'], ['1) 0 0.9813', '2) 2 0.0077', '3) 3 0.0057', '4) 1 0.0054'], ['1) 2 0.5477', '2) 3 0.3393', '3) 0 0.079', '4) 1 0.034'], ['1) 0 0.9558', '2) 3 0.0181', '3) 2 0.0161', '4) 1 0.0101'], ['1) 3 0.971', '2) 0 0.0199', '3) 1 0.0052', '4) 2 0.0039'], ['1) 1 0.961', '2) 2 0.0201', '3) 0 0.0125', '4) 3 0.0065'], ['1) 0 0.9316', '2) 3 0.0443', '3) 1 0.017', '4) 2 0.0071'], ['1) 3 0.8835', '2) 1 0.0831', '3) 2 0.0177', '4) 0 0.0157'], ['1) 3 0.9378', '2) 1 0.0295', '3) 0 0.0245', '4) 2 0.0082'], ['1) 0 0.9744', '2) 3 0.0134', '3) 1 0.0061', '4) 2 0.0061'], ['1) 0 0.7567', '2) 1 0.1202', '3) 3 0.0796', '4) 2 0.0435'], ['1) 0 0.9815', '2) 2 0.007', '3) 3 0.0069', '4) 1 0.0046'], ['1) 1 0.7407', '2) 2 0.1821', '3) 0 0.0519', '4) 3 0.0253'], ['1) 0 0.9639', '2) 3 0.0198', '3) 2 0.0123', '4) 1 0.0041'], ['1) 3 0.9728', '2) 0 0.0106', '3) 2 0.0094', '4) 1 0.0072'], ['1) 1 0.8484', '2) 0 0.0641', '3) 2 0.0507', '4) 3 0.0368'], ['1) 0 0.976', '2) 2 0.0113', '3) 3 0.0071', '4) 1 0.0056'], ['1) 3 0.9804', '2) 2 0.0084', '3) 0 0.0071', '4) 1 0.004'], ['1) 3 0.8733', '2) 0 0.0643', '3) 1 0.0423', '4) 2 0.0201'], ['1) 0 0.9074', '2) 3 0.0601', '3) 1 0.022', '4) 2 0.0104'], ['1) 3 0.9639', '2) 0 0.0165', '3) 2 0.0108', '4) 1 0.0089'], ['1) 1 0.9424', '2) 0 0.0238', '3) 2 0.017', '4) 3 0.0168'], ['1) 0 0.9763', '2) 3 0.0135', '3) 1 0.0058', '4) 2 0.0044'], ['1) 1 0.9667', '2) 2 0.0133', '3) 0 0.0103', '4) 3 0.0097'], ['1) 0 0.3659', '2) 1 0.2473', '3) 2 0.2363', '4) 3 0.1506'], ['1) 1 0.3839', '2) 3 0.3012', '3) 2 0.1963', '4) 0 0.1185'], ['1) 2 0.6038', '2) 0 0.3273', '3) 3 0.0437', '4) 1 0.0251'], ['1) 1 0.9635', '2) 2 0.0165', '3) 0 0.0116', '4) 3 0.0084'], ['1) 0 0.7445', '2) 2 0.1339', '3) 3 0.111', '4) 1 0.0106'], ['1) 0 0.9822', '2) 2 0.0065', '3) 1 0.0057', '4) 3 0.0056'], ['1) 0 0.9809', '2) 3 0.0072', '3) 2 0.0064', '4) 1 0.0055'], ['1) 1 0.9559', '2) 2 0.0197', '3) 3 0.0128', '4) 0 0.0115'], ['1) 2 0.7169', '2) 0 0.1518', '3) 3 0.1197', '4) 1 0.0116'], ['1) 1 0.9419', '2) 2 0.0285', '3) 0 0.0163', '4) 3 0.0133'], ['1) 0 0.9793', '2) 2 0.008', '3) 1 0.008', '4) 3 0.0047'], ['1) 0 0.9691', '2) 1 0.0142', '3) 2 0.0112', '4) 3 0.0054'], ['1) 3 0.9788', '2) 1 0.0086', '3) 0 0.0073', '4) 2 0.0053'], ['1) 2 0.7577', '2) 0 0.1448', '3) 1 0.0572', '4) 3 0.0404'], ['1) 3 0.9739', '2) 1 0.016', '3) 0 0.0056', '4) 2 0.0045'], ['1) 0 0.9683', '2) 2 0.0126', '3) 3 0.0124', '4) 1 0.0067'], ['1) 1 0.7458', '2) 3 0.1565', '3) 2 0.0708', '4) 0 0.0268'], ['1) 3 0.9638', '2) 0 0.0225', '3) 1 0.0098', '4) 2 0.0039'], ['1) 0 0.9755', '2) 3 0.0096', '3) 2 0.0095', '4) 1 0.0054'], ['1) 1 0.891', '2) 3 0.0764', '3) 2 0.0183', '4) 0 0.0143'], ['1) 3 0.4612', '2) 0 0.4274', '3) 2 0.091', '4) 1 0.0205'], ['1) 3 0.9294', '2) 0 0.0524', '3) 2 0.0099', '4) 1 0.0083'], ['1) 3 0.9742', '2) 0 0.0144', '3) 1 0.0066', '4) 2 0.0048'], ['1) 0 0.8648', '2) 2 0.1015', '3) 3 0.0189', '4) 1 0.0148'], ['1) 0 0.9277', '2) 2 0.0409', '3) 3 0.0233', '4) 1 0.0081'], ['1) 0 0.5957', '2) 3 0.2554', '3) 2 0.1333', '4) 1 0.0155'], ['1) 0 0.9204', '2) 3 0.0368', '3) 2 0.0316', '4) 1 0.0112'], ['1) 0 0.9791', '2) 3 0.0099', '3) 1 0.006', '4) 2 0.0049'], ['1) 0 0.98', '2) 3 0.0072', '3) 1 0.007', '4) 2 0.0058'], ['1) 3 0.9742', '2) 0 0.012', '3) 2 0.0073', '4) 1 0.0064'], ['1) 0 0.9346', '2) 3 0.0324', '3) 2 0.0229', '4) 1 0.01'], ['1) 1 0.9602', '2) 2 0.0215', '3) 0 0.0099', '4) 3 0.0084'], ['1) 2 0.6175', '2) 1 0.2697', '3) 3 0.0951', '4) 0 0.0177'], ['1) 1 0.8425', '2) 2 0.1038', '3) 0 0.0355', '4) 3 0.0183'], ['1) 2 0.4661', '2) 1 0.3395', '3) 0 0.0981', '4) 3 0.0963'], ['1) 0 0.9682', '2) 2 0.0141', '3) 3 0.0121', '4) 1 0.0056'], ['1) 0 0.9593', '2) 3 0.0272', '3) 2 0.0074', '4) 1 0.006'], ['1) 0 0.9534', '2) 2 0.0189', '3) 3 0.0156', '4) 1 0.0122'], ['1) 0 0.8119', '2) 3 0.1717', '3) 1 0.0099', '4) 2 0.0065'], ['1) 0 0.9538', '2) 3 0.0219', '3) 2 0.0149', '4) 1 0.0094'], ['1) 0 0.8697', '2) 3 0.1004', '3) 1 0.0225', '4) 2 0.0074'], ['1) 0 0.4128', '2) 2 0.3754', '3) 3 0.1446', '4) 1 0.0671'], ['1) 3 0.9845', '2) 0 0.0059', '3) 2 0.0054', '4) 1 0.0042'], ['1) 0 0.8124', '2) 2 0.1138', '3) 3 0.0627', '4) 1 0.0111'], ['1) 2 0.9453', '2) 3 0.021', '3) 1 0.0172', '4) 0 0.0165'], ['1) 3 0.7907', '2) 1 0.1342', '3) 0 0.0577', '4) 2 0.0174'], ['1) 1 0.9373', '2) 2 0.0302', '3) 0 0.0242', '4) 3 0.0082'], ['1) 1 0.6718', '2) 3 0.2092', '3) 2 0.0879', '4) 0 0.0311'], ['1) 3 0.8293', '2) 2 0.1016', '3) 0 0.0602', '4) 1 0.0089'], ['1) 0 0.9707', '2) 3 0.0188', '3) 2 0.0061', '4) 1 0.0043'], ['1) 1 0.9624', '2) 0 0.0158', '3) 2 0.0134', '4) 3 0.0084'], ['1) 3 0.5517', '2) 0 0.3092', '3) 2 0.1314', '4) 1 0.0076'], ['1) 0 0.9671', '2) 2 0.0173', '3) 3 0.01', '4) 1 0.0056'], ['1) 1 0.5084', '2) 2 0.3825', '3) 3 0.07', '4) 0 0.0392'], ['1) 0 0.456', '2) 3 0.2618', '3) 1 0.2423', '4) 2 0.0399'], ['1) 1 0.8948', '2) 2 0.0428', '3) 0 0.0396', '4) 3 0.0228'], ['1) 1 0.8351', '2) 2 0.0695', '3) 3 0.0554', '4) 0 0.04'], ['1) 0 0.9708', '2) 2 0.0135', '3) 3 0.0095', '4) 1 0.0062'], ['1) 1 0.7944', '2) 2 0.1387', '3) 3 0.0525', '4) 0 0.0144'], ['1) 3 0.9453', '2) 0 0.0431', '3) 1 0.0069', '4) 2 0.0047'], ['1) 0 0.877', '2) 3 0.075', '3) 2 0.0313', '4) 1 0.0167'], ['1) 3 0.7124', '2) 0 0.1454', '3) 2 0.0751', '4) 1 0.067'], ['1) 3 0.9471', '2) 1 0.0247', '3) 0 0.018', '4) 2 0.0101'], ['1) 1 0.3999', '2) 0 0.2824', '3) 3 0.2585', '4) 2 0.0591'], ['1) 3 0.8777', '2) 0 0.0953', '3) 2 0.0204', '4) 1 0.0065'], ['1) 3 0.9837', '2) 0 0.0064', '3) 1 0.0057', '4) 2 0.0042'], ['1) 2 0.9381', '2) 3 0.0316', '3) 0 0.016', '4) 1 0.0143'], ['1) 0 0.9751', '2) 2 0.0112', '3) 3 0.0083', '4) 1 0.0054'], ['1) 0 0.7639', '2) 1 0.1269', '3) 2 0.084', '4) 3 0.0252'], ['1) 1 0.93', '2) 2 0.0479', '3) 0 0.0119', '4) 3 0.0103'], ['1) 1 0.5526', '2) 3 0.2866', '3) 2 0.1227', '4) 0 0.0381'], ['1) 3 0.9632', '2) 0 0.0179', '3) 1 0.014', '4) 2 0.0049'], ['1) 1 0.9658', '2) 0 0.014', '3) 2 0.0129', '4) 3 0.0072'], ['1) 1 0.8426', '2) 2 0.113', '3) 0 0.0272', '4) 3 0.0172'], ['1) 3 0.6032', '2) 2 0.3369', '3) 0 0.0483', '4) 1 0.0116'], ['1) 3 0.8389', '2) 0 0.1054', '3) 1 0.0464', '4) 2 0.0094'], ['1) 0 0.977', '2) 2 0.0098', '3) 1 0.0068', '4) 3 0.0063'], ['1) 3 0.7665', '2) 0 0.2003', '3) 1 0.0217', '4) 2 0.0116'], ['1) 3 0.9845', '2) 0 0.0071', '3) 1 0.0051', '4) 2 0.0033'], ['1) 1 0.9709', '2) 0 0.0117', '3) 2 0.0112', '4) 3 0.0062'], ['1) 0 0.9017', '2) 2 0.0498', '3) 1 0.0306', '4) 3 0.018'], ['1) 1 0.9294', '2) 3 0.035', '3) 2 0.0212', '4) 0 0.0144'], ['1) 1 0.9678', '2) 2 0.0137', '3) 0 0.0104', '4) 3 0.0081'], ['1) 0 0.9567', '2) 1 0.0162', '3) 3 0.0142', '4) 2 0.0129'], ['1) 1 0.9723', '2) 2 0.0122', '3) 0 0.0081', '4) 3 0.0073'], ['1) 2 0.753', '2) 0 0.1327', '3) 3 0.1013', '4) 1 0.013'], ['1) 1 0.8799', '2) 0 0.0599', '3) 2 0.0394', '4) 3 0.0208'], ['1) 0 0.8947', '2) 2 0.0626', '3) 3 0.0371', '4) 1 0.0057'], ['1) 0 0.8635', '2) 1 0.1033', '3) 2 0.022', '4) 3 0.0112'], ['1) 0 0.636', '2) 3 0.3341', '3) 1 0.02', '4) 2 0.0098'], ['1) 1 0.9632', '2) 2 0.0133', '3) 0 0.0128', '4) 3 0.0106'], ['1) 3 0.9677', '2) 0 0.0207', '3) 2 0.0062', '4) 1 0.0054'], ['1) 0 0.9771', '2) 2 0.01', '3) 3 0.0072', '4) 1 0.0057'], ['1) 3 0.9844', '2) 0 0.0059', '3) 1 0.0058', '4) 2 0.0039'], ['1) 1 0.9683', '2) 2 0.0136', '3) 0 0.01', '4) 3 0.008'], ['1) 3 0.8687', '2) 2 0.0867', '3) 0 0.032', '4) 1 0.0126'], ['1) 0 0.9855', '2) 2 0.0053', '3) 1 0.0049', '4) 3 0.0043'], ['1) 0 0.9754', '2) 1 0.0097', '3) 2 0.0077', '4) 3 0.0072'], ['1) 1 0.6718', '2) 2 0.2013', '3) 3 0.1131', '4) 0 0.0139'], ['1) 0 0.9365', '2) 3 0.0335', '3) 1 0.0178', '4) 2 0.0122'], ['1) 0 0.974', '2) 2 0.014', '3) 3 0.0069', '4) 1 0.005'], ['1) 0 0.9812', '2) 3 0.0072', '3) 2 0.0063', '4) 1 0.0053'], ['1) 3 0.9357', '2) 2 0.0448', '3) 0 0.0149', '4) 1 0.0045'], ['1) 1 0.3968', '2) 0 0.2443', '3) 2 0.1898', '4) 3 0.1692'], ['1) 3 0.9783', '2) 1 0.0118', '3) 0 0.0066', '4) 2 0.0033'], ['1) 1 0.9344', '2) 3 0.0284', '3) 2 0.0206', '4) 0 0.0166'], ['1) 3 0.8416', '2) 0 0.0985', '3) 2 0.0371', '4) 1 0.0229'], ['1) 3 0.8448', '2) 0 0.1256', '3) 2 0.0207', '4) 1 0.0089'], ['1) 3 0.8358', '2) 2 0.0996', '3) 0 0.0555', '4) 1 0.0091'], ['1) 0 0.9797', '2) 3 0.0082', '3) 1 0.0064', '4) 2 0.0058'], ['1) 2 0.8523', '2) 1 0.0575', '3) 3 0.0568', '4) 0 0.0334'], ['1) 0 0.8495', '2) 2 0.079', '3) 3 0.0634', '4) 1 0.0081'], ['1) 1 0.963', '2) 2 0.0163', '3) 3 0.0105', '4) 0 0.0102'], ['1) 1 0.9288', '2) 2 0.0464', '3) 3 0.0146', '4) 0 0.0103'], ['1) 0 0.9311', '2) 3 0.0577', '3) 1 0.0064', '4) 2 0.0048'], ['1) 1 0.727', '2) 2 0.1276', '3) 3 0.1249', '4) 0 0.0204'], ['1) 2 0.7026', '2) 1 0.1851', '3) 3 0.096', '4) 0 0.0163'], ['1) 0 0.9512', '2) 2 0.0273', '3) 3 0.0149', '4) 1 0.0065'], ['1) 0 0.5244', '2) 3 0.4402', '3) 2 0.0199', '4) 1 0.0156'], ['1) 3 0.9826', '2) 1 0.0091', '3) 0 0.0046', '4) 2 0.0036'], ['1) 1 0.9599', '2) 2 0.0152', '3) 0 0.0147', '4) 3 0.0102'], ['1) 3 0.3872', '2) 1 0.3455', '3) 0 0.164', '4) 2 0.1033'], ['1) 2 0.7319', '2) 0 0.2209', '3) 3 0.0377', '4) 1 0.0095'], ['1) 1 0.6383', '2) 2 0.2362', '3) 0 0.0851', '4) 3 0.0404'], ['1) 1 0.9442', '2) 3 0.0338', '3) 2 0.0143', '4) 0 0.0077'], ['1) 0 0.9752', '2) 2 0.0105', '3) 3 0.0079', '4) 1 0.0064'], ['1) 3 0.9838', '2) 1 0.0077', '3) 0 0.0054', '4) 2 0.0031'], ['1) 3 0.9761', '2) 0 0.0137', '3) 2 0.0051', '4) 1 0.0051'], ['1) 0 0.794', '2) 3 0.1296', '3) 2 0.0614', '4) 1 0.015'], ['1) 3 0.9283', '2) 1 0.0519', '3) 2 0.0107', '4) 0 0.0091'], ['1) 0 0.9482', '2) 1 0.0209', '3) 2 0.0192', '4) 3 0.0117'], ['1) 0 0.9121', '2) 2 0.0406', '3) 3 0.0396', '4) 1 0.0077'], ['1) 0 0.9628', '2) 3 0.0144', '3) 1 0.0124', '4) 2 0.0104'], ['1) 0 0.9426', '2) 1 0.0345', '3) 3 0.0161', '4) 2 0.0069'], ['1) 0 0.9814', '2) 2 0.0083', '3) 1 0.0053', '4) 3 0.005'], ['1) 2 0.5996', '2) 1 0.2071', '3) 3 0.1743', '4) 0 0.019'], ['1) 0 0.9359', '2) 3 0.0317', '3) 2 0.0254', '4) 1 0.007'], ['1) 0 0.9001', '2) 2 0.0675', '3) 3 0.0238', '4) 1 0.0085'], ['1) 0 0.789', '2) 3 0.1246', '3) 2 0.0773', '4) 1 0.0091'], ['1) 3 0.9458', '2) 1 0.0356', '3) 2 0.0109', '4) 0 0.0077'], ['1) 1 0.7906', '2) 0 0.12', '3) 2 0.0458', '4) 3 0.0437'], ['1) 2 0.695', '2) 3 0.1738', '3) 0 0.1196', '4) 1 0.0116'], ['1) 1 0.9642', '2) 2 0.0176', '3) 0 0.0108', '4) 3 0.0074'], ['1) 3 0.9274', '2) 2 0.0317', '3) 0 0.0284', '4) 1 0.0125'], ['1) 2 0.6818', '2) 0 0.1534', '3) 3 0.0985', '4) 1 0.0664'], ['1) 0 0.956', '2) 3 0.0206', '3) 1 0.0134', '4) 2 0.01'], ['1) 0 0.9056', '2) 3 0.0822', '3) 2 0.0077', '4) 1 0.0045'], ['1) 2 0.9288', '2) 3 0.0411', '3) 0 0.0152', '4) 1 0.0149'], ['1) 2 0.372', '2) 1 0.2852', '3) 3 0.2452', '4) 0 0.0976'], ['1) 0 0.9629', '2) 3 0.0229', '3) 1 0.0101', '4) 2 0.0042'], ['1) 0 0.9569', '2) 3 0.0266', '3) 2 0.0088', '4) 1 0.0076'], ['1) 0 0.832', '2) 1 0.1183', '3) 2 0.032', '4) 3 0.0177'], ['1) 3 0.8185', '2) 0 0.1084', '3) 2 0.0574', '4) 1 0.0156'], ['1) 0 0.9809', '2) 2 0.0072', '3) 1 0.0067', '4) 3 0.0052'], ['1) 0 0.9805', '2) 1 0.0072', '3) 2 0.0071', '4) 3 0.0051'], ['1) 1 0.9531', '2) 3 0.019', '3) 2 0.0173', '4) 0 0.0106'], ['1) 0 0.9322', '2) 3 0.0574', '3) 2 0.0053', '4) 1 0.0052'], ['1) 1 0.8855', '2) 2 0.0795', '3) 0 0.0184', '4) 3 0.0166'], ['1) 3 0.397', '2) 1 0.3798', '3) 2 0.1193', '4) 0 0.1039'], ['1) 3 0.9647', '2) 0 0.0175', '3) 2 0.0133', '4) 1 0.0045'], ['1) 3 0.6142', '2) 0 0.3296', '3) 2 0.0391', '4) 1 0.0172'], ['1) 3 0.9832', '2) 1 0.0085', '3) 0 0.0052', '4) 2 0.0031'], ['1) 3 0.8345', '2) 1 0.1435', '3) 2 0.0154', '4) 0 0.0066'], ['1) 1 0.9594', '2) 0 0.0172', '3) 2 0.013', '4) 3 0.0105'], ['1) 3 0.9182', '2) 0 0.0434', '3) 2 0.0295', '4) 1 0.0088'], ['1) 0 0.7132', '2) 3 0.2593', '3) 1 0.0168', '4) 2 0.0107'], ['1) 3 0.9132', '2) 0 0.0372', '3) 1 0.0278', '4) 2 0.0218'], ['1) 3 0.6728', '2) 1 0.2575', '3) 2 0.0536', '4) 0 0.016'], ['1) 3 0.918', '2) 1 0.0392', '3) 0 0.0241', '4) 2 0.0188'], ['1) 3 0.9702', '2) 0 0.0162', '3) 2 0.0099', '4) 1 0.0037'], ['1) 3 0.7195', '2) 0 0.1211', '3) 1 0.1185', '4) 2 0.0409'], ['1) 0 0.9518', '2) 3 0.0215', '3) 2 0.018', '4) 1 0.0088'], ['1) 3 0.6316', '2) 1 0.2908', '3) 2 0.0664', '4) 0 0.0113'], ['1) 0 0.9644', '2) 3 0.0194', '3) 1 0.0097', '4) 2 0.0065'], ['1) 1 0.9696', '2) 0 0.0112', '3) 2 0.0103', '4) 3 0.009'], ['1) 0 0.9774', '2) 3 0.009', '3) 1 0.0072', '4) 2 0.0065'], ['1) 0 0.9758', '2) 2 0.011', '3) 3 0.0069', '4) 1 0.0063'], ['1) 0 0.5495', '2) 1 0.2593', '3) 3 0.1756', '4) 2 0.0156'], ['1) 0 0.9713', '2) 3 0.0113', '3) 2 0.0096', '4) 1 0.0077'], ['1) 0 0.9797', '2) 2 0.0094', '3) 1 0.0055', '4) 3 0.0054'], ['1) 1 0.9594', '2) 0 0.0217', '3) 2 0.0113', '4) 3 0.0076'], ['1) 1 0.9338', '2) 2 0.028', '3) 3 0.0191', '4) 0 0.0191'], ['1) 3 0.9573', '2) 0 0.0235', '3) 1 0.0145', '4) 2 0.0047'], ['1) 0 0.8999', '2) 3 0.0717', '3) 2 0.0198', '4) 1 0.0087'], ['1) 0 0.7268', '2) 2 0.2049', '3) 3 0.0574', '4) 1 0.0109'], ['1) 0 0.8375', '2) 3 0.1277', '3) 1 0.0265', '4) 2 0.0082'], ['1) 0 0.9665', '2) 2 0.0195', '3) 3 0.0079', '4) 1 0.0061'], ['1) 3 0.9479', '2) 1 0.0291', '3) 0 0.012', '4) 2 0.0111'], ['1) 3 0.979', '2) 1 0.0079', '3) 0 0.0066', '4) 2 0.0066'], ['1) 2 0.5867', '2) 0 0.3496', '3) 3 0.0508', '4) 1 0.0128'], ['1) 1 0.8303', '2) 3 0.1252', '3) 2 0.0264', '4) 0 0.018'], ['1) 1 0.7028', '2) 2 0.1506', '3) 0 0.1174', '4) 3 0.0291'], ['1) 3 0.9863', '2) 1 0.0054', '3) 0 0.0053', '4) 2 0.003'], ['1) 2 0.6', '2) 0 0.2599', '3) 3 0.1136', '4) 1 0.0266'], ['1) 0 0.9824', '2) 3 0.0076', '3) 2 0.0054', '4) 1 0.0046'], ['1) 1 0.904', '2) 2 0.0467', '3) 0 0.0367', '4) 3 0.0127'], ['1) 3 0.9843', '2) 0 0.0068', '3) 1 0.0058', '4) 2 0.0031'], ['1) 0 0.8897', '2) 2 0.0574', '3) 3 0.0473', '4) 1 0.0056'], ['1) 0 0.9647', '2) 3 0.0224', '3) 2 0.0085', '4) 1 0.0044'], ['1) 0 0.9778', '2) 2 0.0084', '3) 3 0.0071', '4) 1 0.0067'], ['1) 3 0.9821', '2) 1 0.0074', '3) 0 0.0069', '4) 2 0.0037'], ['1) 0 0.8228', '2) 3 0.1431', '3) 1 0.0194', '4) 2 0.0147'], ['1) 1 0.9715', '2) 0 0.0099', '3) 2 0.0097', '4) 3 0.0089'], ['1) 3 0.8489', '2) 0 0.0975', '3) 2 0.0372', '4) 1 0.0164'], ['1) 0 0.9673', '2) 2 0.0171', '3) 3 0.0097', '4) 1 0.0059'], ['1) 0 0.9373', '2) 3 0.0493', '3) 1 0.0068', '4) 2 0.0066'], ['1) 0 0.984', '2) 2 0.0067', '3) 3 0.0055', '4) 1 0.0038'], ['1) 3 0.981', '2) 1 0.0085', '3) 0 0.0056', '4) 2 0.005'], ['1) 1 0.9551', '2) 2 0.0214', '3) 3 0.0147', '4) 0 0.0088'], ['1) 0 0.9632', '2) 2 0.017', '3) 3 0.0124', '4) 1 0.0073'], ['1) 3 0.9823', '2) 0 0.0065', '3) 1 0.006', '4) 2 0.0052'], ['1) 3 0.9325', '2) 0 0.0375', '3) 1 0.0247', '4) 2 0.0053'], ['1) 0 0.9661', '2) 3 0.0198', '3) 1 0.0104', '4) 2 0.0037'], ['1) 0 0.9728', '2) 2 0.0128', '3) 3 0.0085', '4) 1 0.0059'], ['1) 2 0.9445', '2) 3 0.0238', '3) 0 0.0195', '4) 1 0.0122'], ['1) 3 0.9838', '2) 0 0.0074', '3) 1 0.0056', '4) 2 0.0032'], ['1) 3 0.967', '2) 1 0.017', '3) 0 0.0127', '4) 2 0.0033'], ['1) 3 0.9557', '2) 0 0.0259', '3) 2 0.0127', '4) 1 0.0056'], ['1) 2 0.4795', '2) 1 0.4056', '3) 0 0.079', '4) 3 0.0359'], ['1) 2 0.5168', '2) 1 0.268', '3) 3 0.1662', '4) 0 0.0491'], ['1) 3 0.9555', '2) 1 0.0174', '3) 0 0.017', '4) 2 0.0102'], ['1) 3 0.9772', '2) 1 0.01', '3) 0 0.0072', '4) 2 0.0057'], ['1) 2 0.9042', '2) 3 0.0474', '3) 1 0.0283', '4) 0 0.0201'], ['1) 1 0.9167', '2) 2 0.0515', '3) 3 0.0173', '4) 0 0.0144'], ['1) 3 0.9203', '2) 2 0.0557', '3) 0 0.0184', '4) 1 0.0057'], ['1) 3 0.8903', '2) 2 0.0482', '3) 0 0.035', '4) 1 0.0265'], ['1) 1 0.9708', '2) 0 0.0118', '3) 2 0.0096', '4) 3 0.0078'], ['1) 1 0.6347', '2) 3 0.3189', '3) 2 0.0263', '4) 0 0.0201'], ['1) 3 0.875', '2) 0 0.1022', '3) 2 0.0161', '4) 1 0.0067'], ['1) 0 0.8098', '2) 1 0.0856', '3) 3 0.0805', '4) 2 0.0241'], ['1) 3 0.9532', '2) 2 0.025', '3) 1 0.0164', '4) 0 0.0054'], ['1) 1 0.946', '2) 2 0.0198', '3) 3 0.0184', '4) 0 0.0157'], ['1) 2 0.6502', '2) 0 0.1718', '3) 3 0.1609', '4) 1 0.0171'], ['1) 1 0.8935', '2) 0 0.0554', '3) 3 0.0272', '4) 2 0.024'], ['1) 3 0.8283', '2) 0 0.1311', '3) 2 0.0274', '4) 1 0.0131'], ['1) 0 0.9527', '2) 3 0.0232', '3) 2 0.0181', '4) 1 0.006'], ['1) 0 0.9188', '2) 3 0.0399', '3) 2 0.0347', '4) 1 0.0066'], ['1) 1 0.8893', '2) 0 0.0598', '3) 2 0.0265', '4) 3 0.0244'], ['1) 3 0.9807', '2) 2 0.0078', '3) 0 0.0065', '4) 1 0.005'], ['1) 3 0.947', '2) 1 0.0365', '3) 2 0.0111', '4) 0 0.0054'], ['1) 1 0.8903', '2) 0 0.0498', '3) 3 0.0301', '4) 2 0.0297'], ['1) 0 0.9712', '2) 3 0.0141', '3) 2 0.0087', '4) 1 0.006'], ['1) 3 0.9745', '2) 0 0.0126', '3) 1 0.0093', '4) 2 0.0036'], ['1) 0 0.6057', '2) 3 0.2644', '3) 2 0.1178', '4) 1 0.0121'], ['1) 1 0.9279', '2) 3 0.0397', '3) 2 0.0248', '4) 0 0.0076'], ['1) 0 0.7657', '2) 3 0.199', '3) 1 0.0293', '4) 2 0.006'], ['1) 3 0.9853', '2) 0 0.0065', '3) 1 0.0051', '4) 2 0.003'], ['1) 1 0.432', '2) 2 0.2888', '3) 3 0.1666', '4) 0 0.1126'], ['1) 3 0.9834', '2) 0 0.0076', '3) 1 0.0049', '4) 2 0.0041'], ['1) 0 0.8599', '2) 3 0.1134', '3) 1 0.0174', '4) 2 0.0092'], ['1) 0 0.9766', '2) 2 0.0109', '3) 1 0.0075', '4) 3 0.0051'], ['1) 1 0.9035', '2) 2 0.066', '3) 0 0.0181', '4) 3 0.0124']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXCviEBHami0"
      },
      "source": [
        "\n",
        "#-------------------Saving the results of the model in CSV file to save time ------------\n",
        "with open(\"emotion_model_result.csv\", 'w') as csvfile: \n",
        "    # creating a csv writer object \n",
        "    csvwriter = csv.writer(csvfile)  \n",
        "    csvwriter.writerows(result)      \n",
        "\n",
        "  "
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ubgvCWfA_uc",
        "outputId": "dd8793e5-7d2c-4a76-892a-acc5472853a9"
      },
      "source": [
        "\n",
        "final_result =[]\n",
        "predicted_data=[] # The result we are getting by running the model\n",
        "for i in  range(0,len(result)):\n",
        "  final_result.append(min(result[i]))\n",
        "\n",
        "for i in range(0,len(final_result)):\n",
        "  extract_data=int(final_result[i][3])\n",
        "  predicted_data.append(extract_data)\n",
        "  extract_data= 0\n",
        "  print(\"Predicted value: \" , predicted_data[i])\n",
        "  print(type(predicted_data[i]))\n",
        "  print (\"Test value: \" , test_data[i])\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  2\n",
            "<class 'int'>\n",
            "Test value:  2\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n",
            "Predicted value:  3\n",
            "<class 'int'>\n",
            "Test value:  3\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  0\n",
            "<class 'int'>\n",
            "Test value:  0\n",
            "\n",
            "Predicted value:  1\n",
            "<class 'int'>\n",
            "Test value:  1\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBWZRlvy2q-T"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UZbVNaZ3AW1"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3RTUZgWbtAU"
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX5M2Cv42YS-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c44938bd-5522-4005-cd98-77271513b342"
      },
      "source": [
        "print(test_data)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['3\\n', '0\\n', '3\\n', '1\\n', '1\\n', '0\\n', '3\\n', '3\\n', '3\\n', '0\\n', '0\\n', '3\\n', '3\\n', '0\\n', '0\\n', '3\\n', '1\\n', '0\\n', '0\\n', '0\\n', '0\\n', '3\\n', '3\\n', '2\\n', '2\\n', '1\\n', '1\\n', '0\\n', '3\\n', '1\\n', '3\\n', '1\\n', '0\\n', '0\\n', '0\\n', '0\\n', '2\\n', '2\\n', '0\\n', '3\\n', '2\\n', '0\\n', '0\\n', '1\\n', '0\\n', '0\\n', '0\\n', '0\\n', '1\\n', '0\\n', '3\\n', '3\\n', '2\\n', '3\\n', '1\\n', '0\\n', '3\\n', '3\\n', '2\\n', '0\\n', '0\\n', '1\\n', '0\\n', '0\\n', '0\\n', '0\\n', '1\\n', '0\\n', '0\\n', '0\\n', '0\\n', '3\\n', '0\\n', '0\\n', '0\\n', '0\\n', '3\\n', '2\\n', '2\\n', '1\\n', '0\\n', '0\\n', '1\\n', '3\\n', '0\\n', '0\\n', '0\\n', '1\\n', '1\\n', '3\\n', '3\\n', '1\\n', '1\\n', '0\\n', '1\\n', '1\\n', '1\\n', '3\\n', '0\\n', '0\\n', '0\\n', '1\\n', '3\\n', '1\\n', '0\\n', '1\\n', '0\\n', '2\\n', '1\\n', '0\\n', '3\\n', '0\\n', '0\\n', '0\\n', '3\\n', '3\\n', '3\\n', '1\\n', '0\\n', '2\\n', '1\\n', '0\\n', '1\\n', '0\\n', '0\\n', '1\\n', '0\\n', '0\\n', '0\\n', '3\\n', '1\\n', '0\\n', '0\\n', '1\\n', '1\\n', '1\\n', '2\\n', '3\\n', '3\\n', '0\\n', '3\\n', '3\\n', '0\\n', '0\\n', '3\\n', '2\\n', '0\\n', '0\\n', '0\\n', '1\\n', '2\\n', '0\\n', '0\\n', '3\\n', '0\\n', '1\\n', '3\\n', '1\\n', '3\\n', '0\\n', '3\\n', '0\\n', '2\\n', '1\\n', '1\\n', '1\\n', '0\\n', '3\\n', '0\\n', '1\\n', '0\\n', '0\\n', '2\\n', '1\\n', '2\\n', '0\\n', '0\\n', '0\\n', '1\\n', '1\\n', '1\\n', '0\\n', '0\\n', '3\\n', '3\\n', '0\\n', '3\\n', '3\\n', '3\\n', '3\\n', '0\\n', '0\\n', '2\\n', '3\\n', '2\\n', '1\\n', '1\\n', '0\\n', '0\\n', '3\\n', '1\\n', '0\\n', '0\\n', '0\\n', '1\\n', '1\\n', '0\\n', '0\\n', '3\\n', '3\\n', '1\\n', '3\\n', '2\\n', '1\\n', '1\\n', '3\\n', '0\\n', '0\\n', '3\\n', '0\\n', '2\\n', '3\\n', '3\\n', '3\\n', '3\\n', '0\\n', '2\\n', '3\\n', '0\\n', '0\\n', '0\\n', '1\\n', '0\\n', '0\\n', '0\\n', '0\\n', '0\\n', '3\\n', '0\\n', '1\\n', '1\\n', '0\\n', '3\\n', '0\\n', '1\\n', '3\\n', '3\\n', '3\\n', '3\\n', '1\\n', '1\\n', '3\\n', '0\\n', '0\\n', '3\\n', '0\\n', '1\\n', '1\\n', '3\\n', '0\\n', '3\\n', '1\\n', '0\\n', '2\\n', '0\\n', '2\\n', '0\\n', '3\\n', '1\\n', '0\\n', '3\\n', '0\\n', '0\\n', '3\\n', '3\\n', '1\\n', '0\\n', '3\\n', '3\\n', '3\\n', '3\\n', '3\\n', '0\\n', '2\\n', '3\\n', '0\\n', '0\\n', '0\\n', '1\\n', '0\\n', '0\\n', '0\\n', '3\\n', '3\\n', '1\\n', '3\\n', '2\\n', '1\\n', '3\\n', '1\\n', '0\\n', '1\\n', '0\\n', '0\\n', '3\\n', '1\\n', '3\\n', '0\\n', '1\\n', '3\\n', '3\\n', '0\\n', '3\\n', '0\\n', '1\\n', '0\\n', '1\\n', '3\\n', '3\\n', '3\\n', '0\\n', '3\\n', '3\\n', '0\\n', '0\\n', '1\\n', '0\\n', '0\\n', '0\\n', '3\\n', '2\\n', '3\\n', '0\\n', '2\\n', '2\\n', '3\\n', '3\\n', '1\\n', '3\\n', '3\\n', '1\\n', '0\\n', '1\\n', '3\\n', '0\\n', '1\\n', '1\\n', '2\\n', '0\\n', '3\\n', '0\\n', '1\\n', '3\\n', '3\\n', '0\\n', '1\\n', '0\\n', '1\\n', '0\\n', '1\\n', '1\\n', '3\\n', '0\\n', '0\\n', '0\\n', '0\\n', '2\\n', '0\\n', '0\\n', '1\\n', '0\\n', '2\\n', '3\\n', '1\\n', '0\\n', '2\\n', '3\\n', '1\\n', '3\\n', '0\\n', '2\\n', '3\\n', '0\\n', '3\\n', '1\\n', '3\\n', '3\\n', '1\\n', '0\\n', '2\\n', '0\\n', '1\\n', '1\\n', '3\\n', '1\\n', '1\\n', '3\\n', '1\\n', '1\\n', '3\\n', '0\\n', '0\\n', '0\\n', '0\\n', '2\\n', '3\\n', '0\\n', '1\\n', '0\\n', '2\\n', '3\\n', '0\\n', '1\\n', '1\\n', '1\\n', '1\\n', '3\\n', '0\\n', '0\\n', '2\\n', '1\\n', '1\\n', '0\\n', '3\\n', '1\\n', '0\\n', '3\\n', '1\\n', '3\\n', '0\\n', '3\\n', '3\\n', '3\\n', '1\\n', '3\\n', '3\\n', '1\\n', '0\\n', '0\\n', '1\\n', '1\\n', '0\\n', '3\\n', '3\\n', '0\\n', '2\\n', '3\\n', '1\\n', '0\\n', '3\\n', '3\\n', '3\\n', '0\\n', '3\\n', '3\\n', '2\\n', '3\\n', '0\\n', '2\\n', '3\\n', '0\\n', '0\\n', '1\\n', '1\\n', '3\\n', '1\\n', '0\\n', '0\\n', '1\\n', '1\\n', '0\\n', '3\\n', '0\\n', '0\\n', '0\\n', '3\\n', '3\\n', '0\\n', '3\\n', '0\\n', '0\\n', '1\\n', '1\\n', '3\\n', '3\\n', '3\\n', '0\\n', '1\\n', '1\\n', '1\\n', '1\\n', '3\\n', '3\\n', '3\\n', '2\\n', '0\\n', '2\\n', '1\\n', '0\\n', '3\\n', '3\\n', '3\\n', '3\\n', '0\\n', '2\\n', '3\\n', '0\\n', '1\\n', '0\\n', '0\\n', '2\\n', '1\\n', '1\\n', '0\\n', '0\\n', '3\\n', '0\\n', '3\\n', '0\\n', '1\\n', '0\\n', '0\\n', '1\\n', '0\\n', '3\\n', '1\\n', '0\\n', '0\\n', '0\\n', '3\\n', '1\\n', '3\\n', '3\\n', '0\\n', '0\\n', '1\\n', '3\\n', '1\\n', '0\\n', '1\\n', '1\\n', '3\\n', '0\\n', '0\\n', '0\\n', '3\\n', '0\\n', '1\\n', '3\\n', '3\\n', '0\\n', '1\\n', '1\\n', '1\\n', '3\\n', '0\\n', '2\\n', '0\\n', '1\\n', '2\\n', '3\\n', '1\\n', '2\\n', '2\\n', '1\\n', '1\\n', '0\\n', '0\\n', '0\\n', '0\\n', '3\\n', '3\\n', '1\\n', '1\\n', '1\\n', '0\\n', '2\\n', '0\\n', '1\\n', '3\\n', '0\\n', '1\\n', '0\\n', '1\\n', '1\\n', '1\\n', '3\\n', '0\\n', '0\\n', '1\\n', '0\\n', '0\\n', '0\\n', '1\\n', '3\\n', '0\\n', '0\\n', '1\\n', '1\\n', '1\\n', '2\\n', '2\\n', '1\\n', '0\\n', '0\\n', '2\\n', '3\\n', '1\\n', '0\\n', '3\\n', '3\\n', '0\\n', '1\\n', '3\\n', '1\\n', '2\\n', '1\\n', '1\\n', '0\\n', '1\\n', '2\\n', '0\\n', '0\\n', '3\\n', '0\\n', '0\\n', '0\\n', '3\\n', '0\\n', '0\\n', '1\\n', '1\\n', '0\\n', '3\\n', '1\\n', '0\\n', '0\\n', '0\\n', '3\\n', '1\\n', '1\\n', '3\\n', '0\\n', '0\\n', '0\\n', '3\\n', '1\\n', '0\\n', '1\\n', '0\\n', '0\\n', '3\\n', '3\\n', '1\\n', '3\\n', '0\\n', '1\\n', '3\\n', '3\\n', '1\\n', '1\\n', '3\\n', '0\\n', '0\\n', '3\\n', '2\\n', '3\\n', '1\\n', '3\\n', '0\\n', '0\\n', '0\\n', '1\\n', '0\\n', '1\\n', '2\\n', '1\\n', '0\\n', '1\\n', '0\\n', '1\\n', '1\\n', '0\\n', '3\\n', '0\\n', '0\\n', '0\\n', '3\\n', '0\\n', '3\\n', '0\\n', '1\\n', '0\\n', '0\\n', '0\\n', '1\\n', '0\\n', '1\\n', '3\\n', '1\\n', '3\\n', '1\\n', '0\\n', '1\\n', '0\\n', '2\\n', '3\\n', '0\\n', '0\\n', '3\\n', '2\\n', '2\\n', '3\\n', '1\\n', '0\\n', '3\\n', '3\\n', '3\\n', '0\\n', '3\\n', '3\\n', '0\\n', '3\\n', '1\\n', '1\\n', '3\\n', '0\\n', '1\\n', '3\\n', '3\\n', '3\\n', '1\\n', '1\\n', '0\\n', '2\\n', '1\\n', '0\\n', '3\\n', '0\\n', '3\\n', '3\\n', '1\\n', '0\\n', '2\\n', '1\\n', '0\\n', '0\\n', '0\\n', '1\\n', '0\\n', '2\\n', '0\\n', '0\\n', '0\\n', '3\\n', '0\\n', '1\\n', '0\\n', '1\\n', '0\\n', '0\\n', '1\\n', '3\\n', '1\\n', '1\\n', '1\\n', '1\\n', '1\\n', '3\\n', '0\\n', '0\\n', '0\\n', '2\\n', '0\\n', '1\\n', '3\\n', '1\\n', '3\\n', '1\\n', '0\\n', '0\\n', '2\\n', '1\\n', '1\\n', '0\\n', '0\\n', '3\\n', '3\\n', '3\\n', '3\\n', '1\\n', '1\\n', '1\\n', '2\\n', '2\\n', '0\\n', '3\\n', '0\\n', '3\\n', '0\\n', '1\\n', '0\\n', '0\\n', '0\\n', '1\\n', '1\\n', '1\\n', '2\\n', '0\\n', '1\\n', '2\\n', '0\\n', '0\\n', '0\\n', '0\\n', '0\\n', '1\\n', '1\\n', '1\\n', '3\\n', '0\\n', '3\\n', '0\\n', '1\\n', '0\\n', '3\\n', '0\\n', '3\\n', '0\\n', '0\\n', '3\\n', '0\\n', '0\\n', '3\\n', '3\\n', '2\\n', '1\\n', '3\\n', '1\\n', '0\\n', '1\\n', '1\\n', '0\\n', '0\\n', '3\\n', '1\\n', '0\\n', '3\\n', '0\\n', '0\\n', '0\\n', '0\\n', '0\\n', '2\\n', '3\\n', '0\\n', '0\\n', '0\\n', '1\\n', '1\\n', '3\\n', '0\\n', '3\\n', '0\\n', '0\\n', '2\\n', '3\\n', '1\\n', '3\\n', '1\\n', '0\\n', '1\\n', '0\\n', '3\\n', '1\\n', '0\\n', '0\\n', '1\\n', '0\\n', '1\\n', '1\\n', '1\\n', '0\\n', '0\\n', '0\\n', '0\\n', '1\\n', '1\\n', '0\\n', '1\\n', '0\\n', '0\\n', '0\\n', '2\\n', '0\\n', '3\\n', '3\\n', '0\\n', '0\\n', '2\\n', '0\\n', '1\\n', '1\\n', '1\\n', '0\\n', '2\\n', '0\\n', '3\\n', '3\\n', '0\\n', '3\\n', '0\\n', '0\\n', '1\\n', '3\\n', '3\\n', '0\\n', '0\\n', '3\\n', '1\\n', '1\\n', '2\\n', '0\\n', '3\\n', '3\\n', '3\\n', '3\\n', '3\\n', '0\\n', '1\\n', '0\\n', '0\\n', '0\\n', '0\\n', '3\\n', '0\\n', '0\\n', '0\\n', '1\\n', '1\\n', '3\\n', '3\\n', '3\\n', '0\\n', '1\\n', '2\\n', '0\\n', '0\\n', '0\\n', '0\\n', '3\\n', '3\\n', '1\\n', '3\\n', '2\\n', '1\\n', '3\\n', '0\\n', '1\\n', '0\\n', '1\\n', '3\\n', '1\\n', '0\\n', '1\\n', '2\\n', '0\\n', '2\\n', '0\\n', '2\\n', '0\\n', '2\\n', '2\\n', '0\\n', '0\\n', '0\\n', '2\\n', '0\\n', '3\\n', '3\\n', '1\\n', '2\\n', '3\\n', '0\\n', '3\\n', '0\\n', '0\\n', '0\\n', '3\\n', '3\\n', '3\\n', '3\\n', '0\\n', '1\\n', '3\\n', '0\\n', '0\\n', '2\\n', '0\\n', '3\\n', '1\\n', '1\\n', '3\\n', '3\\n', '1\\n', '3\\n', '0\\n', '2\\n', '1\\n', '1\\n', '1\\n', '1\\n', '3\\n', '0\\n', '3\\n', '2\\n', '1\\n', '1\\n', '0\\n', '0\\n', '0\\n', '0\\n', '3\\n', '0\\n', '1\\n', '1\\n', '0\\n', '0\\n', '0\\n', '1\\n', '0\\n', '3\\n', '1\\n', '0\\n', '3\\n', '0\\n', '0\\n', '0\\n', '1\\n', '3\\n', '1\\n', '2\\n', '0\\n', '2\\n', '3\\n', '1\\n', '3\\n', '0\\n', '3\\n', '1\\n', '0\\n', '1\\n', '3\\n', '0\\n', '0\\n', '0\\n', '0\\n', '1\\n', '3\\n', '0\\n', '3\\n', '3\\n', '1\\n', '3\\n', '0\\n', '1\\n', '3\\n', '1\\n', '3\\n', '2\\n', '1\\n', '3\\n', '3\\n', '0\\n', '1\\n', '3\\n', '3\\n', '3\\n', '2\\n', '1\\n', '0\\n', '0\\n', '1\\n', '2\\n', '1\\n', '3\\n', '3\\n', '0\\n', '3\\n', '1\\n', '1\\n', '3\\n', '0\\n', '3\\n', '0\\n', '3\\n', '0\\n', '1\\n', '0\\n', '2\\n', '3\\n', '3\\n', '0\\n', '1\\n', '1\\n', '2\\n', '2\\n', '2\\n', '0\\n', '1\\n', '0\\n', '3\\n', '1\\n', '1\\n', '3\\n', '0\\n', '2\\n', '0\\n', '0\\n', '3\\n', '0\\n', '0\\n', '0\\n', '0\\n', '0\\n', '0\\n', '0\\n', '0\\n', '3\\n', '1\\n', '0\\n', '0\\n', '0\\n', '1\\n', '1\\n', '3\\n', '0\\n', '0\\n', '3\\n', '3\\n', '1\\n', '0\\n', '3\\n', '3\\n', '3\\n', '3\\n', '0\\n', '1\\n', '0\\n', '3\\n', '1\\n', '0\\n', '3\\n', '0\\n', '0\\n', '0\\n', '0\\n', '1\\n', '0\\n', '3\\n', '3\\n', '0\\n', '3\\n', '0\\n', '3\\n', '3\\n', '1\\n', '0\\n', '1\\n', '2\\n', '2\\n', '2\\n', '1\\n', '2\\n', '0\\n', '0\\n', '1\\n', '2\\n', '1\\n', '0\\n', '0\\n', '3\\n', '0\\n', '3\\n', '0\\n', '1\\n', '0\\n', '0\\n', '1\\n', '0\\n', '3\\n', '3\\n', '0\\n', '0\\n', '2\\n', '0\\n', '0\\n', '0\\n', '3\\n', '0\\n', '1\\n', '2\\n', '1\\n', '1\\n', '0\\n', '0\\n', '0\\n', '0\\n', '0\\n', '0\\n', '2\\n', '3\\n', '0\\n', '2\\n', '3\\n', '1\\n', '1\\n', '0\\n', '0\\n', '1\\n', '0\\n', '0\\n', '2\\n', '2\\n', '1\\n', '1\\n', '0\\n', '1\\n', '3\\n', '0\\n', '0\\n', '1\\n', '1\\n', '3\\n', '3\\n', '2\\n', '0\\n', '0\\n', '1\\n', '2\\n', '3\\n', '1\\n', '1\\n', '3\\n', '3\\n', '0\\n', '0\\n', '3\\n', '1\\n', '0\\n', '1\\n', '1\\n', '0\\n', '1\\n', '0\\n', '1\\n', '0\\n', '0\\n', '3\\n', '1\\n', '3\\n', '0\\n', '3\\n', '1\\n', '3\\n', '0\\n', '0\\n', '1\\n', '0\\n', '0\\n', '0\\n', '2\\n', '2\\n', '3\\n', '0\\n', '3\\n', '0\\n', '3\\n', '0\\n', '0\\n', '0\\n', '1\\n', '1\\n', '0\\n', '1\\n', '2\\n', '0\\n', '0\\n', '3\\n', '1\\n', '0\\n', '2\\n', '1\\n', '1\\n', '0\\n', '3\\n', '3\\n', '0\\n', '3\\n', '0\\n', '0\\n', '0\\n', '1\\n', '0\\n', '2\\n', '0\\n', '0\\n', '0\\n', '3\\n', '0\\n', '2\\n', '1\\n', '3\\n', '2\\n', '0\\n', '0\\n', '2\\n', '1\\n', '0\\n', '0\\n', '0\\n', '3\\n', '0\\n', '0\\n', '1\\n', '0\\n', '1\\n', '1\\n', '3\\n', '0\\n', '3\\n', '3\\n', '1\\n', '0\\n', '0\\n', '3\\n', '3\\n', '3\\n', '3\\n', '1\\n', '3\\n', '1\\n', '0\\n', '1\\n', '3\\n', '0\\n', '0\\n', '0\\n', '0\\n', '1\\n', '1\\n', '3\\n', '0\\n', '2\\n', '3\\n', '0\\n', '3\\n', '3\\n', '0\\n', '3\\n', '1\\n', '3\\n', '3\\n', '0\\n', '0\\n', '3\\n', '0\\n', '0\\n', '0\\n', '3\\n', '0\\n', '1\\n', '3\\n', '0\\n', '0\\n', '0\\n', '3\\n', '1\\n', '0\\n', '3\\n', '0\\n', '0\\n', '0\\n', '2\\n', '3\\n', '3\\n', '3\\n', '1\\n', '1\\n', '3\\n', '3\\n', '2\\n', '1\\n', '3\\n', '3\\n', '1\\n', '1\\n', '3\\n', '0\\n', '3\\n', '1\\n', '2\\n', '1\\n', '3\\n', '0\\n', '0\\n', '1\\n', '3\\n', '3\\n', '1\\n', '0\\n', '3\\n', '0\\n', '1\\n', '3\\n', '3\\n', '1\\n', '3\\n', '0\\n', '0\\n', '1\\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrpphHNSbtqV",
        "outputId": "7cf46014-74d0-4e14-f294-cc5f9049e76c"
      },
      "source": [
        "print(predicted_data)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3, 2, 3, 1, 1, 0, 3, 3, 1, 0, 0, 3, 3, 0, 0, 3, 1, 3, 0, 0, 0, 3, 3, 0, 2, 1, 3, 0, 3, 1, 3, 1, 0, 0, 0, 0, 2, 3, 0, 3, 0, 0, 3, 1, 3, 0, 0, 1, 1, 1, 3, 3, 1, 3, 2, 0, 3, 3, 2, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3, 0, 0, 0, 0, 3, 2, 2, 1, 0, 0, 1, 3, 0, 0, 0, 1, 1, 3, 3, 1, 3, 0, 0, 1, 1, 3, 0, 0, 0, 1, 1, 1, 0, 1, 0, 2, 1, 0, 2, 0, 0, 0, 3, 1, 3, 3, 0, 2, 3, 0, 3, 0, 0, 3, 0, 0, 0, 3, 1, 0, 0, 1, 1, 1, 0, 3, 3, 3, 3, 0, 0, 0, 3, 1, 0, 0, 3, 3, 2, 0, 0, 3, 2, 0, 3, 1, 3, 0, 2, 0, 2, 1, 1, 1, 0, 3, 0, 1, 0, 0, 1, 1, 2, 0, 0, 0, 1, 1, 1, 0, 0, 3, 3, 0, 0, 0, 3, 1, 0, 0, 2, 3, 0, 1, 1, 0, 0, 3, 2, 1, 0, 0, 1, 1, 0, 0, 1, 3, 1, 3, 2, 3, 1, 3, 0, 0, 3, 0, 2, 3, 3, 3, 3, 0, 2, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 1, 1, 0, 3, 2, 1, 3, 3, 3, 3, 1, 1, 3, 0, 0, 3, 0, 1, 1, 3, 0, 0, 1, 0, 1, 0, 2, 0, 3, 1, 0, 3, 0, 0, 3, 3, 1, 0, 1, 3, 3, 3, 0, 0, 2, 1, 0, 0, 3, 1, 0, 0, 0, 3, 3, 1, 3, 2, 1, 3, 3, 0, 1, 0, 0, 3, 2, 3, 0, 1, 3, 3, 0, 3, 0, 3, 0, 1, 3, 0, 3, 0, 3, 3, 0, 0, 1, 0, 0, 0, 3, 2, 3, 1, 2, 2, 0, 3, 1, 1, 3, 1, 2, 2, 3, 0, 1, 1, 0, 0, 3, 0, 1, 3, 1, 0, 3, 0, 1, 0, 0, 1, 3, 0, 0, 3, 0, 2, 0, 0, 1, 2, 2, 0, 1, 0, 2, 3, 1, 3, 0, 2, 3, 3, 3, 1, 3, 3, 1, 0, 2, 0, 1, 0, 3, 1, 0, 0, 1, 1, 3, 0, 0, 0, 0, 2, 3, 0, 1, 0, 1, 3, 0, 1, 1, 1, 1, 3, 0, 0, 2, 1, 1, 0, 3, 1, 0, 3, 1, 3, 3, 3, 1, 3, 1, 2, 3, 1, 0, 0, 3, 1, 0, 3, 3, 0, 2, 3, 3, 0, 3, 3, 0, 0, 1, 3, 2, 3, 0, 3, 0, 0, 0, 1, 1, 3, 1, 0, 0, 1, 1, 0, 3, 0, 0, 0, 1, 1, 0, 3, 0, 0, 1, 1, 3, 3, 3, 0, 1, 1, 1, 1, 3, 3, 3, 2, 0, 3, 1, 0, 1, 3, 3, 3, 0, 2, 3, 0, 0, 0, 0, 2, 1, 1, 0, 0, 3, 0, 0, 0, 2, 2, 0, 1, 0, 3, 1, 0, 0, 0, 3, 3, 0, 3, 1, 3, 1, 0, 1, 0, 1, 1, 3, 0, 1, 0, 3, 0, 3, 3, 3, 0, 1, 3, 2, 0, 0, 2, 0, 1, 1, 3, 1, 1, 1, 1, 1, 0, 0, 0, 0, 3, 3, 1, 1, 1, 1, 2, 0, 1, 3, 0, 3, 0, 1, 1, 1, 3, 0, 0, 1, 0, 0, 3, 1, 0, 0, 0, 1, 3, 1, 2, 1, 1, 0, 0, 1, 3, 1, 0, 0, 3, 0, 1, 3, 1, 0, 1, 1, 0, 1, 2, 0, 0, 3, 0, 0, 0, 3, 3, 0, 1, 1, 0, 3, 1, 0, 0, 1, 3, 1, 0, 3, 2, 0, 0, 3, 1, 0, 1, 0, 0, 3, 3, 1, 3, 0, 1, 3, 3, 1, 1, 0, 0, 0, 3, 3, 3, 1, 3, 0, 0, 3, 1, 0, 1, 2, 1, 0, 1, 0, 1, 1, 0, 3, 0, 0, 0, 3, 0, 3, 0, 3, 0, 0, 2, 1, 1, 1, 3, 1, 3, 1, 0, 1, 0, 2, 3, 0, 0, 3, 1, 2, 3, 1, 3, 3, 3, 3, 0, 3, 0, 0, 3, 1, 1, 3, 0, 1, 3, 0, 3, 1, 1, 0, 2, 0, 0, 3, 0, 3, 3, 1, 0, 1, 2, 3, 0, 0, 1, 0, 2, 1, 0, 0, 3, 0, 1, 0, 1, 0, 3, 1, 3, 1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 0, 1, 0, 1, 0, 1, 0, 0, 2, 1, 0, 0, 0, 3, 3, 3, 0, 1, 1, 1, 2, 2, 0, 3, 0, 3, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 1, 1, 1, 3, 0, 3, 0, 1, 0, 3, 0, 3, 0, 1, 3, 0, 0, 3, 0, 2, 1, 3, 1, 0, 1, 1, 0, 3, 3, 1, 0, 3, 0, 0, 0, 0, 0, 2, 3, 0, 0, 0, 1, 1, 3, 3, 3, 0, 2, 1, 0, 1, 3, 1, 0, 1, 0, 3, 1, 0, 0, 1, 0, 1, 0, 1, 3, 0, 1, 0, 1, 1, 0, 1, 0, 0, 3, 3, 0, 3, 0, 0, 0, 3, 0, 1, 3, 2, 3, 2, 0, 3, 3, 0, 0, 0, 0, 1, 3, 3, 0, 0, 3, 1, 1, 2, 0, 3, 0, 3, 3, 3, 3, 1, 0, 0, 3, 0, 3, 3, 0, 0, 1, 1, 0, 0, 3, 1, 0, 2, 3, 0, 0, 0, 3, 3, 3, 3, 2, 1, 3, 0, 1, 0, 1, 3, 1, 0, 1, 2, 0, 2, 0, 2, 0, 2, 3, 0, 0, 0, 1, 0, 3, 3, 1, 2, 3, 0, 3, 0, 0, 0, 3, 3, 3, 3, 0, 1, 3, 0, 0, 2, 0, 1, 1, 1, 3, 3, 1, 3, 0, 0, 1, 3, 1, 1, 3, 0, 3, 3, 1, 1, 0, 1, 0, 0, 0, 0, 1, 2, 0, 2, 0, 1, 0, 0, 1, 0, 3, 0, 0, 0, 1, 3, 1, 3, 0, 2, 3, 1, 3, 0, 3, 1, 1, 1, 3, 0, 0, 0, 0, 1, 3, 0, 3, 3, 1, 1, 0, 1, 3, 1, 3, 0, 1, 3, 3, 0, 1, 3, 3, 1, 2, 1, 0, 0, 1, 0, 1, 3, 2, 0, 3, 1, 1, 3, 0, 3, 0, 3, 0, 1, 0, 0, 3, 3, 0, 1, 1, 2, 2, 2, 0, 1, 0, 3, 1, 1, 3, 0, 2, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 1, 1, 1, 0, 0, 3, 3, 1, 0, 1, 3, 3, 3, 0, 2, 0, 3, 1, 0, 3, 3, 0, 0, 0, 1, 0, 3, 1, 0, 3, 3, 0, 3, 1, 0, 1, 0, 1, 2, 1, 0, 0, 0, 1, 2, 1, 0, 0, 3, 2, 3, 0, 1, 3, 0, 1, 3, 3, 3, 0, 0, 0, 0, 0, 0, 3, 0, 1, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0, 3, 0, 2, 3, 1, 1, 3, 0, 1, 3, 0, 1, 0, 1, 1, 0, 1, 3, 0, 3, 3, 1, 3, 3, 2, 0, 0, 1, 1, 3, 1, 1, 3, 3, 0, 3, 3, 1, 0, 1, 1, 0, 1, 2, 1, 0, 0, 0, 1, 3, 0, 3, 1, 3, 0, 0, 1, 0, 0, 0, 3, 1, 3, 1, 3, 3, 3, 0, 2, 0, 1, 1, 0, 1, 2, 0, 0, 3, 1, 3, 2, 1, 1, 0, 3, 3, 0, 3, 0, 0, 0, 0, 0, 2, 0, 0, 0, 3, 1, 2, 1, 3, 2, 0, 0, 2, 2, 0, 0, 0, 3, 0, 0, 1, 0, 1, 3, 3, 3, 3, 3, 1, 3, 0, 3, 3, 3, 3, 3, 0, 3, 0, 1, 0, 0, 0, 0, 0, 1, 1, 3, 0, 0, 0, 0, 3, 3, 2, 1, 1, 3, 2, 0, 1, 3, 0, 0, 0, 3, 0, 1, 3, 0, 0, 0, 3, 1, 0, 3, 3, 0, 0, 2, 3, 3, 3, 2, 2, 3, 3, 2, 1, 3, 3, 1, 1, 3, 0, 3, 1, 2, 1, 3, 0, 0, 1, 3, 3, 1, 0, 3, 0, 1, 0, 3, 1, 3, 0, 0, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GB9xHX594HE7",
        "outputId": "53b2ba86-18ad-41cd-b29c-ff7b7bf313bf"
      },
      "source": [
        "def getAccuracy(test_data,predicted_data):\n",
        "   n = len(test_data) \n",
        "   correct = 0\n",
        "   for x in range(n):\n",
        "       if test_data[x] == predicted_data[x]:\n",
        "           correct += 1\n",
        "   return (correct/n) * 100.0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "accuracy = getAccuracy(test_data,predicted_data)\n",
        "print(\"we got the accuracy by \", accuracy ,\"%\")\n",
        "  "
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "we got the accuracy by  83.46235045742435 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySd3hmtweHYF"
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLUrymIqBBp1"
      },
      "source": [
        "\n",
        "def model(tweets):\n",
        "\n",
        " model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "\n",
        " text = tweets\n",
        " encoded_input = tokenizer(text, return_tensors='tf')\n",
        " output = model(encoded_input)\n",
        " scores = output[0][0].numpy()\n",
        " scores = softmax(scores)\n",
        "\n",
        " temp_list=[]\n",
        " ranking = np.argsort(scores)\n",
        " ranking = ranking[::-1]\n",
        " for i in range(scores.shape[0]):\n",
        "    l = labels[ranking[i]]\n",
        "    s = scores[ranking[i]]\n",
        "    temp_list.append(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n",
        " return temp_list"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ue4W8g1HBC6A"
      },
      "source": [
        ""
      ],
      "execution_count": 19,
      "outputs": []
    }
  ]
}